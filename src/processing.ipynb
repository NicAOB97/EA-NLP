{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80,000 hours podcast text transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('80k-podcast.txt') as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robert W: Hi, I�m Rob, Director of Research at 80,000 Hours. Today I�m speaking with Miles Brundage, research fellow at the University of Oxford�s Future of Humanity Institute. He studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.\\n',\n",
       " '\\n',\n",
       " 'Miles�s research has been supported by the National Science Foundation, the Bipartisan Policy Center, and the Future of Life Institute. Thanks for making time to talk to us today, Miles.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: [00:00:30] Thanks for having me.\\n',\n",
       " '\\n',\n",
       " 'Robert W: First up, maybe just tell us a bit about your background and what questions you�re looking into at the moment.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Sure. I used to work on energy policy when I was living in Washington DC, and I�ve gradually moved into AI policy over the past few years, because it seems like it is more of a neglected area and could have a very large impact. I recently started at the Future of Humanity Institute and I�m also concurrently finishing up my PhD in human and social dimensions of science [00:01:00] and technology at Arizona State University. In both capacities, I am interested in what sorts of methods would be useful for thinking about AI in a rigorous way, and particularly the uncertainty of possible futures surrounding AI. I have a few more specific interests such as openness and artificial intelligence and the risks related to bad actors.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Are you trying to predict what�s gonna happen and when and what sort of effects it will have?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: [00:01:30] Not necessarily predict per se, but at least understand what�s plausible. I think that it�s very difficult to predict with any high confidence what�s gonna happen and when, but understanding, for example, what sorts of actions people could carry out with AI systems today or in the foreseeable future in different domains like cybersecurity and information operations, production of fake news automatically [00:02:00] and autonomous drones and so forth. I think understanding the security landscape of those sorts of things doesn�t necessarily require a very detailed forecast of when exactly something will occur so much as understanding what�s technologically possible given near term trends. Then you don�t necessarily have to say this is when this particular accident or malicious behavior will occur, but just that these are the sorts of risks that we need to be prepared for.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Does that bring you into contact with [00:02:30] organizations that are developing AIs like Google or Open AI?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, absolutely. Those organizations have an interest in making sure that AI is as beneficial as possible, and they�re keenly aware of the fact that they can be misused and that there might be accident risks associated with them. For example, I held a workshop fairly recently on the connection between bad actors and AI and what sorts of bad actors we might be concerned [00:03:00] about in this space, and we had some participation from some of those groups.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of bad actors are you thinking of?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: You can look at it from a number of different perspectives. Depending on the domain that you�re interested in, you might end up with different sorts of actors. In the case of physical harm associated with AI, you might look at autonomous weapons and the weaponization of consumer drones, and in that case it�s something that would be [00:03:30] particularly appealing to state actors as well as terrorists, and we�re already seeing the weaponization of consumer drones, though without much autonomy if any, in the case of ISIS overseas. There are cases like that where we can foresee that particular groups when given more advanced capabilities such as higher baseline levels of autonomy in consumer drones, would be able to and would like to carry out damage on a higher [00:04:00] scale.\\n',\n",
       " '\\n',\n",
       " 'In other cases, it�s less clear. For example, there are concerns around mass surveillance, and it�s often not totally clear whether one should be more concerned about states or corporations, depending on what sorts of risk you�re concerned about. For example, corporations currently have a lot of data on people, and there have been a lot of concerns raised about the use of AI to make decisions that have critical impacts on [00:04:30] people�s lives, such as denying loans and court decisions and so forth that are being informed by often not very transparent algorithms. Those are some concerns, but there�s also a potentially different class of concerns around authoritarian states using AI for oppression.\\n',\n",
       " '\\n',\n",
       " 'I think there�s a range of risks, but generally speaking there�s going to be a steady increase in [00:05:00] the floor of the skill level as these AI technologies diffuse. That is, there will be more and more capabilities available to people at the bottom of the scale, that is individuals as well as people with more access to computing power, money, and data at the higher end. It�s not totally clear how to think about that because on the one hand you might expect that the biggest actors are the most concerning because they have the most skill and resources, but on the other hand they also [00:05:30] are sometimes, in the case of democratic governments, held accountable to citizens, and in the case of corporations held accountable to stakeholders. You might also think that you would be more concerned about individual rogue actors. That�s an issue I�m still trying to think through in the context of the report that I�m writing based on that workshop.\\n',\n",
       " '\\n',\n",
       " 'Robert W: People often distinguish between the short term risks from AI, like self driving cars not working properly or algorithmic [00:06:00] bias, and then the longer term concerns of what we�re gonna do once artificial intelligence is as smart as humans or potentially much, much more intelligent. Which one of those do you spend more time working on?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I try to focus on issues that span different time horizons, so I think that AI and security is something that we�re already seeing some early instances of today. It�s already being used for detection of cyber threats, for example, and the [00:06:30] production of vulnerabilities in software. There�s a lot of research going on, which an example being the DARPA cyber grant challenge recently, where there�s automated hacking and automated defense. There�s already stuff happening on the front of AI and security as well as the economics of AI and issues surrounding privacy, but it�s not totally clear whether the issues in the future will just be straightforward extensions of [00:07:00] those or whether there will be qualitatively different risks. I think it�s important to think about what�s happening today and imagine how it might progressively develop into a more extreme future, as well as to think about possible discontinuities when AI progresses more quickly.\\n',\n",
       " '\\n',\n",
       " 'Robert W:I should say this point, that we have a profile up on our website about the potential upsides and downsides of artificial intelligence, where we go through all of the basics. [00:07:30] Miles, you have a forthcoming guide to work on AI policy and strategy that we�re gonna link to. Here, we�re gonna try to go beyond that. If you find yourself a little bit confused, then potentially just go on to read one of those documents first.\\n',\n",
       " '\\n',\n",
       " 'Which policy or strategy questions do you see as most important for us to answer in order to ensure that the development of artificial intelligence goes well rather than in a bad way?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think there are a lot of questions, and some of the ones I was talking about earlier surrounding security [00:08:00] and preventing the weaponization of AI by dangerous actors is one. There are also other issues that have been widely discussed in the media and by academics, such as accountability of algorithms, transparency, the economic impacts of AI and so forth, but one that I think is particularly important, particularly over the long term, this is a case where there might be a need [00:08:30] to think about possible discontinuities, is coordination surrounding AI safety. There�s been a lot of attention called to AI safety in the past few years and there�s starting to be a lot of concrete and successful research on the problem of avoiding certain AI safety failure modes, like wire heading and value misalignment and so forth, but there�s been less attention paid to how do you actually incentivize people to act on the best practices and the good theoretical frameworks [00:09:00] that are developed, assuming that they will be developed.\\n',\n",
       " '\\n',\n",
       " 'That�s a potentially big problem if there�s a competitive situation between companies or between countries or both and there�s an incentive to skimp on those safety measures. There�s been a little bit of work, for example, by Armstrong et al at the Future of Humanity Institute looking at arms race type scenarios involving AI where there would be an incentive to skimp because you�re concerned about losing the lead in an [00:09:30] AI race. We don�t really know what the extent will be, but it might turn out that there are significant trade offs between safety and performance of AI systems. One might be tempted to, in order to gain an advantage whether economic or military or intelligence-wise, ramp up the capabilities of an AI system by adding more hardware, adding more data, adding more sensors and effectors and so forth, but that might actually be dangerous if you don�t understand the full [00:10:00] behavioral envelope of the system, so to speak. And how to constrain human actors from doing that is a very difficult problem.\\n',\n",
       " '\\n',\n",
       " 'I, along with others at the Future of Humanity Institute and other different organizations have started thinking about what is an incentive compatible mechanism to ensure that people do the right thing in that case. That is to say, we don�t want to ask people to do something that�s totally against their interests if [00:10:30] they actually do have an interest in developing these systems and protecting themselves, militarily or otherwise, but we also want to ensure that they�re constrained in some sort of way. One example would be, this is not necessarily a fully fleshed out example, but just to illustrate the sort of thing I�m talking about, would be some sort of arrangement between AI developers such that in order to gain access to the latest breakthroughs or the latest computing power, you would need to submit to some sort of [00:11:00] safety monitoring and adhere to certain best practices.\\n',\n",
       " '\\n',\n",
       " 'That would create an incentive if you want to be on the cutting edge to participate in those safety protocols. Again, that�s not a fully worked out example, but that�s an example of the class of things that I would like to see more of. Developing specific proposals for how to ensure that the right thing that�s being developed on the technical front actually gets implemented.\\n',\n",
       " '\\n',\n",
       " 'Robert W: [00:11:30] Do you look much at, say, what governments should be doing in this area? Whether there�s regulations that we should be putting in place or at least preparing to put in place in future?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. I think that that ties in with what I was saying a little bit, in that one might envision that as AI capabilities develop, that it�ll be increasingly seen as a matter of national security that a country be on the leading edge. I think it�s not super clear how [00:12:00] to navigate that yet. I think there�s some low hanging fruit. Clearly, in my opinion, it would be beneficial for countries to have some experts in house in their governments who actually know something about AI and are able to deal with crises as they arise, if they arise, and to be able to think carefully about the impacts of AI on the labor force, for example.\\n',\n",
       " '\\n',\n",
       " 'I think there�s some low hanging fruit and some policies that have been developed [00:12:30] for dealing with some of the near term issues, but for the longer term issues, it�s not super clear that we want to rush into a situation where governments are leading this, if that would turn out to only accentuate the arms race dynamics that we should be trying to avoid if AI is seen even more as a national security issue in an unhelpful way. I think it�s incumbent on those who are thinking about the long term policy issues around AI to develop a more positive [00:13:00] proposal that involves not just the US government getting involved for the purpose of accelerating American AI capabilities, but a more collaborative approach.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I suppose with previous dangerous technologies like nuclear weapons or chemical or biological weapons, there�s been agreements to try to slow down their development and deployment. Is that something that could potentially happen here at the international level?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think it�s possible [00:13:30] to imagine some sort of slowing down among a small number of actors, and we�re fortunate that there is a high concentration of computing power and talent and skill in a fairly small number of organizations. There�s pretty much no chance of some random person out competing the top organizations in AI in a surprising way. It�s possible that with some of the top companies and [00:14:00] nonprofits as well as countries, they could coordinate in some sort of way. Not to postpone AI across society for a long period of time, but at least to be cautious in the later stages of development and to allow time for mutual vetting of safety procedures and things like that. I think it�s possible to imagine some sort of coordination, but the technical and the political factors interact to a large extent.\\n',\n",
       " '\\n',\n",
       " 'To the extent that we actually have good safety measures that [00:14:30] are efficient and that don�t introduce a lot of overhead, computational or otherwise, into these systems, then we�ll be in a better place to actually get people to coordinate because it won�t be imposing a lot of costs on them. Likewise, to the extent that we�re able to coordinate better, we�ll be in a better position to actually get people to implement those mechanisms if they do impose some performance penalty.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Can you think of any major bits of progress we�ve made on Ai strategy [00:15:00] questions over the last couple of years?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: The example I mentioned of the paper by Armstrong et al called Racing to the Precipice is one example, and it presented sort of a stark version of the arms race scenario, that I think there are reasons to be more optimistic than are presented in that paper because they look at only a certain set of assumptions, as with any model. More generally, I think there�s been progress in [00:15:30] the development of principles and criteria for good policies in recent years. For example, with the Asilomar conference and the set of Asilomar AI principles, I think that was a good step towards developing some shared understanding around the need to avoid arms races and concerns over AI weaponization and so forth.\\n',\n",
       " '\\n',\n",
       " 'I still think that there�s a lot of room for progress to be made. There was a paper [00:16:00] put out by the Future of Humanity Institute by Nick Bostrom, Carrick Flynn, and Allan Defoe recently that looks at policy desiderata for the development of machine superintelligence. There�s a lot of good material there, but I think they would be the first to admit that we�re still at the high level principles stage of developing these policies. We have a better understanding of what we want to avoid than we did a few years ago, and what a good policy proposal [00:16:30] would look like, but we don�t yet have anything super actionable.\\n',\n",
       " '\\n',\n",
       " 'I think the situation is a bit analogous to where AI safety was a few years ago, where there was starting to be an articulation of what the problem was with the book Superintelligence and various other publications. People were starting to take the problem seriously and started to have a vocabulary for what a solution would look like with value alignment and [00:17:00] other terms being coined, but there wasn�t yet any concrete research agenda. Subsequently, there have been a lot of concrete research agendas, as well as technical progress on some parts of those research agendas with work by Open AI and Deep Mind and others leading the way. I think we�re potentially in a similarly exciting phase on the AI policy front where we have a decent understanding of what the problem is. [00:17:30] With the example of avoiding arms races being forefront in my mind, at least, but we don�t have clear models and case studies that we can point to as ways forward. I think that�s the next step, is moving into more concrete proposals and trying to balance some of the trade offs that have been identified in recent papers.\\n',\n",
       " '\\n',\n",
       " 'Robert W: It seems like a lot of the latest thinking in this area isn�t [00:18:00] written up in public yet, partly because people don�t want to publicize their views at this point or just because it takes a long time to get papers published. What do you think�s the best way for someone who�s interested in working in the area to get up to speed on the issues?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think, again, it�s somewhat analogous to the situation with AI safety a few years ago. I think the book Superintelligence was a big step forward on that front in terms of having a single reference to point people to. [00:18:30] It�s less clear if there�s one single reference to point people to on AI policy issues, though I�m hoping that the career guide that I�m working on will be somewhat useful in that regard. Another resource that comes to mind is a syllabus for a class on the global politics of AI developed by Allan Dafoe at [Oxford] and the Future of Humanity Institute. That�s a very detailed list of resources. I�m sure there�ll be a link [00:19:00] provided for this interview somewhere.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: That�s another set of resources. It�s essentially a long list of both formal academic publications as well as things in the more gray literature, such as blog posts, which is, I think, the sort of thing that you�re referring to as things that aren�t necessarily written up academically, but are somewhat accessible. Even there, there�s still some things like [00:19:30] Google Docs that are used internally and so forth. If, based on reading those sorts of things, this is clearly something you�re interested in, then the obvious next step is just to get in touch with people who are working on these issues and indicate what your interests are. There might be things that aren�t available online that they can point you to.\\n',\n",
       " '\\n',\n",
       " 'Robert W:You mentioned the Asilomar statement of principles. Do you just want to describe that?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Sure. Two [00:20:00] years ago, there was a conference on beneficial AI held in Puerto Rico. That led to an open letter on AI, and subsequently some investment by Elon Musk in the Future of Life Institute, which supported a lot of grants in this space. I think that that led to a lot of attention to the issue and there were thousands of people, including a lot of AI scientists, who signed onto that letter, and then subsequently [00:20:30] the Asilomar principles, which were developed at a conference two years later, this year at Asilomar in California, developed a more specific set of proposals for what sorts of things AI scientists should be thinking about. Not just the fact that there should be more research, but also things like capability caution, so being attentive to the fact that we don�t know for sure what the upper limits of AI capabilities [00:21:00] will ultimately be, and the things I mentioned about avoiding arms races and being concerned about AI weaponization more broadly.\\n',\n",
       " '\\n',\n",
       " 'I think that�s a good example of developing a consensus view on what we want to see and what we don�t want to see. We want to see AI benefiting society as a whole, and we don�t want to see it leading to the accruing of benefits to a small number of people or to large scale war [00:21:30] or anything like that. I think it�s encouraging to see not just that, but also the development of other sets of principles through the IEEE and their effort on ethically aligned AI design.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I read those principles and they seem quite strong on paper. Do you think people are likely to follow through on them?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think there�s somewhat of a gap between the level of abstraction [00:22:00] of the principles and concrete steps that people can take. For example, it�s not totally clear what any individual can do to stop an arms race, for example, but I think it�s a step in the right direction to know where you�re headed, and then figuring out exactly what to do about that is the next step.\\n',\n",
       " '\\n',\n",
       " 'Robert W: In your guide, you suggest that people could potentially work on AI strategy and policy questions at places like Google Deep [00:22:30] Mind or Open AI, where artificial intelligence is actually being developed. What concretely could you see people doing at places like that if they took a job there?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: To some extent, people at those organizations are doing something fairly similar to what I and my colleagues are doing in academia, which is thinking about what the problems are and trying to develop solutions. I think the benefit of being on that side of things is to have [00:23:00] more direct exposure to what�s happening on the ground, so to speak, in AI development, and I think that can be useful for developing a set of what sorts of problems are actually cropping up as a result of the development of capabilities that actually exist as opposed to just ones in the future.\\n',\n",
       " '\\n',\n",
       " 'I think it�s also notable that people at these organizations have been very active in the efforts that I�ve mentioned, such as raising attention to AI safety, which was an initiative that [00:23:30] involved people at Google Brain and Open AI and the concrete problems in the AI safety paper, and people at DeepMind have been very influential in calling for more AI safety work and they�ve been publishing fairly early on this matter relative to other organizations. I think there�s a lot of reason to think that these organizations are playing a positive role and I think it would be a good place to be involved in thought leadership on [00:24:00] these topics, as well as doing direct research.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How hard it is to get a job at a place like that? I would imagine they not only have a few people working on AI strategy or policy. Are these extremely competitive roles?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I would say they�re pretty competitive, and I think that�s a general phenomenon in anything related to AI these days, whether on the policy side or the technical side. I think that this is a growth area for [00:24:30] sure. If one develops expertise in this topic and has something to contribute, then I think there will ultimately be opportunities available to you in the next few years. Again, I want to draw an analogy to AI safety where there was a lot of concern about will there be jobs in AI safety three years or so ago, and I think that deterred some grad students from working in this space. Now the situation seems to look a bit better in that there�s a fair number of advertisements for [00:25:00] postdocs and people being hired at top labs to work on these issues. I think we�ll probably see something pretty similar, not necessarily it becoming not competitive, but there being a little bit of slackening over time as there�s a need for developing larger teams to work on these topics.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Another path you�ve talked about is going into politics or policy roles, for example as a Congressional staffer [00:25:30] or perhaps at a think tank like the Brookings Institute or the National Science Foundation, or potentially just going into party politics in general. What useful things do you envisage people could do there?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think in the same way that working at an AI lab would give you a better sense of what�s practical on the technical front, working for Congress or Parliament or a political party would give you a better sense of what�s practical on the political front. In addition to that, [00:26:00] developing a better intuition for the system that you�re dealing with and what the constraints are, I think it�s also potentially a good way to actually influence what�s actually done by those bodies. For example, in the US Congress right now, there was just recently announced an AI caucus to organize the discussion of members of Congress on issues related to AI, with particular focus on issues related to the [00:26:30] future of work. I think over time, we�ll see more discussion of the longer term sorts of issues that we�ve been talking about in those fora and they would be useful to have people who are concerned about making sure that the right thing is ultimately done working in those places and able to actually act on the best current understanding developed by researchers and by practitioners.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I think a big risk of going into politics or policy [00:27:00] in general would be that your career progresses, but you�re not specifically in one of the places that ends up having a say in how these things go. You just end up working in a Congressional committee that turns out not to be that relevant. Are there any roles that you can take early on, or what can you do to position yourself well so that you don�t just get sidelined in the end?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think if you intend to work in politics, then you have to be somewhat opportunistic about [00:27:30] jobs and the ebb and flow of political opportunities. For example, if one Congressional committee is declining in its relevance in terms of AI, then one should consider about jumping ship towards somewhere that�s more relevant. I don�t think that taking one particular job will lock you in forever, but building up some political capital and some human capital in that area could be useful, [00:28:00] even if you ultimately want to switch to a different organization.\\n',\n",
       " '\\n',\n",
       " 'I would push back a little bit on the idea of not being relevant just because you�re not at the organization where people are developing the latest AI. As I mentioned before, I think that�s a very exciting opportunity and along with academia, it�s a great place to work on these issues. Governments still play an important role in framing these issues [00:28:30] and in convening discussions. An example of this would be the preparing for the future of artificial intelligence report put out by the White House Office of Science and Technology Policy last year, which was the result of four workshops that brought together experts in a wide variety of areas: in law and economics and AI technology and safety. I think you still have an opportunity to [00:29:00] frame the discussion and move the ball forward, even if you�re not working in the latest labs.\\n',\n",
       " '\\n',\n",
       " 'Robert W:What would be some of the best places to apply to in that area?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: It�s a bit tricky to answer that in general. It depends on factors like what your citizenship is and what particular policy issues you�re interested in, but some fairly robust recommendations would be to [00:29:30] look � For a particular class of people that have technical backgrounds, one area to look at would be AAAS fellowships. The American Association for the Advancement of Science has a fellowship program where they essentially put people in rotations in organizations like Congress and the executive branch, where they can draw on their technical [00:30:00] experience, and someone with a background in AI would probably end up in an AI relevant organization. I have heard a lot of good things about that being a good way to develop career capital and experience and understanding of how the political system works, but that�s not necessarily going to result in being in the exact right location for solving AI policy problems the next few months.\\n',\n",
       " '\\n',\n",
       " 'I think it�s really hard to say where that would be or even if there is such a place because I think we�re still, [00:30:30] despite all the progress that�s being made, in a relatively early stage. There isn�t even yet a nomination for the Director of the Office of Science and Technology Policy in the White House. Otherwise, I would say that that is a good place to go. Likewise, we don�t really know what the agenda of the Trump Administration is on the AI front, and there�s a lot of discussion, [00:31:00] but not a lot of institution building in governments at the moment. It�s hard for me to answer that in general, but I think that getting experience is a pretty robust thing to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: A situation that I encounter reasonably often is I meet someone who�s really smart, who is very interested in this topic, who might be able to make a great contribution. At the moment, there aren�t that many groups that are hiring for these roles. [00:31:30] You�re at the Future of Humanity Institute, and I guess there�s also the Future of Life Institute, but there aren�t that many places that someone can potentially apply. What advice should I give someone in that situation? Should they continue studying, perhaps, or building expertise so that they�re in a better position to apply in the future when the number of positions grows?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Again, it�s hard to answer that in general. It depends on what background the person has and what sorts [00:32:00] of jobs they�re interested in. I think if one casts a fairly large net in terms of what AI issues one wants to work on, there�s a fairly large range of organizations. It�s not just FHI. There�s also the Center for the Study of Existential Risk, the Leverhulme Center for the Future of Intelligence, the Tech Policy Lab at the University of Washington, and various faculty programs. [00:32:30] Various academic programs around the country, in the US, and around the world are looking to hire people, for example, as postdocs or a faculty member in areas related to AI and policy if you�re someone with an academic background. There are probably a fair number of policy positions at tech companies that aren�t necessarily specific to AI, but that would lead to getting valuable experience in the broad area of tech policy.\\n',\n",
       " '\\n',\n",
       " 'To [00:33:00] generalize a bit, I would recommend casting a fairly large net and working in adjacent areas if you can�t immediately work in the exact right place. This is not the sort of area where people have been working on AI policy for decades and it�s hard to break in for that reason. It�s more just that it�s still a field that�s growing. I don�t think that that growth will stop. I think there will be more opportunities in the future, but if you can�t find the right opportunity [00:33:30] now, then talk to people in the field about what sorts of opportunities are on the horizon and try and work in adjacent fields. That might be one thing to consider.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What do you think would be the biggest challenges for someone starting out, trying to start a career in this area?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Depending on one�s background, I think the main challenge would be boning up on areas of weakness. For example, if you have a policy background, you might want to focus on [00:34:00] learning more about AI, and if you have a technical background, you might want to focus more on learning about policy. I think one of the challenges is what you mentioned before about there not being a super clearly accessible literature on the topic, and I think there�s some effort being going into addressing that with the career guide and the bibliography that I mentioned.\\n',\n",
       " '\\n',\n",
       " 'I�m not really sure that there�s any specific [00:34:30] set of pitfalls that I would recommend that people avoid besides not neglecting the areas in which they�re weak. Don�t try and rest on your laurels in, say, technical areas or policy areas, because it�s a fairly interdisciplinary area, and you would need to think about what sorts of disciplinary perspectives would be beneficial to solving the problem you�re interested in, and not just what is your background, though of course you want to draw on your strengths.\\n',\n",
       " '\\n',\n",
       " 'Robert W: [00:35:00] Because it�s a field that is in its fairly early stages and is growing quickly, it�d be a good fit for someone who was really able to set their own direction and meet people and potentially attract funding to do what they want to do, someone who�s gonna create the opportunities that they want to go into.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, it�s an area that�s evolving and I think a lot of the organizations that exist today and I have mentioned as [00:35:30] good places to work and so forth didn�t exist five years ago.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Some of them didn�t exist one year ago.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, some of them didn�t exist one year ago. It�s an area in flux, and I think as with politics, you should be opportunistic about thinking about what�s the right place for you to be in. Yeah, I think it�s a very exciting area to work in and I think that a lot of people will find that it�s an area that would benefit from [00:36:00] their background.\\n',\n",
       " '\\n',\n",
       " 'Robert W: If you had someone that could go into any organization or role and that�d be a good fit, what kind of person would you think is most valuable out of everything.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Most valuable person. I don�t really have an answer for that.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You don�t have an answer for that.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Is there any, say, vacancy out there that you just wish that someone could fill it?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: One vacancy that comes to mind is for the policy researcher [00:36:30] position at DeepMind. They�re currently hiring for someone to specifically look at AI policy. I think the case is pretty clear that that would be a good place to work and a good role to have if you�re interested in influencing the conversation around AI policy and being exposed to the latest developments. Yeah, if you�d be a good candidate for that and you�re interested in that, you should definitely apply.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Sometimes I find people who want to work on AI and policy because they don�t see themselves as [00:37:00] quite cut out for technical research, perhaps because their math skills aren�t quite good enough. Is that a sensible way to go, or is in some ways the AI strategy work harder just because it�s less clear, it�s less precise than technical work?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I wouldn�t say it�s harder or easier. It�s just different. Even on the technical side, there�s a lot of fuzziness. AI safety wasn�t very crisply defined a few years ago, and it�s starting to move more in the direction [00:37:30] of technical rigor. I think the same thing will happen to AI policy to some extent, but there will also always be some element of relationship building and qualitative analysis and so forth. That certainly is not exactly the same as solving technical problems. Yes, I do think that having a different background, it would make sense to choose different areas to work with based on your background, but I wouldn�t say that working [00:38:00] on the safety problem is totally technical and the policy problem is totally non-technical, but certainly there�s some correlation there.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kind of level of technical understanding do you have to get to in order to be able to make a useful contribution?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I�m not sure that you need any specific level in order to make some contribution. For example, there are a lot of roles that can be played by people who are just good at distilling the literature on a particular topic. For example, if you�re interested [00:38:30] in understanding the role that AI could play in authoritarian states, then you don�t necessarily have to have any technical background to read up on the literature around surveillance and coercion and things like that. I think there�s a lot of room for good synthesizers and people who are curious and interested to dip in areas they�re unfamiliar with.\\n',\n",
       " '\\n',\n",
       " 'I think there�s also a fairly high ceiling for what would be useful. I think it would also be [00:39:00] good to have people with very strong technical backgrounds thinking about the nitty gritty game theory issues related to arms races and so forth. I think it�s a pretty broad range of possible skill sets that would be useful.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You�re thinking if you can just understand what, you can speculate about what an AI might be capable of doing in the future. Then if you know economics or if you know social science, then you can think, what implications would that have [00:39:30] for politics and that kind of thing, even if you don�t understand specifically how the AI would work.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. Ideally, you know a little bit about each of those things, but you don�t need to be an expert in everything. It�s impossible to be an expert in everything. It�s not feasible to learn about every single discipline, of which I�ll list several in this policy career guide that you mentioned. Clearly, you cannot be a deep expert in [00:40:00] technical AI, economics, sociology, political science, etc., but knowing a little bit and being able to collaborate with people with complementary skill sets is also very valuable. I think a lot of the most exciting work in the next few years will involve collaboration between people on the technical side and on the policy side.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How can someone tell which kind of paths they�re most suited for?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I don�t think that there�s any clear algorithm for [00:40:30] this, but talking to people in the area and reading some of the literature and thinking about where you think the gaps are, what areas you think that you could shed light on given your background is definitely something you should do. You should look at what sorts of job opportunities are available and cast a fairly broad net because you don�t necessarily know what�s going to strike your interest until you stumble upon that opportunity, and networking is also super helpful. [00:41:00] I think not just in terms of finding out about job opportunities and figuring out where there�s a nice fit, but also finding out about different organizational cultures and where you might fit in is also useful. I don�t think there�s a single algorithm, but networking and reading more of the literature and looking at job postings are all good things to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of places can people network? Are there any events that are open to people who don�t yet have roles?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: [00:41:30] All of the AI conferences are open to anyone who is capable of paying for the trip and the ticket. Some examples of big AI conferences are NIPS, ICML, IJCAI, AAAI, and so forth. NIPS is probably the biggest [00:42:00] machine learning conference right now with an emphasis on deep learning. ICML is also a very large one. Actually, I�m not sure which one of those is bigger. I haven�t been to ICML, but I�ve been to NIPS a few times. ICLR is also a deep learning specific conference. I think going to conferences like that and specifically going to workshops and symposia that are relevant to policy questions, which often happens at these conferences.\\n',\n",
       " '\\n',\n",
       " 'For example, at NIPS last year, there was [00:42:30] a symposium on the AI and the law. There are opportunities like that to network and find out more about how people in that area are thinking and to meet people who have similar interests. That�s just on the AI side. It�s also we�re thinking about more policy oriented conferences. There�s some conferences like the Governance of Emerging Technologies Conference, the We [00:43:00] Robot conference series, and probably a few others that would be of interest to people who are either on the technical side and want to move in a more thinking about policy side, or just they�re in the policy side but they want to specifically seek out people interested in AI. Those would be good places to look.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Are any of the organizations involved open enough that someone who�s interested in the area can drop by and get to know people?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. I think [00:43:30] particularly on the academic side, though to some extent also at corporations, there�s a fair amount of openness. For example, at FHI, we host a lot of visitors who just want to learn more about our work. The same is true of other organizations I�m familiar with.\\n',\n",
       " '\\n',\n",
       " 'Robert W: It sounded like in your guide, you thought the best undergraduate major was something like combining a technical subject, like maths or [00:44:00] computer science with a more social science topic like politics or economics. Assuming that post people listening to this have already chosen their major, what should people do at the postgraduate level? Should they do a PhD or just go and work directly at Google if they can?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I�ve probably said this a million times, but I don�t think there�s a clear answer for everyone. [00:44:30] Certainly it doesn�t hurt to have an advanced degree. Of course there are opportunity costs, but there�s certainly more job opportunities available for someone who has a PhD. For example, I don�t know if it�s a hard requirement, but certainly it would be a benefit for people applying to a fair number of industry jobs if they had an advanced degree, as well as in academia. It�s hard to avoid the fact that for academic jobs, as much as [00:45:00] we would like to hire people solely based on their demonstrated skill, it�s often also the case that there are university policies surrounding what sorts of degrees you need for different roles and pay grades and so forth.\\n',\n",
       " '\\n',\n",
       " 'There�s definitely value to having a graduate degree, despite the non-trivial opportunity cost if you have an opportunity. As with all these things, it depends on someone�s background and if you have a clear opportunity to make an impact, and [00:45:30] you could perhaps get some of the best of both worlds by also taking classes online or enrolling at a local university and being a part time PhD student or master�s student. There might be ways to have your cake and eat it too, but it would depend on the particular person.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I guess the most obvious options at the postgraduate level are things like machine learning or computer science or economics or I guess public policy? Is that useful?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: [00:46:00] Just to give a particular example, at Oxford we have a handful of people who are interested in AI safety and AI policy, as well as broader issues related to the future. Just at the organization that I�m at, FHI, there are people who either are currently enrolled or who soon will be enrolled in programs in mine, which is human and social dimensions of science and technology, as well as machine learning, cybersecurity, and zoology. There�s [00:46:30] a wide range of possible areas that one could study and find an advisor that is suitable and interested in supporting interesting work.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Some people worry that doing work in this area could be harmful if not done well. For example, it could be that, as you were saying earlier, that regulation in this area could be done poorly and would actually be harmful overall, or that bringing greater attention to the issue could reinforce the arms race dynamic, or tentatively, having people [00:47:00] get involved who don�t have enough technical expertise could bring the area into disrepute. How seriously do you take those kinds of concerns? Should people be cautious about jumping into the area if they don�t feel like they�re really on top of things?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think that is a reasonable concern, and there is, in fact, some risk of discrediting yourself or your cause by being too alarmist or whatever the particular [00:47:30] charge might be. On the other hand, there�s a lot of noise in the system no matter what. Whatever you do in the near term, there will be Terminator pictures and people who dismiss AI safety and all sorts of other things you can�t really control. I think they�re also risks of not acting, which is not being able to help out when there�s an important program and you have something to contribute. I would say if that�s something that concerns you, then talk to people about your beliefs and [00:48:00] what your beliefs are, and look at surveys of experts and see what the reasonable distribution is among people who have studied the topic more. I don�t think it�s an all or nothing sort of thing where you either have to stay ignorant or be a super expert. I think there�s a middle ground where you develop your confidence little by little as you go, and try and remain as modest as possible in areas that you have little expertise.\\n',\n",
       " '\\n',\n",
       " 'Robert W: [00:48:30] I suppose you could always track the things that people like you or Nick Bostrom are saying and how you speak about the issues, so that you don�t come across as alarmist and you have a measured and precise tone.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Right, right.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You mentioned earlier that AI could potentially be used in military applications, and that�s one way that things could go badly. Given that, do you see much value in people going into military or intelligence roles with the hope of limiting this in the future?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Possibly. [00:49:00] It�s really hard to say. It would depend on the particular role, so probably a role higher up in the chain and more on the policy and strategy sides of things, as opposed to the more operational or logistical or just execution side of things would be more likely to give you an opportunity to affect those sorts of things. There might also be benefits to that beyond just having a direct impact on the issue while in that [00:49:30] role. You could also then develop valuable connections and know the right people to influence from the outside down the road or have a better understanding of the dynamics of the problem and what needs to be done to solve it. I definitely wouldn�t dismiss it, but I also wouldn�t necessarily recommend it across the board as the best thing to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I guess the intelligence services and military are just such enormous bureaucracies, it�s just unlikely [00:50:00] perhaps that you would be in the right place at the right time, or have enough discretion in those roles to be able to make a difference. As you say, it could help you to build up your career capital so you can get other good, useful roles in the future, and you know how the system works, so you could potentially influence it from outside and talk to the right people.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, exactly. Again, it would depend on the particular person and what other options they have, but it�s certainly worth considering.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What about foreign policy? Could it be useful to go into Department of State or something like that and work on arms [00:50:30] control agreements? Even just current arms control agreements so you understand how they work and can think about how they might work for AI in future.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think that could be very useful. Looking at arms control and foreign policy, both from an academic perspective and from a practitioner perspective, I think, is very valuable, and it�s something that�s not our strong suit in the AI safety and policy community right now. More could be done on that front, but I think some [00:51:00] of the same caveats apply as to the military and intelligence side of things, that the State Department, for example, is a very large bureaucracy.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How do you feel about more general attempts to make the world better, such that when AI is developed, things are more likely to go well? For example, just improving the quality of government in general, trying to get the right kinds of people elected, or improving our ability to forecast the future across the board. I guess one worry would be that that�s just not [00:51:30] sufficiently leveraged on this specific problem, and if you think AI is really likely to be the key issue, then you want to work on that specifically. Maybe if you could improve our ability to forecast technological changes in the future across the board, then that could be a better option. Also, you just have a lot more options if you�re open to broader ways of improving the world.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I don�t think there�s a clear decision for everyone, but both at the group level and at the individual level, it often makes sense to take a sum it up a portfolio [00:52:00] approach. There are people at FHI who also are affiliated with the Center for Effective Altruism, and involved in building the EA community like Owen Cotton-Barratt and Toby Ord. There are people who straddle that boundary and look for opportunities to benefit both sides, and there are also people who focus on one thing or the other. I don�t think there�s a clear right answer, but certainly the sorts of trade offs you mention sound reasonable.\\n',\n",
       " '\\n',\n",
       " 'On the one hand, one would want [00:52:30] the world to be in a better place, for there to be more people who are thinking carefully about making good decisions and benefiting the future people in various important decision making roles. It�s quite uncertain what impact you could have on AI, and likewise, even if you work directly on AI, it�s not totally clear that that�s always the best thing you should do. I have a slight bias towards thinking that direct work needs [00:53:00] more attention at the moment, but I think that you should take that with a grain of salt, because I would like more people to help solve the specific problems I�m working on.\\n',\n",
       " '\\n',\n",
       " 'Robert W: We should potentially continue trying to get people involved. We�ll just become AI researchers right away.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Right.\\n',\n",
       " '\\n',\n",
       " 'Robert W: That�s all of my questions. Was there anything you wanted to add? Any inspiring message?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, I�ll just circle back on what I was saying earlier, [00:53:30] which is that AI safety was in a very nebulous stage of development a few years ago, and it took the work of Nick Bostrom in Superintelligence and Stuart Russell in giving a lot of talks and writing op-eds to call more attention to it and give it more legitimacy, and then subsequent work was done to refine the issue and develop research agendas by people, including the authors of he �Concrete Problems in AI Safety� [00:54:00] paper. Now we have a lot of postdocs and graduate students working specifically on this.\\n',\n",
       " '\\n',\n",
       " 'We have people in industry specifically working on well defined problems in this space, and we have the opportunity to make a similar transition in AI policy over the next few years. We�re [moving] from fairly high level desiderata and problem framings to specific proposals and formal models and good white papers and so forth [00:54:30] over the next few years. It�s an area that would benefit from a lot of people�s expertise. I would definitely encourage people who think they might have some relevant expertise to seriously consider it.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I�ll just reiterate that. Since I wrote our problem profile on positively shaping artificial development, we�ve heard from a lot of people who think that this is the most neglected area within AI safety. I�m very keen to get more people working on these problems and getting more organizations, hiring for them. [00:55:00] If it�s something that you�re interested in, I think you should definitely be looking into it more. We�ll put up links to guides where you can find out more and meet the right people.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Great, thanks for having me Robert.\\n',\n",
       " '\\n',\n",
       " 'Robert W:Yeah, thanks so much and have a great day, and look forward to talking in future.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: All right. Thanks. Bye.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Here at the Centre for Effective Altruism, we�re interested in finding ways to compare what it means to do good, and to figure out which ways of doing good do the most good. We ask questions like, �Which charity should you donate to if you want to help as many people as possible? What should careers should you follow if you want to improve the world? Which cause areas have the largest impact?� These are the sorts of questions that we think it�s really important to get clear so you know how you can make a real difference.\\n',\n",
       " '\\n',\n",
       " 'Robert W: To help us answer some of these questions, we�re joined by Professor David Spiegelhalter, the Winton Professor for the Public Understanding of Risk at Cambridge University. Professor Spiegelhalter spent much of his life trying to improve public understanding of statistics, science, and risk in ordinary life. He regularly appears in the UK media and writes on his blog, Understanding Uncertainty. It�s great to have you with us today, David.\\n',\n",
       " '\\n',\n",
       " 'David S: Hi.\\n',\n",
       " '\\n',\n",
       " 'Robert W: We�ve got so many things we�d like to talk about. We�ll see how far we get, but first, tell us a bit about who you are and your position.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, I�ve got a strange job, really. I�m in the maths department at Cambridge, and I teach statistics to undergraduates, but I�m actually funded by a hedge fund, Winton Capital Management. I�ve received my chair in order to improve the public understanding of statistics and risk. What I have done since I�ve had that job, for the last eight years, is I suppose to try to join the general community of people who are trying to improve the way numbers in particular are discussed in society.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of questions do you research and what outreach do you do to help the public understand numbers and uncertainty better?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, I get asked to do a huge amount of stuff. I suppose helping in various agencies, communicating risk, about cancer risk, about the risk of screening for example, risks and benefits of screening. My background is in medical statistics, so that�s what I get to do quite a lot, but I�ve done TV programs about climate change, I�ve done radio stuff about all sorts of threats to society, from Fukushima and so on. Everything to do with trying to get a handle on what are the important threats to us individually, and how we might go to make comparisons between those.\\n',\n",
       " '\\n',\n",
       " 'I�m very interested in rather than the great global existential risks, I�m more interested in the things that affect us all individually, about how we eat, our exercise, transport, and so on.\\n',\n",
       " '\\n',\n",
       " 'Jess W: One of the things that I find really interesting in this area is that there�s a lot of evidence that people tend to be very poor at estimating lots of different probabilities and so end up overestimating the scale of some problems or risks that they face, and underestimating others. From your experience, what kinds of problems do you think that people tend to be most prone to overestimate or underestimate and get wrong in very harmful ways?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, there�s been a lot of research on this by psychologists as you know, and of course when we talk about people, I always include myself in this. I�m a subject to the winds of my emotional gut reactions as much as anybody else. I�m not making any claim about some superior knowledge and rationality compared with everybody else, but we just know that the way we respond to things, to simplify, we can think in two different ways. We can respond with our guts or we can try to engage our brain and think slowly about stuff. This is so relevant when it comes to risk.\\n',\n",
       " '\\n',\n",
       " 'Particularly this idea that what�s available to us, this availability heuristic is very strong, that what we hear in the news, we hear about Ebola, we hear about terrorism, we hear about the latest threat that might be in what we eat and the way we travel, and we get very concerned about this, whether it�s a plane crash or whatever. Because that�s what�s in the news, that�s what is available to us. That�s what�s so prominent, but of course, so many of these risks are actually very small indeed.\\n',\n",
       " '\\n',\n",
       " 'This is not a threat to us, particularly at all. The things that are much more familiar, and we don�t hear much of, for example, heart disease, cancer, all the sort of stuff that we have get largely amount of that is because of the way we live, our lack of exercise, our crummy diet and so on. Of course, people get a bit bored with that, and don�t get so concerned about it.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Do you feel you�ve had any success improving public understanding of risks and getting people to focus on the stuff that really matters?\\n',\n",
       " '\\n',\n",
       " 'David S: Oh, I don�t know. It�s very [inaudible 00:04:23]. I mean, it�d be great to think so. I�ve been involved in some good projects I�ve been very proud of. For example, the redesign of the cancer screening leaflets in the UK which present the benefits and risks of cancer screening in a very balanced way. They�re hugely innovative in that they don�t actually recommend people go for screening. They just say, �Well, these are the possible benefits, these are the possible harms, make up your own mind.� I believe that�s the right way to go about communicating risk, is not to say, �Oh, you�ve got to watch, you�ve got to watch out, this is terrible.� Say, �Well, if you do this, this might happen, or it might not happen, and weigh it up,� and actually give people credit for some intelligence which I think people basically are. Don�t think people are so stupid.\\n',\n",
       " '\\n',\n",
       " 'What I am proud of is being part of a general community that�s very strong in Britain, to do with public engagement in science, which I�m just a small part of that because it covers material on the radio, stuff on television, stuff in some newspapers, and in various agencies. For example, in Statistics Authority, which is just trying to take a much more critical attitude to the way that numbers and evidence are used in society. I think it works. In Britain, we�re rather good compared with most people about, I don�t know, we don�t have these massive fears of vaccinations and nuclear power, of even GMOs. I think this is a sign that we in this country have developed quite a good public engagement with science community.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, I was actually going to ask you about what potential you think there is for young people who want to have an impact in their career, going into a career path similar to your own and trying to get into public outreach and engagement to have a broader impact, rather than necessarily just doing research and academia. How hard do you think it is to do this kind of thing? It sounds like this is definitely something you wish more academics did work on.\\n',\n",
       " '\\n',\n",
       " 'David S: Absolutely. I mean, I really came to it quite late as a real part of my career. As I said, again, because a philanthropist in a hedge fund provided the funding to be able to do this full time, but I was doing some before as part of my job. I increasingly feel that it�s actually a duty of academics who after all, are publicly funded, for a certain proportion of them to really spend some time on public engagement. It doesn�t suit everybody, it�s not everybody at all, although I think everybody should have a website explaining what they do. They should be also supported and there should be incentives within the career structure for academics to do this.\\n',\n",
       " '\\n',\n",
       " 'I�m quite pleased, in Cambridge for example, when we�re looking at promotion of people, their public engagement is taken very seriously indeed. It is something I strongly support. It�s not everybody, but I think it is a very important for academics to do, as they�re doing their work, in whatever area, it has a relevance to society and it can potentially improve society. They should be working on it. I personally think statistics is a particularly important area for improvement of society. I feel this very strongly, many statisticians do. In terms of public engagement, the Royal Statistical Society has now got an initiative of training up statistics ambassadors, young statisticians who want to do this, who really want to get out there and communicate the importance of their work and try to improve the way things are done with numbers and evidence. I think this is so exciting, such fun.\\n',\n",
       " '\\n',\n",
       " 'Robert W: A lot of young people we meet are thinking of going into academia. Do you feel like that�s a good place to be, to have a big impact in the world? Of course, there are also some drawbacks that you might be aware of from your experience of being in academia?\\n',\n",
       " '\\n',\n",
       " 'David S: Yes, yeah. I wouldn�t necessarily say to recommend, �Yes, you really should go into academia.� It doesn�t suit everybody and frankly, [inaudible 00:08:17] lifestyle is it [inaudible 00:08:18]. It�s very tough now. In most careers, there seems to be a lot more pressures on than there used to be, so it can be quite a tough call, I think. But there is potential to do a lot of good stuff and people I work with and know in other areas, whether they�re working on natural threats, natural disasters, whether they�re working in botany, whether they�re working in any area, plant sciences, whatever, are really dedicated to trying to improve a lot of the world. I�m so impressed by the dedication.\\n',\n",
       " '\\n',\n",
       " 'But it is a tough job, and you don�t necessarily see in the push to publish the work, you�ve got to do all this, you�ve got to go through all this business in order to build one�s career. It doesn�t necessarily appear at first that you�re actually doing great job in what you do. Very little of it you can see a direct impact. However, as I know what�s very good about it, is it�s building a lot of transferable skills that you can use. I find that in my statistical skills for example, are in massive demands, by all sorts of agencies. I could point in particular to the importance of the millennial development goals and the sustainable development goals which are replacing them, in the monitoring of the state of welfare in different countries around the world has become absolutely vital.\\n',\n",
       " '\\n',\n",
       " 'Just being able to measure things and know what�s going on and to develop good ways to do surveys, to actually look at what works and how you can improve these measures has become enormously vital, enormously important. I think, well again, I�m coming back to � I think statistics is a particularly valuable area to go into.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, absolutely, and just a little bit more broadly, the key idea behind effective altruism is that we want to try and figure out which altruistic activities, which charities, which career paths, which causes do the most good. We think that being able to measure things, or at least trying to measure things is incredibly valuable in making progress on those questions. But obviously many of these questions are incredibly difficult to answer, so a common criticism that we often get is people saying something like, �Oh, it�s impossible to figure out which charities or careers do the most good so you�re wasting your time trying, and you should just go with your gut or help personal causes.� What do you think of this objection and do you think there�s anything to it? Concern about there being too much uncertainty.\\n',\n",
       " '\\n',\n",
       " 'We often respond by saying that it doesn�t mean that we shouldn�t try and estimating this clearly the best we have, but what are your thoughts on this challenge and how to respond to it?\\n',\n",
       " '\\n',\n",
       " 'David S: I mean, this is not a comment that�s just restricted to the area that you�re talking about, it happens all the time. It used to happen in healthcare. People used to say, �Oh, you can�t put a value on a human life. We�ve just go to do all we can to do good and we have to just go with our guts, essentially.� No, that view now, certainly in the UK, has been completely discredited. You can put a value on a human life. We do all the time.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I�m not sure everyone would agree.\\n',\n",
       " '\\n',\n",
       " 'David S: Not sure everyone can agree, and of course �\\n',\n",
       " '\\n',\n",
       " 'Jess W: But you have to.\\n',\n",
       " '\\n',\n",
       " 'David S: � But NICE, for example, has been doing this for over a decade, putting the value on the marginal benefit of particular healthcare intervention and assessing what should be supported under National Health Service according to reach all the criteria. Now, we know this is not perfect. We know this does not measure everything, but it�s explicit, it�s transparent, and they�ve done their best, and I think it�s a massive success, and a huge example of other countries, how you can go about it. Now, we know it�s never perfect, it�s never perfect, but because people then trying to take that approach which I think has been extremely beneficial in healthcare, and move it into other areas.\\n',\n",
       " '\\n',\n",
       " 'Now, how can you do it � So people are trying to do it with the environment. �How can we measure the value of a tree?� Well, people are trying to measure the value of a tree, and I know what the value of a tree�s worth, and it�s quite a lot. But working out all these different, trying to measure the benefit of a or how [inaudible 00:12:31] sustainable environment, et cetera. What�s the value of a species? What�s the value of this? I mean, it seems it�s too easy to sit back and say, �Oh, you can�t do it. You can�t measure that.� Well, you can have a good go, you can have a good go. As you say, it�s always a balance between trying your best, realizing you�re never going to be able to do it, but not to be put off having a good go.\\n',\n",
       " '\\n',\n",
       " 'It�s like a lot of things we do in statistics, trying to measure things, trying to model things, that we always know what we do is inadequate, but that doesn�t mean we can�t do useful things.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Can you tell us a bit more about NICE and how they prioritize health treatments within the United Kingdom?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. I mean, that�s something I�ve worked on, a medical statistician. I�ve been a huge supporter of NICE, and essentially what they do is when they decide whether to have a new vaccine or whether decide to have a new recommend for drugs to be paid for under the NHS, for example, they will look at what�s the expected benefit of that intervention, and what�s the expected cost. Then, they look at how much it�s going to cost to provide an extra, or they call it �QALY,� a quality adjusted life year. One year life, it�s discounted if it�s poor quality, you will discount it, it won�t be worth a whole year.\\n',\n",
       " '\\n',\n",
       " 'Then, essentially they can make the comparison and they look, if something comes in at least than 20,000 pounds of QALY, just paid for, just, �Yep, fine, we�ll pay for it under the NHS.� If it�s more than about 30,000, then they really try to say, �Well, we don�t want to pay for this� and try to go back to the drug company to renegotiate a price, for example. Between 20,000, and 30,000, well, that�s more in the gray zone. This has been enormously beneficial, partly in order to go back to drug companies and get them to reduce their prices, and also to see that some interventions, for example, cancer screening, come in extremely cheap. They�re just worth doing.\\n',\n",
       " '\\n',\n",
       " 'It also means you have to be explicit, for example, in how much you value the future rather than the present, because they put in a discount rate, currently three and a half percent. Now, that means that a year of life in 25 years� time, for example, is only worth about half what a year of life now is worth. That�s taken into account. Now, that of course is controversial and if we were talking about big policy decisions, for example about climate change, that�s far too big a discount rate. We wouldn�t care less about the world in 100 years time if we used that discount rate, so in different areas you might be wanting to use a different discount rate. But these are taking economic ideas and moving them into very human decisions, and I think they are enormously helpful and illuminating, but they�re never perfect.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. I�ve actually written an article with Professor Toby Ord about why we shouldn�t discount health, even though we should discount financial returns, about why\\n',\n",
       " 'it�s appropriate in one case and not in the other. We�ll put up a link to that on the website. A lot of people who are planning out their careers, they feel just overwhelmed by uncertainty. They don�t know how much they�re going to earn in one career or what�s the chances of getting into academia, or being elected to parliament. How do you think people ought to make decisions when they�re just surrounded by so much uncertainty?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, I mean you could say we�re always surrounded about � This again, this is common to everything we do. We never know what�s going to happen in the future, which is great. People don�t want to know what�s going to happen in the future. People don�t even know what they�re going to get for Christmas, let alone what�s going to happen for the rest of their lives. So, this is just a common problem, and we need to distinguish what we might call risk and uncertainty.\\n',\n",
       " '\\n',\n",
       " 'The risk is when it�s a very well defined problem, there�s short-term issues and buying lottery tickets. You know what the chances are, and then we can take a rational approach. When we�re dealing with deeper uncertainty, when really are not even sure what the options are, of how our lives might develop, then it�s very difficult to take a completely formal approach. I wouldn�t try to say, �Oh, we should be able to do all this mathematically� at all, however, the basics of qualitive ideas, of thinking through a rational decision are still very valuable. Because you think of, �What are the options available? What are the possibilities? What are the possible consequences of what I might do?�\\n',\n",
       " '\\n',\n",
       " 'Now, that�s always inadequate. You�re never going to be think about the thing, and however, by thinking through that, some things might come immediately apparent, some immediate rankings of what is preferable and not preferable might become apparent. Otherwise, you might need to fall back and think on broader strategies for making decisions in the face of what we call �deeper uncertainty.� Now, this is a deeply contested area. When governments make policies, very often they�re in these situations where they can�t even think of all the things that might happen, so they�re in the situation of deeper uncertainty and people have tested various ways to go about it.\\n',\n",
       " '\\n',\n",
       " 'The first thing is to not to think that you can optimize. You can�t be perfect, you cannot be perfect. The sort of suggestions people make in terms of general decision-making in these contexts are to do with flexibility and resilience. You don�t want to commit yourself to something you�re not going to be able to change because you can�t predict everything that�s going to happen. You want to build in flexibility and adaptivity to your decision, and that�s the same in any business, any government, any project at all. The other thing is building resilience in which you are essentially can cope with the unexpected and things.\\n',\n",
       " '\\n',\n",
       " 'That means both the possibility of really good things happening as well as the possibility of really bad things happening, that you are making sure that you haven�t put all your eggs in one basket, essentially. So if something goes wrong, you haven�t completely gone up a total blind alley that you can�t renegotiate, which means operating from a most robust basis as possible. It means not trying to optimize the single path, but building in robustness, which means I think in terms of career of course, building in transferable skills you�re going to be able to use in a variety of circumstances. I think these are standard tactics to respond to deeper uncertainty, which can be used in any situation.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, that�s definitely something that the organization [inaudible 00:18:46] does now, is to advise people on how to have a big impact with their careers really focus on is especially earlier in your career, keep your options open, choose things that are robust under lots of different possible things happening. One other question that we find we come up against a fair amount in thinking about careers and charities and all kinds of things is how do you go about choosing between a small chance of some really huge outcome, like saving the world from a pandemic that kills millions, against some high probability of a very moderate or a more moderate outcome, helping people in your community?\\n',\n",
       " '\\n',\n",
       " 'This seems like a very difficult thing to deal with. Do you have any thoughts on how people can go about making these kinds of decisions?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, very difficult but again, these are common decisions always in our lives. Do we go for the high risk option or the safer option? I personally think that actually, a mixed strategy is something that businesses I think would recommend. Although, that�s not what I do. I�m not involved in any investment decisions, but often I think people take an idea of portfolio of risk taking, because these are all risks. You can�t guarantee with anything that you do. These are all risks, and so you might have a certain amount invested in rather safe things with a safe return, you�re pretty sure that what you�re going to do is going to have a reasonable return, and that you really can predict the consequences of what you�re using your career or your money for, what you�re getting for.\\n',\n",
       " '\\n',\n",
       " 'On the other hand, on more speculative, where you might be supporting something that actually might not come to anything. Either because the solution it�s promising won�t work or the threat won�t arise in the first place. I think actually a mixed strategy in these circumstances, again, I wouldn�t want to just put all the eggs in one basket. I think it wouldn�t be appropriate. I also personally feel that � This is completely my personal reaction that I give, is that I quite like a mixed strategy in which some things are the big [crosstalk 00:20:54]. You might slowly anonymous, where you�re giving money to large organizations that�s doing something which is contributing in a probably fairly predictable way to improving things with people, versus the more personal ones where you do have the personal contact.\\n',\n",
       " '\\n',\n",
       " 'You might get slightly more emotional feedback from them, partly because they might encourage you to perhaps take on a more � Feedback from the altruism might encourage you to take on a bigger [inaudible 00:21:24], a bigger commitment in the future.\\n',\n",
       " '\\n',\n",
       " 'Robert W: There are various ways of thinking about how to quantify the risks or possible gains associated with an activity. Can you explain what a micromort or a microlife is?\\n',\n",
       " '\\n',\n",
       " 'David S: Oh yeah, yeah. These are units that � Well, we invented the microlife. Someone else invented the micromort. It�s just a way of trying to get a common scale, particularly because a micromort is acute risks, things that might kill you on the spot, but otherwise you�re healthy. It�s a one in a million chance of dying because of an action you might take. Skydiving or something is approximately 7-10 micromorts. Microlife, which is something to do with chronic risks, which is the risk of � I think [inaudible 00:22:07] quite interesting, which is bad lifestyle. Lack of exercise, or smoking, or drinking, or bad diet, whatever, which isn�t going to kill you on the spot but is likely to shorten your life. Not definitely at all. Maybe not, but probably will.\\n',\n",
       " '\\n',\n",
       " 'It�s a reduction of half an hour in your life expectancy, and I find these quite useful, because it enables you to compare all these different things like diet and smoking and exercise and things, and makes me for example, obviously smoking is where you�re the biggest return or the biggest in terms of not smoking, the biggest return. Drinking at low doses, well actually, it doesn�t make much difference. [inaudible 00:22:53] it�s a rather huge concern, unless you�re really swigging it down. But exercise, again, I think is extremely important, and diet�s important. It just gives you a feeling of where the priorities are, which of course, it�s all a bit obvious. They�re the priorities that people do in public health, but it makes slightly unconcerned about other things that people might be really concerned about.\\n',\n",
       " '\\n',\n",
       " 'Robert W: These concepts were invented to help people get a better understanding and make better decisions. Do you think that we could invent new concepts to help people think about how they can do more good in the world?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. A microaltru or something. It�s difficult. I mean, a QALY is a good one. If your area of cause is to do with saving lives, improving health, lethal risk to people, then QALY�s a good one. It�s already established, you can look at how many years of life are you going to gain from your intervention, and we know that if you�re giving money to some particular charities, which I know that you promote, they�re extremely cost effective in that way. It�s more difficult if you�re the broader idea, for example, of supporting education, preserving the environment, the actual environment, and so on. Quite hard to put a number on that, within a particular area of cause, then you might be able to do it, but having metrics that go right across different areas is difficult.\\n',\n",
       " '\\n',\n",
       " 'I know that people criticize the attempts to do it. I still think it�s worth trying to do, and people struggling to do this within the environment, and so on. I don�t think I�ve got any great idea to be honest. I mean, one possibility of course is looking at general measures of wellbeing, which again, people are doing. It�s measured by national governments now to try to measure wellbeing, in terms of how people feel about their lives. General wellbeing indices which are not just health related, or length of life related, I think have got great potential. You�re not going to be able to get the perfect measure on any of them.\\n',\n",
       " '\\n',\n",
       " 'Robert W: People often ask, �How would you rate how good your life is going from 1 to 10?� Typically they give answers between six and eight, at least in the UK.\\n',\n",
       " '\\n',\n",
       " 'David S: [crosstalk 00:25:27].\\n',\n",
       " '\\n',\n",
       " 'Robert W: Maybe we could have a micro happy, which would be something as good as moving someone from six to seven, or seven to eight on this welfare scale.\\n',\n",
       " '\\n',\n",
       " 'David S: Yes, exactly. I think those scales, well actually, I find them very revealing when people do mention them, particularly across cultures, as you said. Everyone says seven, [inaudible 00:25:49]. Maybe because they like the number seven, but it also reflects this idea, �Well, it�s not bad. Mustn�t grumble. Could be better, but mustn�t grumble� so slightly British, stoic attitude. I think it�s almost predictable what people are going to respond, but I find, for example, the age distribution of these responses and how the age distribution varies across cultures extremely interesting.\\n',\n",
       " '\\n',\n",
       " 'The idea of whether one could move groups of people on that scale would be very important. When I think of charities, and some of the ones I give to, which are not directly I don�t think any of them actually make people live longer, but would hope to improve people�s wellbeing in the sense of happiness considerably.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah, it is a fascinating area of research, and there are significant international differences. I think in the UK and America and Australia, people do tend to give seven or maybe eight if they�re feeling particularly good, but in South America, people are just for some reason, more exuberant and tend to often give eights, even when they�re relatively poor. By contrast, in Eastern Europe and Eastern Asia, like Japan and China, people tend to give relatively low scores relative to how their life seems to be going from the outside.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. No, no, people in different cultures, I think in Africa they�re even lower. People are very �\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. There are some countries where the average is as low as 4 out of 10, which maybe half of people are giving less than four which is a bit unfortunate.\\n',\n",
       " '\\n',\n",
       " 'David S: Again, I mean, they�re very culturally, I think, specific. But I think what I see is relative changes perhaps compared with the average in that community that one would [inaudible 00:27:41] for. You can�t just get everyone up to eight or whatever. I mean, there are very different cultural attitudes to do with, I think, that are reflected in those questions.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. By trying to estimate risks or the size of problems in this quantitative way, do you think we risk being biased against approaches that are harder to quantify?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, it always is a problem that people start believing the model, the measure, which is only a very inadequate thing. Starts becoming the thing that you are in fact then trying to, you�re focused on to the detriment of all others. There�s a real problem with over metricizing any activity, pretending that you can measure the benefits of everything. The academic world, of course, is a real example of that where people start getting obsessed with impact factors and nonsense like that. There�s a real danger with believing these things too much\\n',\n",
       " '\\n',\n",
       " 'Also in time, I think we should have a damn good go at trying to do it. Just because something is difficult and cannot be done perfectly doesn�t mean that you can�t at least have a stab at it, because you may be � Often these things can get really clear around things. Great precision might not be required in order to determine that actually this is not an effective use of the sources compared with [inaudible 00:29:07].\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah and I think that rather giving us a perfect answer to all questions using models and looking at different factors, can as you say just help you to identify clear differences or areas where you need to get more information because you just have no idea to [fight 00:29:23] all that in or something, so it can definitely be very useful. Finally, just to what extent that the average person can benefit from a better understanding of statistics, learning more statistics? Either in their everyday lives, or in being more effective as altruists. To what extent do you think, I guess, relating back to the past question, it could potentially limit us?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, I think of course I�d say this, wouldn�t I? But yeah, I do believe that people would benefit from having a greater idea of stats and measurement, and number, and quantity, and the frailties of that as well. Because the point about understanding statistics is both understanding their strength and their weaknesses. The two are absolutely hand in hand. What people tend to do at the moment, they�re I suppose not confident with numbers and how they are constructed, because they are in their whole, are always constructed. Someone has chosen what to measure, and they tend to either accept them as if they�re God given truth and that�s the number and that�s so vital, and that is it, that�s what we�re looking for, or reject them out of hand. �Oh, you can�t put numbers� � Just damn lies and statistics and, �This is nonsense.�\\n',\n",
       " '\\n',\n",
       " 'Those two extreme views are both equally idiotic. But if you�ve got a slightly more nuanced view, you�ll realize that statistics, numbers, and measurement are an incredibly valuable tool, but they�re just a tool, and they have their frailties and inadequacies. They both need to be able to teach and understand the strength and the limitations. What I�m involved in a number of educational projects in which we�re trying to do just that.\\n',\n",
       " 'Robert W: It�s really a matter of trying to assign the appropriate weight to each different kind of evidence or each different piece of evidence that comes your way?\\n',\n",
       " '\\n',\n",
       " 'David S: I mean, in the end, what statistics is is to do with is quantitative evidence. That�s what we want, what we�re talking about, and evidence can be good, bad, or along a whole scale. It�s not a true or false. It�s just evidence, and we�re trying to weigh up our evidence, which we do all the time in our lives. We do this. Our guts tend to be a bit fallible when it comes to weighing up evidence. We tend to be too influenced by certain salient things that attract our attention. As you�re standing back and trying to be a bit cooler about weighing up evidence, I think it�s a really valuable thing to try to do in all areas of life, it doesn�t matter what.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, absolutely, which is not to say that you should completely discount that gut feeling, but often just supplement it with some extra evidence source statistics.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. I don�t trust them a bit, but I still go with them.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah. I think that�s about all we�ve got time for now, but thanks so much, David. This has been very informative and really interesting.\\n',\n",
       " '\\n',\n",
       " 'David S: Okay, thanks very much.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Right. If you�d like to hear more from Professor Spiegelhalter, then you can find his blog Understanding Uncertainty online, and he�s often on the radio in the UK. In particular, on �More Or Less� on BBC Four.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Hi! I�m Robert Wiblin, Director of Research at 80,000 Hours and welcome to the podcast. If you want to make sure you never miss an episode from us, you could subscribe by searching for 80,000 hours in whatever app you use to get podcasts. That way you can also speed up the episode, which is how I much prefer to listen to interviews. Next week, I�m scheduled to speak with Alex Gordon-Brown about working in quantitative trading in order to and to give, which I expect to be very engaging.\\n',\n",
       " '\\n',\n",
       " 'Today�s conversation really goes into the weeds and I learned a great deal from it. If you�re looking for personal advice on how to pursue a career in technical AI research, stick around because we get to that in the second half. I apologize for the audio quality on my end. I think we�ll have that fixed up by next time. If you�d like to offer any feedback on the podcast, please do email me at rob at 80000hours dot org. We�re still figuring out how we can best use podcasts to help our readers and I�ll try to respond to everyone. Without further ado, here�s my conversation with Dario Amodei.\\n',\n",
       " '\\n',\n",
       " 'Today I�m speaking with Dario Amodei, a research scientist at OpenAI in San Francisco. Prior to working at OpenAI, Dario worked at Google and Baidu and helped to lead the project that developed Deep Speech 2.0 which was named one of 10 breakthrough technologies of 2016 by MIT Technology Review. Dario holds a PhD in Physics from Princeton University where he was awarded the Hertz Foundation Doctoral Thesis prize. Dario is also the co-lead author of the paper, �Concrete Problems in AI Safety�, which lays out in simple terms the problems we face in making AI systems safe today. Thanks for coming on the show, Dario.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Hi.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We plan to talk about the motivations behind technical AI safety research, the Concrete Problems paper, and how someone can pursue a career in it for themselves. First, we�re at the OpenAI office here in SF, tell us a bit about OpenAI and how you ended up actually working here.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: OpenAI is a non-profit AI research lab. It was originally founded by Elon Musk, Sam Altman and a few other folks. Generally, we�re working on following the gradient to a more general artificial intelligence and making it safe. I joined around, what was it? July of last year, so about a year ago which was a few months after it started. I came here because there were a number of � I thought there were a number of really talented researchers here and it was a good environment in which to think about safety in the context of AI research that�s already being done.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI was only founded about 18 months ago? Is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It was about 18 months ago, yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How many staff does it have now?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think there�s, last I counted, about 55 people here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Has it been difficult hiring that many people that quickly?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�ve actually never worked at a startup before. Our CTO, Greg Brockman, was previously CTO of a startup called Stripe, which now has around a thousand people or so. It�s definitely hard. He�s really good. It is not something that I�ve been super involved in except on the safety side.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess it�s the Bay Area way to explore some growth in organizations. Who�s backing it? What�s the budget like?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t know if I can give exact numbers in the budget. The main donors at this point are Elon Musk, Sam Altman and Dustin Moskovitz through Good Ventures [00:03:27].\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s what you do pretty similar to what�s going on, at DeepMind? Are there important differences?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I would say that the general research agenda at OpenAI and it�s focused on reinforcement learning and in learning across many environments and trying to push forward the boundaries of what�s done instead of just focusing on supervised machine learning. I would say that�s very similar to DeepMind and probably it�s one thing that sets OpenAI and DeepMind apart from other institutions. We both have a similar focus on safety. We both have safety teams. I would say OpenAI is trying to be a smaller organization that focuses on hiring just the people that we want the most. That�s been one of the big difference. There�s probably some differences in culture as well that are a little bit intangible and hard to describe. I think generally our view of AI works and what to build in AI and the focus on safety are actually pretty similar between the two organizations.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You studied Physics, right? Had a PhD in Physics? Then you switched into AI, or was your Physics-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My Physics work was, in particular, specialized in biophysics. I was thinking about models coming from statistical physics and applying them to models in the brain. Then also using the techniques from Physics and electronics to make measurements to try and validate those models. I come from a Physics background but I�ve been thinking about intelligence for quite a while and how intelligence worked. I think, when I get my PhD, I wanted to understand that by understanding the brain. By the time I was done with it and by the time I did a short postdoc, AI was starting to get to the point where it was really working in a way that it hadn�t worked when I started my PhD.\\n',\n",
       " '\\n',\n",
       " 'I felt like maybe it was starting communication the best way to understand intelligence would be to actually directly work on building parts of it rather than studying the messiness of the brain. That was what led to that switch.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you want to give a quick pitch for why, what kind of artificial intelligence is so important?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think you can give a standard argument that though a lot of people are familiar with which is that if you think about any technology that humans have created, what�s allowed us to create that technology since the sanitation, flight, medicine, improvements in human health, improvements in the ability to feed the world. All this has been generated by our intelligence. Our intelligence is relatively fixed.\\n',\n",
       " '\\n',\n",
       " 'If we�re able to build something that was able to match or exceed our intelligence, then that would really be increasing the engine, produces a lot of the great things that we do and ultimately maybe I�m, maybe it would take a long time, would give us a much more complete control over our own biology and neuroscience could make us whoever and whatever we want to be, could end conflict, war or diseases, that stuff. That sounds a little utopian but I think if we push this technology far enough and all goes well, then that will lead to a result either immediately when we build it or over a somewhat longer period of time. I don�t see any reason why those things can�t happen. I think that�s the basic reason to work on AI.\\n',\n",
       " '\\n',\n",
       " 'As I�ve written, there are these safety issues where we can imagine situations in which it doesn�t actually go well to the extent that that�s a risk, that�s also risk that we can reduce. We can also have leverage by focusing particularly on reducing that risk. On both the positive side and the negative side, it seems like a-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Quite a lot of leverage.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, there�s a huge amount of leverage to be had. The previous stuff I was doing was in biology. It�s great, you can help people, you can try and try and cure some disease but this feels like it�s more getting to the root of problems.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What does the name �OpenAI� mean? Does that relate to the parts the organization is taking?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I wasn�t actually present at the time that OpenAI was founded or the name was chosen. I wasn�t the one who picked the name. I think there�s been fair amount of misunderstanding. I think there�s one group of people who think it�s all about open source, and releasing open tools. There�s another set of people who, I don�t think many people think this anymore but who for a while thought that it was about making an AGI without any safety precautions and just giving a copy to everyone and that this would somehow solve safety problems. These were too early misconceptions that were around long before I joined OpenAI. My understanding is that it�s meant to indicate the idea that OpenAI wants the benefits of AI technology to be widely distributed. Assuming-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Rather than only going to the owners.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Assuming that safety and control problems are solved and we build AGI, there�s then a question about who owns it, what happens with it, what world do we live in after it�s created. Again, this is � I wasn�t the one who named this or set the specific mission statement but I think Elon�s intention with it was trying to think ahead given that we built an AGI and it�s not wildly unsafe how were its benefits distributed throughout humanity. I think openness is intended to indicate the idea that these benefits should accrue to everyone. That�s my understanding.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI is a non-profit.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It is a non-profit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If you developed a really profitable AI, how does that work? OpenAI become incredibly rich and then it gives out the money to everyone?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Personally, I�ve no interest in getting rich from AGI. I think it would do so many interesting and wonderful things to humanity that I think-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The question of getting a larger share is silly.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: The meaning of money would change quite a lot and even maybe the psychological motivations that would want me to get a larger share or things I could change and my want to change. In many ways, shares in terms of money are maybe not the right way to think about it but I think there�s all kinds of stuff that could happen when AGI happens. Some of the things I think about are where that could go or what that could mean. The summary is we don�t know very much because it�s something we haven�t done yet. A lot of it�s speculation.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What do you research here at OpenAI?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I mainly work on safety. We have a safety team that�s so far, myself and Paul Christiano. Paul was co-author of mine on the Concrete Problems paper and has also written a lot online on his blog about AI. He�s probably one of the people who�s I think done the most to promote clear thinking about the problem and tie it to current AI. We have a third person joining in a few weeks who I�m super excited about. We�re trying to build up a team that focuses on technical safety. We also do a little bit of strategy stuff which is how do we get different organizations that are working on AI to cooperate with each other, how do we cooperate with policy makers on questions like these. We�re also thinking a little bit about those issues but mainly technical safety.\\n',\n",
       " '\\n',\n",
       " 'I also do some stuff that�s not strictly technical safety but is generally done to stay up to date on where AI is currently going. I did some work on a transfer learning a while ago that was really a little bit safety motivated but trying to make environments that are broad enough that it�s possible to see distributional shift to our distribution problems. That�s the range of stuff I work on.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What was the organizational culture like here? What kind of people does OpenAI attract?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think we�ve generally been very selective in who we pick. Generally it�s people who are very talented machine-learning researchers but also people who, I would say not everyone, but a large faction of people here really do think in terms of eventually getting to AGI. At least some people, a significant fraction are quite interested in or at least supportive of safety work related to that or related to what we do. Now there�s a wide range of beliefs on how to work on safety, how possible it is to work on safety from our current vantage point. There�s a wide distribution of use but broadly people are pretty supportive.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI recently moved away from software development, is that right? To more focus on machine learning?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: That�s not quite right, it�s more � I think what you�re referring to is we had a project called Universe, which actually I was somewhat involved in the machine learning side. The idea of that project was to make a lot of environments that agents could learn using. The way we did was using something called the VNC protocol to connect directly to a browser through pixels and so that would allow you to play thousands of flash games and navigate weba tasks. It turned out to be a case. I was actually really excited about this because I saw this as a test bed to study safety. If you have hundred flash racing games, you can train the agent on one flash racing game and then see how it behaves badly when you transfer it to another flash racing game. You can study some of these open world problems where an agent has a very wide space to explore and a wide range of actions it could take.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: This is one thing that, and our researchers have been working on is teaching computers to play computer games really well like a superhuman level.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this is � DeepMind has worked on this with ATARI games. We were taking it to another level with any game you can find in the internet. This ended up being a project that I think could probably be described as a little bit ahead of its time. It turned out that in order to connect this way, we needed all the different workers who are applying our algorithm to be asynchronous with one another and for reasons that were complicated when we figured out � we only figured out later. Actually such asynchronous communication was really hard to make it play well with ML. it led to a lot of complexity.\\n',\n",
       " '\\n',\n",
       " 'We�re, to some extent, de-emphasizing that project now. We�re actually trying to move to doing the same thing with a more synchronous environment. Basically, the same idea but more in a way that more amenable to ML benchmarking and to measuring how well we�re doing and doesn�t have this kind of hard to interact with property. It�s more like we made a tool and it was a good first attempt that add something ambitious but it wasn�t quite the right tool so now we�re working on changing it to a version that�s better, I wouldn�t say we�ve gone away from software engineering so much as we�ve been experimenting with how to produce tools and takes a few iterations to get that right.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Turning now to the broader issue of superhuman AI development, what do you see as the potential dangers here? Why should anyone be worried about this?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My attitude, to start off with, has always been, although I do think about AGI, which is a term I prefer to use than super intelligence because I think no one knows whether a machine will rocket past human level or not so that�s something that could happen or not. AGI is something that I think definitely will eventually happen. I prefer to talk in terms of that. Even within safety, in Concrete Problems, I explicitly try to think in terms of not how powerful the systems are but conceptually what can go wrong with them.\\n',\n",
       " '\\n',\n",
       " 'The same kind of thing could go wrong with an AGI as could go wrong with a very simple agent playing a video game or robot cleaning your house. If it has the wrong objective function, if you don�t specify its goal correctly, it can do something unpredictable and therefore dangerous. In general, when I talk about safety, I talk about safety generically whether it�s in powerful systems or very weak systems. All that said, with respect to powerful systems in particular, I think there is a possibility that if we either do a bad job specifying the goals of complex systems or just they�re unreliable in the way that self driving cars are unreliable.\\n',\n",
       " '\\n',\n",
       " 'A self-driving car has to have a very high standard of safety in order to trust it to drive on the road. For almost a decade now, we�ve had self-driving cars that are 99.9% safe. But that�s not enough, we need them 99.999% safe. With AGI which is something that � it�s going to take a lot more novel strategies than self-driving cars, the space in which it operates is a lot broader than self-driving cars. If you just transpose that kind of safety testing from self-driving cars to general intelligence, even with all the controls you put on and even with all the safety standards, it�s clear that at the very least we�re going to have a big challenge in making sure that something doesn�t go wrong. If something does go wrong, it would be easy for or it might be easy for a large amount of harm to be done relatively quickly.\\n',\n",
       " '\\n',\n",
       " 'You have your AGI controlling the stock market or the economy or something and it just doesn�t know how to do it very well yet and something goes wrong. It takes a long time to unwind that. There�s a long tail of things of varying degrees of badness that could happen. I think at the extreme end is the Nick Bostrom [00:18:41] style of fear that an AGI could destroy humanity. I can�t see any reason and principle why that couldn�t happen.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If it was sufficiently powerful or sufficiently good at accomplishing its goals.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If it was sufficiently powerful and like safety had been handled sufficiently badly, that is definitely something that can happen. I think there are folks at places like MIRI who say that this is the default outcome or this is likely to happen or there�s almost no way to avoid it, or you have to solve some incredibly hard math problem to avoid it. I don�t generally agree with any of those things but I think this is a possible outcome and at the very least as a tail risk we should take it seriously. I think another thing I�m worried about is that the wrong, even if we manage to make a super human AI safe or an AGI safe, then it might be used for the wrong ends.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Deliberately.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Deliberately used for the wrong ends by a disturbed individual or an organization whose views are not aligned with humanity or nation state whose views are not aligned with humanity. That�s in my mind, the range of risks.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you think there�s much of a chance that the risks are being overblown here and in fact it�s just going to end up delaying something that could be incredibly useful and make life a lot better?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It may very well turn out. Maybe it�s more than 50% chance that as we get close and closer to AGI then it becomes clear how to make something safe. Maybe it�s just like the goals are specified in a way that�s a very corded off from the task that are done that there are certain problems of nature like scanning brains or something that we need AIs to do for us in order to gain control over our biology or control over resources. Then there are human values and maybe there can be an efficient division of labor where there isn�t much confusion or maybe safety problems are just a bunch of research and � they�re just a corner of machine-learning research where we haven�t done much yet and so we haven�t tried. I can think of lots of ways and maybe it�s even the most probable way where things turn out totally fine. But I wouldn�t-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You don�t want to count on it.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I wouldn�t in any of those cases [00:21:04] say the risk was overblown. It�s like, supposed you have a fire alarm and someone�s cooking a barbecue and it�s smoke, you wouldn�t call like installing the fire alarm overblown. It�s just sometimes you�ll have a fire and sometimes you won�t.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You want to take precautions.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Installing the fire alarm is the right course of action. I think of this as a precaution. I don�t really think of it, I don�t think of anything I do as slowing down the rate of AI progress or at least I�m not trying to do that. I think of it as broadening the scope of AI progress and thinking about AI in a more interaction and human-centered way. If anything maybe it accelerates progress a little bit although that�s probably a minor effect. If people are worried about progress being slowed down, I don�t believe anything I do is close to that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Closing that, yeah. How much of OpenAI�s work is focused on these kinds of problems? It sounded like 5% of the staff but I guess other people are worried about too?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think, broadly, most of people at OpenAI are worried about or at least think these issues are worth thinking about. That�s different from who is actively doing their technical work on it. I would say it�s three or four people now and hoping that that grows somewhat. We�re actively looking for really talented people. I think OpenAI as an institution has the general idea that in order to work on AI safety, you have to be at the forefront of AI. Also if you are at the forefront of AI, you have a better ability to implement AI�s safety in the final system that�s built.\\n',\n",
       " '\\n',\n",
       " 'Many people are interested in safety in the long run but I think until recently and even so now, I think many people here don�t know if there�s a way to work on safety right now. They�re skeptical that you�re able to work on safety right now with concrete work. I�ve been trying to change that with Concrete Problems and with this recent paper that Paul and I wrote on learning complex human preferences. We�re trying to show that there�s concrete work that can be done and that�s had a variety of reactions. Some people are like, yes, this is exactly what you meant with safety work now I see how it can be done. Some people are like, well that�s machine learning work, I don�t actually see how it connects to AGI and so then we�ll try and write another paper and say, �Okay, this is the line we�re drawing and this is how we think it gets us there.� It actually could turn out that this is mostly just ML work and the final systems we build are different enough that for whatever reason this ends up not being relevant to safety.\\n',\n",
       " '\\n',\n",
       " 'Again, I�m pretty happy in that world. If there was nothing concrete it was possible to work on safety and I instead ended up doing a different direction in machine learning, then that ends up being fine. Then it will turn out we couldn�t have worked on safety until later and then we�ll work on safety later. Whereas in the world where it does matter, it�s really great and really impactful to get a headstart on it.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�m curious to get your view on this debate that I�ve seen online. You have this contrast, some people like perhaps Bostron could be a case of this in the book SuperIntelligence of talking as though once we have a super human AI then it will get very much smarter very quickly and it could potentially just solve all of these problems, it could solve aging, like solve all of our health issues. Since the people criticizing this online saying you just think this because you�re a bunch of nerds and you think that thinking is the way to change the world, the way that everything gets done, but it�s not going to be so simple even if you had a very intelligent machine it wouldn�t necessarily be able to solve those problems. Do you have a view on that debate?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�d rephrase the debate a little bit. I think there�s an interesting technical question of like, let�s say I built an artificial general intelligence tomorrow and because it�s software, let�s say I made a hundred thousand of them. How much does that fundamentally change our society and our technological capability? A lot of it is just, you can look at individuals throughout history that manage to discover a lot more than other individuals. You look at-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Von Neumann.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Von Neumann or Einstein or one of these figures who just manage to be leaps and bounds ahead of others. The question is like what�s the ceiling on that. If we invented AGI tomorrow, would it take a couple of days to scan all of our brains into software, upgrade us, give us indefinite life extension, or would it just be like, �Oh, it�s more humans to talk to�. I think it�s actually complicated. I think some people act like it�s obvious one way or another but it�s not really something, I have a lot of certainty on in part because I think modern science has experienced a lot of diminishing labor like-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Diminishing returns.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Diminishing return like depletion of low hanging fruit. It could turn out that like solving biology is just this exponentially complicated combinatorial problem or it�s limited by data and experiment. Of course, maybe the machines will allow us to do the experiments much, much faster. Then there�s some limit on the physical reaction time of the biological systems. When you put it all together, do we get zoom to do something much much faster than we ever could or do we get just some mild acceleration of what humans can be doing? I feel like many people act as if the answer�s obvious but as someone with a background in Biology, even thinking about all the directions in which machines can optimize it, my guess is machines could probably make things happen pretty fast but I think there�s huge uncertainty here and I don�t really think anyone knows what they�re talking about on this question.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: My background is in economics. I imagine if you had an incredibly smart AI and it was trying to figure out macroeconomics, like understand recessions and booms and bust cycle, I suppose it could have lots of conceptual breakthroughs but you take the measurements so quickly and you can�t really run experiments so you could end up being the, processing of the data that we get is extremely good and very fast but then the data only comes in so quickly and there�s only so much you can actually learn.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: There are some simple stuff, which is like, I wouldn�t be surprised if for example a really powerful AI wouldn�t be able to understand our macroeconomic systems because this data issue but it would be able to design a better macroeconomic system. It�s weird. There are some stuff I feel like you just redesign it and you and do it much better. There�s other stuff that it�s just really difficult. I find this puzzling. I�m pretty agnostic on it. I don�t really have a good answer on the kind of like nerds think AI can solve everything question. I think there are some deep set problems in human nature and so just solving resource constraints isn�t going to solve war or probably in some ways already solved resource constraints. But maybe having true AGI will allow us to redefine what it means to be human and we�ll ultimately-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Elevate ourselves above conflict.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Will elevate ourselves above our petty human bickering or maybe the petty human bickering will prevent us from being able to elevate ourselves so we�ll be stuck. I don�t know about that either.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ll reach superhuman levels of petty bickering perhaps.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t actually know. It�s actually very hard to know.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ve mentioned a few times this paper, �Concrete Problems in AI Safety�, let�s dive into that. Before we discuss those problems, what was your impetus for writing it?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I had been aware of the work of the AI safety community for awhile but had in general felt that � I wasn�t particularly happy with the way they were phrasing things. It didn�t seem like what they were describing was actionable and there wasn�t a lot of ties to like � AGI was generally discussed in these very abstract terms like having the utility function and having incentives to do this or that, discussing things at these very abstract level, I couldn�t help but feel there were a lot of implicit assumptions that were not really being discussed.\\n',\n",
       " '\\n',\n",
       " 'At the same time, the mainstream machine learning community, which I�ve been a part of for about a year and half, having a lot of experience with speech recognition systems, one thing that I found about neural nets is that they�re very powerful but they�re very unevenly powerful. The key example I gave early on was you can train a speech recognition system on 10,000 hours of American accent of data. For someone with an American accent that gets it perfectly, then you give it someone with a British accent or an Indian accent or something, and it just does terribly on it. Of course, if you train it on enough diversity of accents then start generalizing better. Generally, when we build engineering systems, that silent, random failure, it�s not something that we see as a desirable property in systems we build particularly safety-critical systems.\\n',\n",
       " '\\n',\n",
       " 'The idea that fixing those problems was not just a one-by-one thing where we�re like, �Op! We�re using neural net again in this self driving car, what statistical test for everything we can get.� We�re using a neural net now in a drone, let�s make sure it doesn�t shoot someone. That we could have principles behind what gives us guarantees on the behavior of a system or at least what gives us statistical guarantees. That seems super interesting to me and it really didn�t seem like a � it seemed like very few people were actually working on it.\\n',\n",
       " '\\n',\n",
       " 'Me and some of my colleagues, Chris Olah at Google, Paul Christiano, who�s now at OpenAI, Jacob Steinhardt at Stanford, John Schulman here, and Dan Man� who is another Googler had all thought a little bit about this problem. We decided to get together and write down all of our ideas in a paper that would lay out an agenda for what, why we think this is a thing. In particular, I think � I felt that the machine learning community as a whole was a little bit confused. I think that they largely thought AI safety was about fears that AIs would malevolently rise up and attack their creators. Even when they didn�t think it was about that, they worry that the people who talk about AI safety will feed into fears that it�s about that.\\n',\n",
       " '\\n',\n",
       " 'I felt like this was a silly state of affairs and that of course we can do research on making systems safer and more reliable that doesn�t prey on these fears. In particular, we can even do research that ultimately points towards AGI. I think the important thing is that we shouldn�t go around with every other word we say being AGI in particular like the research itself shouldn�t be specific to AGI. You can�t really research AGI now because we can�t build an AGI. I think the very standard technique when doing research on a topic is if you want to think about a topic that�s abstract or in the future then come up with a short term bridge to it that lets you think about something conceptually similar in a way you can empirically test now. That was the general philosophy behind the paper and the philosophy behind the follow-ups that we and others have done to implement the research agenda described in the paper.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What are some of the concrete problems? Do you want to tackle one or two here?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah sure. I can go into them briefly. I think we made a distinction between problems that relate to, what happens if you don�t have the right objective function and what happens if you do have the right objective function but something goes wrong in the process of learning or training the system. Not having the wrong objective function, the extreme version of that is what�s talked about and the classical AGI safety stuff which is you want to specify a goal and you, for whatever reason, you know you have some simple instantiation of the goal and it ends up not quite being the right thing. We call that-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The genie problem?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. We call that �reward hacking�. Few months ago, using an environment in the now de-emphasized Universe program. I had an example of like a boat race where the boat is supposed to go around and a few laps and what it�s trying to do is finish the race as fast as possible. The only way to be able to get points and you can�t change this because it�s the way the game is programmed is you get points as you pass targets along the way. But it turns out there is this little lagoon with all these targets and the targets also give you turbo so they make you go faster and faster. You can just loop around in this little tiny lagoon and not finish the race. In one sense, you shouldn�t be surprised it�s the correct solution, it�s how you get the most points. The idea is that the mapping from, well, this is the reward function to this is the behavior that leads to it is a very twisted mapping and so the point is that it�s-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s not what you would have intended it for it to be maximize it.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s not what you would have intended it to be. The lesson of that is that it�s very easy to make small changes in the reward space and have that lead to big differences in the behavior space and also for the mapping to be very opaque for you to look at a reward space think you know what it means and in actuality it leads to something very different than what you would have expected. We call that generalized reward hacking. Then there was another problem called negative side effects, which is a little related to that, which is just that if your reward function relates to a few things in your environment and your environment is very big, then there�s a lot of ways for you to do destructive things. It�s one particular way in which it�s easy to specify the wrong reward function.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Because you haven�t put in side constraints?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. You haven�t explicitly put in the 10,000 other things.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: All the things you care about.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: 10,000 other things you care about. Then there was this thing called scalable supervision, which is, if you�re a human trying to specify a goal to a machine learning system even if you have a clear idea of what it is that needs to be done, then you don�t have enough time to control or give feedback on every action that an AI system does, and therefore limits to your ability to control and supervise can lead to a system behaving in a way you hadn�t intended because it interpolate in the wrong way. Those are the problems with like the classical AI safety type problems like you have the wrong � you somehow, you gave yourself the wrong goal in a way that was hard to understand. Then the more technical right problems that relate to, you know, your system was trying to do the right but something went wrong, it�s things more like this thing we called distributional shift which is when your training set is different from your testing sets.\\n',\n",
       " '\\n',\n",
       " 'The classical example of this is � when I was at Google there was an incident where Google�s photo captioning system had been � they had this photo captioning system that was trained on a lot of photos. It turned out that most of the photos were statistically biased to be photos of Caucasian people. There were also a lot of animals and monkeys in it. Unfortunately, this system reacted when a black person took a picture of themselves and the photo it tagged them as a gorilla because it has only seen humans with white skin so this was, of course, incredibly offensive and Google had to apologize for it. They even had � they even thought of this a little bit ahead of time but the neural net ended up being so screwed up that it didn�t even warn them that it was in a region of the state space it was dangerous.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Because the algorithm has no concept of what�s offensive and what was not.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, the algorithm, it�s just-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: But it can produce a pretty horrifying outcome.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Exactly. It�s just a statistical learning system. It doesn�t know about racism, it doesn�t know about racial slurs. It doesn�t know about what�s offensive. It�s just a learning algorithm and it just learns from the data it was given. There turned out to be some problems with the data that it was given and there turned out to be some problems with the algorithm. It just innocently produced this extremely offensive result. This is � the world of neural net is full of this. I think something related to this distributional shift is adversarial examples, which my colleague, Ian Goodfellow, works on a lot, which is when you intentionally adversarially try to disrupt an input to a machine learning system and make a very small change to it that causes something bad to happen.\\n',\n",
       " '\\n',\n",
       " 'They were a little complimentary. Adversarial examples is like a small but carefully chosen, like perturbation to it, whereas [inaudible 00:39:34] of distribution is this holistic perturbation to it. Resistance against those two is � it�s separate. You�re talking about two orthogonal directions in the perturbation space. These are all issues with making sure that when you train something that it behaves in a new environment the way you would intend it to behave or if it goes wrong but it fails gracefully. We haven�t put a lot � when we put some work into this area, we cite a lot of papers in the Concrete Problems paper. I think relative to the stampede of work in mainline AI, I�d like to see more of this stuff.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I think you did an interview with the Future of Life Institute where I think you talk about this paper for about half an hour so people who are interested can go and listen to that and you get more details on each of those different five problems. How do these problems tie together the long term concerns with the short term ones that we have today?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think the attempt was to come up with some conceptual problems that relate to both that have long term and short term versions. With something like a distributional shift, the short term version of it is something like the gorilla. The long term version would be something like, well I�ve trained in AGI in a simulation and then I put it in the real world and a lot of things are different. Does it break a lot of stuff without meaning to? The super intelligent version of it is like whatever, it�s like the marks-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s just the same but too extreme.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: More extreme, it�s building a Dyson sphere, it�s never built a Dyson sphere before where like if something go wrong, whatever outlandish thing you can think of. I think the point and the explicit strategy was that people often contrast long term versus short term approaches as if working on short term safety and long term safety and like different topics and like they trade off against each other. What I�d rather do is have a thread running from long term to short term things where you identify what the fundamental problems are. Then you work on them on short term problem. Then as the systems get more powerful, you update your techniques. It creates this more symbiotic where you�re following along.\\n',\n",
       " '\\n',\n",
       " 'I think safety shouldn�t be anything different from reinforcement learning. Reinforcement learning is a general paradigm for learning systems. You can do something as simple as walk across a grid, all the way up to playing Go, all the way up to perhaps building a system that�s as intelligent as humans. I probably wouldn�t literally use reinforcement learning but � the reinforcement learning is a general paradigm that runs from things that are very simple, the things that are very complicated. I guess the idea was to do the same thing for safety, come up with some general principles that will carry across towards very powerful systems. I wouldn�t say these problems tell you everything that could go wrong with powerful systems. I think there are almost certainly things that are very specific to powerful systems.\\n',\n",
       " '\\n',\n",
       " 'My general view is I�m much less confident in our ability to identify those problems. Maybe we can, some people are trying but let�s � my view is just, it seems like there�s a lot seen on the table. Let�s identify the problems we can identify, let�s work on them, and then whatever is left, we either have to work on them very late in the process or maybe someone can identify them but that seems like the higher hanging fruit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The hope is that in order to solve the long term problems, you want to find cases that are similar today where you can get feedback on whether it�s actually helping.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, exactly. I think that there�s a magic of empiricism because it�s very easy to engage in long chains of reasoning about a topic that don�t get tied back to reality. Of course, the risk of working on short term stuff is that it doesn�t matter or it doesn�t generalize. The compromise I�ve come up with is try and think of things that are conceptually general and then try to tie them into empirics [00:43:48].\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: To that end, has OpenAI made any noticeable progress on these problems or other problems?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think about three weeks ago, Paul Christiano and I and Tom Brown here, and three people at DeepMind including Jan Leike, Miljan Martic and Shane Legg came out with a paper called �Deep Reinforcement Learning from Human Preferences�. This works on the reward hacking, scalable supervision side of things. The way this paper works is normally you have a reinforcement learning algorithm it has a goal or a reward function. The agent acts to maximize that reward function. This works pretty well for something like chess or go where the behaviors are incredibly complicated but evaluating the goal is pretty easy. With go, it�s like, are you in a winning position, you have more territory? With chess, it�s have you checkmated the king or have you been checkmated?\\n',\n",
       " '\\n',\n",
       " 'It�s really easy to evaluate these simple goals with the script and so you can run the algorithm through millions or even hundreds of millions of games and the goal evaluation is easy. Most of the stuff that we do in real life, the goal was complicated. It�s like carry on the conversation, or be effective personal assistant to a human, which means scheduling things for them, making their life easier but not enabling all their private information to their boss or whatever. There�s a lot of like context sensitive stuff, which is part of what makes, it�s part of what leads to safety problems. If I take a complicated set of goals like that and I try and forced it into the framework of a hard-coded reward function, it�s going to lead to something that makes everyone unhappy because the two things don�t fit together.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You said that it�s maximizing on one dimension and then fails on all of the others?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. Or just that the intrinsic number of bits of complexity in something like hold the good dialogue is very high. If I try and program that in, I might be going to be programming for a very long time in which case I�ll probably make an error. Or, if I try and make what I programmed simple, then there�s just not going to be enough bits of information to fit the actual complex nature of the goal. I�m either going to be very error prone or I�m just not going to be capable of learning what I need to learn. That�s why people talk about like strategies for absorbing values and things.\\n',\n",
       " '\\n',\n",
       " 'What our paper basically does to address this is it replaces the fixed reward function with a neural net based model of the human�s reward. The idea is you have a reinforcement learning agent that�s learning, and in the beginning, it starts acting randomly, and every one in a while, it gives some examples of its behavior to a human. It will come out with two video clips. The human looks of the video clips and says is the left better or is the right better? The human says left is better or right is better. If it�s playing pong something, then if the left is point got scored on you and the right is you scored a point, then the human will say the right is better. Then, the agent builds a model of what reward function would lie behind the human�s expressed preferences. The reward function becomes something implicit and learned, observed from the human�s behavior.\\n',\n",
       " '\\n',\n",
       " 'Then the RL [00:47:33] agent gets to work saying, �Yup! This is what I think the human�s goal is. I�m going to go and try to maximize this.� But then it comes back to you and it gives you more examples of behavior, and then, the human decides in those. Over time, the human is given more and more subtly different examples of behavior. The reward predictor in response learns to discriminate them and gets a more refined understanding of what the human prefers, and in the RL algorithm then tries to maximize that. The consequences of its behavior are then given back to the human. It�s this kind of-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s kind of three steps, like the human, and then AI that�s trying to figure out what the humans are optimizing for and then the thing that does that. But then, most of time it�s asking the intermediate AI, is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Both is to have [crosstalk 00:48:20] intermediate model of what the human wants.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, you have three parts. You have a model of what the human wants. You have the RL algorithm that�s maximizing that model, and you have the human that feeds-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That trains the-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: That trains the model. But also the RL algorithm feeds back to the human so that it basically, whatever the RL algorithm has learned to do, it goes back to the humans. It basically says, �Okay, is this what you wanted? Of the things I�m now doing, which do you want more?�\\n',\n",
       " '\\n',\n",
       " 'It�s this gradual preference elicitation, which helps to get around the � if you get things wrong by a little then you get the wrong behavior. It�s unfolding behavior in real time and incrementally showing you the consequences of the behavior that you�re seen. By no means does this solve all safety problems. It�s just one little bit of progress on one.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: One brick in the wall.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, one safety problem. This is an example of this thing I�m talking about. We use this both to solve ML task that you couldn�t solve before because the reward functions were too hard to specify and then impact on safety is obvious because it allows us to specify goals more easily. There�s all kinds of other problems, you can have with it, it has to scale, there are other safety problems you don�t want AI systems tricking you. There�s so much. This thing is � there�s an example of what we did. I think we�re going to try and do a lot more it.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How many times do you have to get feedback from the human to solve this problem? Is it a reasonable number?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. It depends on the task. On some of these ATARI games, which take about 10 million timesteps to learn, usually a human has to give feedback a few thousand times. Less than 1% or a tenth of a percent the human actually has to pay attention to. We managed to train this simulated little neural robot to do a backflip with a few hundred times steps. It�s the human clicking for about 30 minutes or so. We�re trying to get that number down because-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: This is the learning from human preferences paper?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ll put up a link to that. You can take a look at this little worm thing here that learns to, let us jump progressively from this flailing around than focusing on the ground.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It got a little bit of media coverage. My favorite headline was �What this back flipping noodle can teach you about AI safety�.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It seems quite a bit.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think it was �Here�s what this back flipping noodle can teach you about AI safety�.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That�s some good click bait. Apart from those five issues that you talk about in the paper, what do you think are some other important problems or open problems in the field?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: One thing we didn�t discuss in the paper is the issue of transparency of neural net. This is trying to figure out why a neural net does what it does, which you could eventually extend to why is a reinforcement learning system taking the options it takes. It just has a policy. It�s in a situation it runs a bunch of things through its neural net and it says, �I�m going to move left�, or �I�m going to bend my joint�. It doesn�t really have much explanation for what it does. If we could explain why, break down the decisions made by neural nets, then that could help with feedback, could help with making sure that systems do what we want them to do and that they�re not doing the right thing for the wrong reasons, which might mean they would do the wrong thing in another circumstance.\\n',\n",
       " '\\n',\n",
       " 'I think that�s a pretty important problem. My co-author on the paper, Chris Olah, did a lot of work in that area with Deep Dream, which is all of the back propagated images generated by neural nets that was originally designed to be a way to visualize what maximally activates a given neuron within a neural net. It was initially a transparency technique. That�s an area that Chris is very excited about. That�s another area I think we should work on. I mentioned before adversarial examples. I think that�s an area that�s already getting a decent amount of attention but probably should get more like everything in safety should get more. I think that�s an area we should work on, and also that has like short term safety implications. Someone could sabotage a self-driving car with adversarial examples. We certainly wouldn�t want that to happen.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We can�t have that.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Interesting. Is that a problem for a rollout of self-driving cars now? That someone might put up a word sign that confuses them?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�m not the expert on it. I definitely don�t want to give anyone any ideas about how to do that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess it would certainly end up being criminal I would think to do that in the same way as hacking a computer system.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�d be extremely illegal. I don�t actually know the details of whether that�s feasible or not and wouldn�t discuss them if I did.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Of course, yeah. As we progressively work towards being able to control AIs that we�re developing, do you think it�s going to be possible for people [53:31] to understand the solutions that we have developed? You�ve discussed this three step process by which you train a machine or a reinforcement learning algorithm to understand the humans and then that trains the machine learning algorithm on the other side. I can understand that. There�s other big breakthroughs in history that you can get, like quantum physics, like it�s a particle and a half a wave and probably grasp it. Do you think it�s going to look like that? Or would it be just impossible technical details that-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: The AI itself or the safety?\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess the way that we�re going to get machine learning or like other AI technologies to do what we want rather than flip out in some way we don�t expect.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I guess there�s two possible questions. One is are we going to understand at a very granular level every decision that�s made? Then there�s, are we going to understand the principles by which the system operates? I think we better understand the principles by which the system operates. If we don�t understand those, I don�t know how we can build them. If we did build them, I would definitely worry about their safety. I think it�s realistic to understand the basic principles on which something is built. But then there�s a question of on what level of extraction do we understand it right? The principles on which a visual neural net are built are very simple. It�s back propagation and alternating linear and non-linear components. That�s pretty much all there is to understand. Then the question is how much do we know about what goes on inside the neural net. That�s the question of transparency.\\n',\n",
       " '\\n',\n",
       " 'I�m optimistic that we�ll gain a better understanding of transparency inside neural nets. The question is how does that actually help us on safety, how do we actually use it? There�s a lot going on inside neural nets even if we could individually understand every piece of them. How does that actually help us? There�s more units than I can read and understand. I have to have some way of translating [00:55:17] that into something actionable like correcting bad behaviors or something. Somehow, that component of it has to fall into place as well. I don�t know yet how that�s going to happen. I don�t know if it�s possible. I think it�s urgently important research area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Let�s turn now to how someone might be actually able to pursue a career in AI safety. What are the natural paths to getting a job at OpenAI or other similar organizations?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think my advice is going to be focused on the kind of AI safety work that I�m excited about. For example, MIRI does some safety work that�s more based on mathematics and formal logic. If you wanted to do that, you�d need a different background. The safety work that I�m most excited about, I think, it sounds obvious but the two things you most need are an extremely strong background in machine learning and a real deep interest in AI safety. I think to break those down, I think the first one is, certainly at OpenAI we really try hard to have a really high bar for hiring people. Just because someone wants to work on safety doesn�t mean that we lower the machine learning bar at all.\\n',\n",
       " '\\n',\n",
       " 'We have a lot of people here who are very good so going to get a PhD in machine learning, going to get a PhD with trying to work with the best people you can work with, doing the most groundbreaking work. There�s no ceiling to how much of this helps. My sense has been that people who have a deeper understanding of machine learning, if they�re interested in AI safety, also tend to really grasp AI safety issues better provided they think about them. That�s the second component, which is, I want people who really have a deep interest in safety not just, �Oh. it would be good if systems didn�t� � It would be good if self-driving cars didn�t crash but have a broad view of where we�re going with AI, which could be totally different from my vision, might not involve AGI but this general idea that we want to build machines that do what humans want and carry out the human will. I think that idea is a broad one and I want people working on safety to have a broad, broad view of that issue.\\n',\n",
       " '\\n',\n",
       " 'In the AI community, I don�t think the second one is lacking. There are many people who are passionate about the second one. I think the limiting factor is just a very strong machine learning talent.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We just wrote a career review of doing a machine learning PhD, which we�ll put up a link to and then you can have a read. Is it machine learning or bust? Are there other options like other PhDs that people could do that it could be relevant like Computer Science or Philosophy or Data Science?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My PhD wasn�t in machine learning. We have a number of people here who have backgrounds in neuroscience or another area of computer science or mathematics or physics. It�s entirely possible if you happen to be educated in another area to go into this field. But I think going forward, if you�re a young student, I don�t particularly see a case for doing a PhD in another field if what you want to do is machine learning.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Grab the bull by the horns.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I guess I�m saying it�s pretty easy to convert skills in related areas. Sometimes it gives you perspectives that you don�t have. If you want to do machine learning, you should get a PhD in machine learning. I think another thing I�d add is we do have some people working here who don�t have PhDs. My co-author, Chris Olah, actually never even went to college. He just straight went to Google. He had to do a lot to prove himself. The level of technical ability you need to show is not lowered, it�s even higher when you don�t have the educational background, but it�s totally possible.\\n',\n",
       " '\\n',\n",
       " 'I would say the most important thing is just being able to do a lot of impressive and creative machine learning work. I would even go so far as to say it�s not my expertise but even the people doing safety work that doesn�t involve machine learning, I get pretty nervous when we don�t have a strong background in machine learning because even if they think that a machine learning system can�t be made safe, they should know enough to understand why they think that�s the case and what they think the alternatives are.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That includes, I guess, people doing mathematical research or philosophy research?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If it relates to AI safety. I would say that even those people, I would encourage them to learn as much machine learning as possible if only because they should understand approaches that they�re partaking.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it fair to say that you think that the approach you�re taking where you study machine learning and try to actually improve AGIs is the best way to make AGI safe that you�d rather see someone do that than go into these other adjacent areas?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s a little complicated because I think that as systems get more complicated, there may be ways in which we combine neural nets for formal reasoning. There�s been some work by my friend, Geoffrey Irving, and some of his colleagues on doing theorem proving. Basically, using neural nets to select the lemmas to be used for the next theorem. If you take that far enough, you can imagine versions of reasoning systems that basically, they traverse some well-defined reasoning graph. They make logical conclusions that are tractable but it�s all driven at the bottom by neural nets driven intuition. The neural nets decide what conclusions you draw and where your thinking goes.\\n',\n",
       " '\\n',\n",
       " 'I think this is how humans do symbolic tasks like physics or math or anything like that. We�re neural nets at the bottom and then we have a layer on top of that that is � we use those neural nets to represent symbolic reasoning. Computer could probably do that even better because it can make sure that it never makes a mistake in this symbolic reasoning. The symbolic reasoning engine is there. You can imagine having formal guarantees on that formal reasoning. I think when we get to that it�ll look different from the way things are currently being done. I�m really not against using formal reasoning methods and using mathematics but I think it�ll be possible to do that work more productively once we understand how it fits in with current systems.\\n',\n",
       " '\\n',\n",
       " 'I actually don�t know. Maybe there�s stuff that�s being done now is productive but I�m pretty suspicious of anything where you can�t get that type empirical feedback loop because I think it�s really easy for people to fool themselves.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It sounds like the key piece of advice is do a PhD in machine learning. What universities can people go to? What supervisors for a Phd? [01:02:44] Do you have any suggestions then?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I do want to repeat again that there are ways not to do a PhD. In particular, a number of people go to PhD for a year or two and then doing internship here or at DeepMind or somewhere, and then get hired so you can partial PhD can work. That said, I think the usual suspects are places like Stanford, Berkeley, Cambridge or Oxford in the UK, Montreal, Yoshua Bengio�s Group is pretty well-known for doing a lot of good stuff. Then, there are kind of number of other places. Definitely, the PhD pools are where we hire. A lot of our folks, because we know a lot of the relevant professors.\\n',\n",
       " '\\n',\n",
       " 'Again, we have some people here who didn�t do the PhD work. I think the most important thing is being able to keep up with the literature and make, creative, original discoveries that are novel and that stay on pace with what everyone else is doing. If you can do that then that�s the best thing. When I talk to people who I want to switch into machine learning from another field, the advice I always give them is just get every possible model you can. If you�re trying to learn supervised learning, get all the image now, models like implement them yourself, read the paper, implement it, read the paper implement it. Same for supervised learning, same for generative models. You just get this knack for it after you�ve done it for a while. It�s really just practical hands on experience. You just get a sense for these things once you�ve done them for a while. You also find out quickly how good you are.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I might be showing my naivete to ask this but machine learning is only way that you can approach AI right? There�s like other paradigms on how you produce another artificial intelligence, is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this has historically been the case. It�s gotten a little bit, I think complicated. In the old days, we had things like expert systems that were based purely on logical reasoning. Generally, they found that those systems were very, very brittle because they couldn�t represent the high dimensional space that we see. For example, vision system that�s based purely on rules, it�s difficult because if I�m trying to identify a face or an object, I�m trying to identify these blobs in distribution space. In some sense, these problems are inherently statistical. The rule based systems actually don�t end up working all that well. Historically, there were statistical system and there were rule based systems.\\n',\n",
       " 'I think we can say, now the statistical systems have pretty decisively won. I sometimes hear people say things like, �I don�t think AGI could be built using machine learning or I don�t think AGI could be held safely using machine learning. I don�t think AGI could be held safely using machine learning or something. I think when people say that they aren�t really thinking carefully about the alternatives, I�m quite sure that a pure rule-based system is just not going to work because of the thing I said that you have to ground what you�re doing and sensory information and the sensory information is just inherently statistical and fuzzy. Pure rule-based systems I think are not going to work. What could happen is the thing I described before where you have a machine learning systems, deep neural nets being used to drive logical reasoning systems.\\n',\n",
       " '\\n',\n",
       " 'That will be a hybrid of the two but people are working that, that will be considered within the field of machine learning. I think we�ll often, in the future, machine learning may include logical reasoning processes but there�ll be at a higher layer. What ends up happening, it will involve reasoning in the same sense that goal-playing systems involve a goal but it won�t really be like those rule-based systems that we had before. You can�t be 100% sure of this but I think just the basic argument that percepts are these statistical blobs and so you have to use the statistical system at least at the beginning to measure them. Then whatever concept you draw from them end up being fuzzy statistical concepts. If you want to bring those back to logical reasoning, the reasoning have to exist on a plane that�s subtracted from that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So there�s not some other AI paradigm that people should be doing if deep studying?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Again, in the history of AI, there were a lot of rule-based systems. Then there were critiques written of them. I forgot the guy�s name, Hubert Dreyfus or something who basically, he was like this continental philosopher who wrote this critique that people found really hard to understand but what it was really saying was just percepts of these statistical blobs. If you make a rule-based system, you�re not going to � you�re always going to make mistakes and your system is always going to be brittle and wronged a lot.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Assuming someone has been studying machine learning or one of these other related areas that�s potential path in, is there a natural path from studying to actually getting a job at OpenAI or another organization? Are there intermediate steps that people have to take?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think, again, PhD students who do impressive first author work on papers are people we are generally very excited to interview. If you�re in a good PhD program and you do some work, then I�d definitely do some good work. I�d definitely encourage you to apply here. For people who are relatively early in their careers or come from another background, there is a program at Google called The Brain Residency program, that allows you to study machine learning with the experts for a year. Then, that allows you to know to train your skills. We�ve had a number of residents who have applied here or elsewhere. That ends up being a good thing.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Speaking of which, there�s a bunch of different organizations, right? There�s OpenAI, there�s Google DeepMind, Google Brain, Vicarious was another company or maybe more in the past. There�s the human compatible AI group at Berkeley. You can like go through a couple of these that you might recommend working at?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: As I said, OpenAI and DeepMind are probably the most focused on reinforcement learning. They probably spend the most time thinking about AGI. Not everyone here does but it�s a focus here more than it is elsewhere. Google Brain is where I was before coming here. That was the original research group at Google. I would say a more kind of decentralized group that works on a wider set of topics. Chris Olah there thinks about safety. There is � you mentioned the Centre for Human-Compatible AI which is Stuart Russell�s group. We collaborate with them some. We have some interns from there come here. I think Stuart�s been someone who�s been thinking about safety for a while. That�s another good place to work on it. Vicarious to my knowledge, doesn�t think about safety. I could be wrong.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Are there other groups that you missed here? Are there any other government research projects? Anything in China?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Not that I�m aware of. Of course, there�s MIRI and FHI.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Right the Future of Humanity Institute and the Machine Intelligence Research Institute (MIRI). They�re less doing machine learning where it gets more strategic and-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Their focus is less on machine learning although I think Stuart Armstrong at FHI collaborated a bit with DeepMind, which something that I think was broadly machine learning related. This was like studying interruptibility and corrigibility or something like that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: At 80,000 Hours, we talked to people reasonably open who would be interested in doing a job like what you�re doing. Is there any way that they can get indicators early on about whether that�s possible or whether they just wasting their time and I�m sure looking at other options because if it�s not going to be a good fit, either they don�t maybe have the machine learning chops so cultural they�re not going to a good fit or some other reasons.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think the thing I mentioned about implementing lots of models very quickly. If you want to know where you�re good, a way that is a good proxy for how well you�ll in grad school at well at for the test we give when people apply here is find the machine learning model that�s described in a recent paper, implement it and try and get it to work quickly. This is a painful process for you and you really don�t like doing it, then you aren�t going to like any of the research that of either on AI safety or other AI stuff.\\n',\n",
       " '\\n',\n",
       " 'If you find you can do this quickly and/or, you really, really like doing it. You find it addictive, then that�s an indicator that this is something, this might be something you really want to do. I wouldn�t worry about the cultural stuff. If you�re skilled in this area and passionate about this area, I don�t think you�ll have-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That�s not going to be a barrier.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t think it will be a barrier. I don�t think you�ll have any problems. You try and be ask open and welcoming as we can, we don�t have the luxury of selecting people and, anything other than their.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You don�t like our favorite TV shows.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I mean. That�s just wasteful and pointless.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah, absolutely. How well you can people to do that? Cannot they do that as an undergraduate? Is it more like a PhD level thing?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: People can do it in high school like, if you meet a 17 year old, how do you get them into machine learning. I�m like, just go home and implement these models. You actually don�t need any kind of formal education. You probably need a thousand dollars to buy a GPU. I have considered various times, shall we have a like for grand program where it�s like, if you�re like 17 years old and you want to get into machine learning, I�ll just buy you a GPU.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah. Yeah why not.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If you�re interested in AI safety, that was a thousand dollars. Most adults living in the developed world can afford a thousand dollars but most 17-year-olds might not be able to. If they don�t already have access to one it might be a good way to get people started early.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If there�s a 17-year-old listening here who wants to go and build their machine learning model, what should they Google for, what should they start reading?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Because I�m from OpenAI/DeepMind direction research, thinking about reinforcement learning, trying to-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What�s the difference between Reinforcement learning and Machine learning? Do you want to-?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Machine learning is the broader topic, within there are several different areas. There is supervised learning which is where you try and predict some data that�s been labeled. An example of a supervised learning problem would be like you�re given images and they correspond to objects. This is an image of a dog, this is an image of cat, this is an image of a computer. You train the network on lots of pairs of here�s the image, here�s what it is. Then it learns over time to map the two to each other.\\n',\n",
       " '\\n',\n",
       " 'Supervised learning has this static quality where it�s like a one off. You�re trying to like, predict one thing from another. Reinforcement learning is more a setup where you�re interacting in a more intertwined way with an environment. The game of go is like this. You make a move, and then the opponent or your environment makes a move, and then you make a move again and overall you�re trying to win the game and the reward or figuring out whether you�ve won the game or how well you�re doing can be delayed by a long time. The reason I focus a lot on reinforcement learning and why OpenAI focuses a lot on it is that reinforcement learning and things like it, the extended versions of reinforcement learning seems like a better fit for what intelligent agents do in general.\\n',\n",
       " '\\n',\n",
       " 'Often, I have very long range goals, I�m trying to get an education, trying to get a PhD, trying to have a career, trying to start a family or something. These are all things that unfold over years and involve interacting with my environment in this very complicated way. Reinforcement learning is the only paradigm we have that even close to capturing this,\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Sorry. I cut you off. We�re figuring out what the seventeen year old should read to get that foot in the door.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Lots of papers in reinforcement learning. I read about what�s called the DQNs � it�s Googlable. It�s not common acronym. Deep-Q learning that this was a paper done by DeepMind in 2013. Policy gradients, particular A3C and just follow the tools of recent reinforcement learning things that have showed up on arXiv. Just go to /r/machinelearning on Reddit and look at some recent papers in the deep neural net literature. Look at them try to re-implement it, see if you can get results as good association the results that others get. It�s really pretty self-contained and you don�t need that much help. If you�re having trouble getting started implementing them, then you can start by fermenting popular papers like DQN, you can find it an existing implementation that start with that and try to fiddle with it to see if you can make it better.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What kind of program or you running, because you�re moving out doing this in Excel. What�s the-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No. It�s typically, you�ll use a, the typical is a Python with Tensorflow. Tensorflow is this tool that Google Brain team made for doing general computations but in particular deep neural net computations. You�ll find a large fraction of this office implemented in Tensorflow or some similar framework so python�s pretty easy to learn, Tensorflow is pretty easy to learn. Read some Tensorflow code as some stuff that�s been implemented, learn Tensorflow and implement some stuff yourself.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What�s the range of roles available? How do they vary?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: You�re talking about within machine learning or within safety-related stuff?\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess mostly within safety I think.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Okay. I can talk about what�s being done here at OpenAI. I would say there is two main directions. There�s the technical safety stuff, and there�s the policy side of things. On the technical safety, there�s not a lot of people working on it yet but the Human Preferences paper that I showed you is a good example of it. A lot of the papers we cite in Concrete Problems are good examples of this work. DeepMind just have some recent good examples of safety work. I think the skill set, as I�ve said, is very similar to the typical machine learning skill set. You should also be willing to work in the field that has relatively sparsely populated literature, which means coming up with your own ideas or working very closely with someone who�s one of the people generating the ideas in the field.\\n',\n",
       " '\\n',\n",
       " 'That has some downsides in that. You have to set more of your own direction but also has some upsides that you can be one of the first people in a to totally new field. That�s what excited me about writing Concrete Problems. I can work on something that 200 other people will work on or I could try and set a new direction. Maybe it won�t be exciting at all and you know, �Oh well! At least I did something interesting.� Maybe it�s turns out to be really exciting and that�s like a bet that I�m perfectly happy to take.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What would you recommend to someone who is considering entering the AI safety industry doing the machine learning work. They�re worried that they�re not going to have such great long term career options elsewhere, especially compared perhaps to doing machine learning work in a more commercial way with less of a safety focus, of just going into what pays the most or has the best career.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think ML work is so hot right now that anyone who goes into it, particularly on the fundamental research side, it�s easy to transition to applications. I think the kind of safety work that we�re doing has many of the same skills as any other area in machine learning, even though the subject matter is very different. I think some who does that is going to be in a very strong position to do very well in the future. I think it�s probably, even if we�re going into for altruistic reasons, it probably just also having to be on the most secure career, high paying, financially secure career areas. You could go into � we recently had someone leave OpenAI who became head of AI at Tesla, head of all of AI reporting directly to Elon Musk.\\n',\n",
       " '\\n',\n",
       " 'I myself want to stay at OpenAI and work on safety. I want to keep working on the research end all the way until we get AGI whenever that is. If I didn�t want to do that, if I wanted to leave, there�s like plenty of wonderful things that I can do and the same would be true of other people who come here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it more of a concern from people who would be working at MIRI doing non-machine learning safety research.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I mean I think most of the people there are smart people who either had or could have really great careers as software engineers. They probably have great, great, great options as well. I generally get the sense, people who go to MIRI are really passionate about MIRI�s mission and tend to worry about this less. The amount of buzz and hype is definitely not as high as it is for the machine learning bolt.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Pretty often, we talk to people who are saved in the mid 20s and they did a fairly quantitative degree, maybe like economics or logic. I do know machine learning particular. Is it possible for them to great to get into this? Was it all just over for them at 25?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My own example is, until I was 28 or 29 or something, like I hadn�t any machine learning. It�s definitely possible to do this. My main advice is the same advice that I�d give to you know, the 17 year old that we talk about earlier, which is just implement as models as you can as quickly as you can. Just to see if you have the knack for it you really enjoyed doing it because this is going to be greater than 50% of what your job consists of. Just knowing how to have the real intuition to implement neural net models and have them work, how to put together new architectures that do new things.\\n',\n",
       " '\\n',\n",
       " 'First, implementing these papers and then tweaking them. That�s a really cheap to give out whether this is a career that you�re good at and that you�ll enjoy. I wouldn�t recommend by going back and doing another PhD in machine learning. I think once you have a PhD like there are some positions where Google wants you for instance, wants you to have a PhD when they hire you. They don�t really care what area it�s in. They do want to know that you�re like committed to some new area that you want to go into but it�s more important for the places that care about whether you have a PhD, which we don�t care that much. Even though at places that care, things more important, you have a PhD and that you have it in some particular field.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If you already have a PhD in Philosophy, then you should go and learn ML directly or do some internship somewhere.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I would say learn ML, implement a bunch of models then go do an internship or the Brain Residency program at Google, come do an internship with us or at DeepMind, all these are viable options and each step gives you a better ideas of whether this career path is really for you.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Really, it�s just a case of someone who did an undergraduate degree in Economics can jump in and try to learn machine learning, try to train machine learning on my computer.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I think there�s � if you know how to program Python and you can learn Tensorflow quickly that it�s a very empirical field. Of course there�s lots of hidden knowledge that researchers know that they tell each other but that it�s hard to express in the papers. You won�t pick up on everything but you can certainly get started this way. Then talking to people about models, you�ve implemented talking to professional researchers to get a sense of what�s an exciting to work on next. That�s enough to get you started.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How does working at OpenAI or Google compare to machine learning role in academia?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I generally tend � I�m a bit biased but I generally tend to give people the advice to come to industrial labs. I think one reason is the industrial abs have gotten, by industrial labs, I mean OpenAI even though it�s not for profit. Just the large non-academic research center, they tend to have more resources, more compute, and in part because of this, I think they�ve been winning the talent war recently.\\n',\n",
       " '\\n',\n",
       " 'I think it still makes sense to go do a PhD. I think staying in academia your whole life, I mean, I guess if you become a professor then it becomes a lot easier to collaborate with the industrial labs. Both we in DeepMind have people who were our professors and spent part of their time there and part of their time here. It�s all very feasible and there�s a little of mobility between the two. But in general it felt many people will disagree with me but I felt that the most groundbreaking work has tended to arc the industrial labs. Over the last couple of years at least.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah, interesting. Is there any effort to change them? Are you universally is trying to catch of it just too expensive to get the research projects.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, there�s Yoshua Bengio�s group in Montreal as you know is quite larger, it�s one of the few major figures in deep learning who�s resisted the pressure to go into the industrial world. His lab does a lot of great work, Pieter Abbeel at Berkeley, Percy Liang at Stanford, and just a number of others including folks who do work that�s not necessarily related to deep learning. There�s a lot of interesting work everywhere. At least the kind of safety that I work on tends to play best with the cutting edge of ML work and explicitly tries to keep up with the cutting edge of ML work.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That reminds me, given the cost, how is OpenAI funded? Is it just the donations or are you also like selling products at things point?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No, we�re non-profit. I think I mentioned earlier the major donors are Elon Musk, Sam Altman and Dustin Moskovitz at this point.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s just the donations.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: At this point it�s just donations, yes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Okay, interesting. Do you think, is it possible for you to sell things to go extra computational power if you need or like it starts selling services. It�s a legal issues, it�s outside the area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I�m not an expert in this. I think � I�m not sure you can sell stuff if you�re non-profit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is the work frustrating because you�re not sure whether solutions actually exists and you beat your head against the wall for quite a while before you figure out, well maybe that there isn�t a way of solving it the way that you thought?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think actually that�s the case in any area of machine learning where you�re trying to do original research. If you�re trying to do something worthwhile, they you don�t already know if it can be done and you have to try stuff that seems crazy. It might not � it�s true if any area, especially true of an area that�s very new like AI safety. Yeah, I definitely agree that one of the trade offs for working with AI safety is that on one hand you have this exciting ability to work on a new field that�s just starting. It could be very impactful but at the same time no one�s defined what successful work looks like.\\n',\n",
       " '\\n',\n",
       " 'We�re still laying out what the problems are and what the work is that needs to be done. I think it definitely requires an attitude of being willing to do more to define problems yourself. You need to be more creative instead of doing something that�s an incremental improvement on the thing, the thing that was done last. To me that�s a good property.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it a good role for someone who a lot of grit and willing to persist with things despite adversity. They�re pioneering in your area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think that quality is useful in any important or original work and it is here as well.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Turning now to non-machine learning approach I to tackling the problems in AIs safety. What kind of non technical approached do you see as promising? I interviewed Miles Brundage of the Future of Humanity Institute, recently. Do you have a view on any of the AI policy topics that we spoke about?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I don�t know precisely what you guys spoke about. I spend a little bit of my time speaking about the relevant policy issues. I think if the humanity at some point builds AGI then we�re going to have to think about both how to handle safety issues as we�re building it. Some of the coordination issues that going to come up with respect to safety and also the question of who uses it, what it�s used for. One example is this, you can imagine that maybe if it�s possible, a really good way to build again would be to build an AGI, and instead of doing anything with it in the world, try and, if it�s possible, first developed the develop the capability to have it advise you on this situation you put humanity in by building it. Look, we just opened this can of worms by creating you. Can you analyze our strategic situation and say what we should do because we�re aware that if we don�t use you in the right way, or we hand you to the wrong person, then it could be really bad for humanity.\\n',\n",
       " '\\n',\n",
       " 'If we�re able to turn the problem in on itself that way, they would be really good. That�s partially-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Get the AI to make the world safe for AI.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, it�s partially a technical question and it�s partially a policy question, which is how do we get ourselves in a situation where we can do that. I think that there�s a lot of players. There�s going to be more AI organizations, government actors, will someday have something to say about Ai. They already have something to say about AI. Someday they�ll have something to say about AGI. When we�re more in the world where AGI is going to happen. What strategies should we take towards all of the actors. How do we make sure that when everything is put together, it leads to a good outcome. Is there anything we can do today to deal with these distant problems.\\n',\n",
       " '\\n',\n",
       " 'Those are the set of policy issues that we tend to think about. There�s also some more thought on short term policy issues. How can we get people to think about more mundane issues of safety, should the government regulate things, what should policies be on self-driving cars and stuff, what should policies be on automation and job creation. We do some of that stuff but a lot of people think about that so we tend to focus more on the long range stuff. It�s less actionable business there aren�t that many people thinking about it. We might as well do whatever thinking we can on it. Which might be there�s nothing that actionably be done, but we want to at least consider.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you have any thoughts on how we can ensure all of the players cooperate and avoid having an Arms Race where they just try to incorporate and avoid having an arms rice where they just try to improve their machine learning techniques really quickly without regard to safety. It seems like you�re collaborating a great deal with DeepMind.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this was in part motivated by the idea of the orgs working together. It helps that myself and some of the founders of DeepMind have known each other for a while. We all think about AGI and think, that safety issues are important. When people at the major organizations are friends with each other and work to actively collaborate, then that reduces the probability of any kind of conflict because people know each other. There isn�t fear or uncertainty. If there�s a disagreement, we can work it out. Then the question is, how does that scale to there being a lot of organizations. How is that scale to others who get involved once they see how powerful AI is. Can we make them cooperate as well? My hope is that we can. No, it�s not an easy thing.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you think it would be good or bad thing if AI were developed sooner. There�s been a kind of this explosion of investment in machine learning and improvement. Is this something that we should be pleased about or concerned about, or just neutral, we�re not sure whether it�s good.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s hard to say, the obvious bad thing is if you�re like Really afraid that there�ll be safety problems with AGI then you might think it was a bad thing. A lot of people think it�s bad thing for that reason. My view is we�re relatively early in the game and I think there�s a substantial probability that the gloomy analyses are really misunderstanding the safety problem and how a safety problem works. Some counter wrist to worry about that something bad happens to the world in the meantime while we�re trying to develop AGI, or that AGI is used in a bad way. I guess a couple of years ago, I often made the argument that we were in a relatively peaceful geopolitical time so it would be good that AGI will be built. I�m starting to wonder that in the last year we�re not in such a peaceful geopolitical state.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s a little bit less clear.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Last year so that maybe we�ll not in such a peaceful geopolitical state.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: As we�re recording everyone is flipping out about North Korea developing intercontinental ballistic missiles.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No, these things are pretty deeply concerning. There�s been a lot of political instability in the western world in the last year. Aside from the usual reasons why this might make me unhappy, it�s made me unhappy because it creates a less stable political environment in which AGI would happen. I don�t know. I will say, I think we�re better off if AGI is developed in a stable political environment with leaders who are intelligent and have reasonable views. I�d like that to happen. I no longer know whether that means that AGI can happen soon or that it should in a long time. I guess it depends whether the current trends that we�re seeing in the last year continue or if they�re only a blip. If they�re only a blip, then that doesn�t matter. In a few years, we�re back to where we are before. But if we�re on a general trend in bas way direction, then maybe it�s bad to wait to long.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess that�s the difficult thing to time.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think it�s pretty complicated. I think pure safety considerations tell us that it�s always good to have more time although at the same time, some of the hardest safety problems to solve maybe problems we can�t solve until the last couple of years, until we build AGI. In that case, delay doesn�t really help us. It just delays the crunch period.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s like someone trying to finish an essay by a particular deadline. If they know they�re only going to do it the night before, then it doesn�t much matter when the deadline is.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It doesn�t much matter when the deadline is. I think it�s a complicated question. It�s not a variable that I have a lot of control over. It�s happening at the field level. I prefer to try to control variables that I have some control over. One thing I have control over is that it seems like there�s at least some safety work that can be done now and so I�d like to do it. It seems like there are some ways that different AI organizations are not collaborating now that we can encourage them to collaborate. I�ve also been working on that. I think those efforts have been successful and so I feel like it�s been good to cause things to happen that wouldn�t have caused otherwise. Then there are all these other things that I feel like I have no control over whatsoever.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�ve taken out an awful lot of your time here. I�m sure you have to get back to your research, hit these deadlines. Anything you�d like to say to people who are considering following your example and doing this research before we finish?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: We�re of course hiring for very talented, machine-learning people who care a lot about AI safety. We welcome applications at OpenAI. We collaborated a lot with the DeepMind safety people. I�m always, as part of this collaborative spirit, I think that�s a really great team as well and people should apply there as well. It�s convenient to have a place that�s in Europe and a place that�s in the US. I think there�s a lot of good work going on at several different places.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�ll just add 80,000 hours has been doing a whole lot of research into his question of how can we positively shape the development of artificial intelligence and we�re coaching some people to try to help them get jobs at places like OpenAI. If you feel like you�re in a really good position to do that, then fill out the application on our website. We think it�s one of the most high impact roles that someone could take if they�re able to do it, which is one of the reasons why we�ve looked into it so much. Hopefully, over the next few years, we�ll see quite a lot more people going into this field and it wouldn�t be so neglected. But it�s been fantastic to have you on the show, Dario.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yes, thanks for having me.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Hopefully, we can check back in a couple of years and find out what OpenAI has been up to, and hopefully you�ve found lots of new talented people to work in the area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Hopefully.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Fantastic. All right. Thanks so much.\\n',\n",
       " '\\n',\n",
       " 'Hi I�m Robert Wiblin, Director of Research at 80,000 Hours. Thanks so much for joining. The last few episodes have been really well received and it�s been gratifying to see our regular audience growing quickly.\\n',\n",
       " '\\n',\n",
       " 'If you haven�t subscribed yet, search for �the 80,000 Hours Podcast� wherever you get podcasts. That way you�ll never miss an episode about something you�re interested in, can listen in while you�re cleaning or travelling, and can speed the conversation as you like.\\n',\n",
       " '\\n',\n",
       " 'Today�s episode is the longest so far � two and a half hours. But if you listen through you�ll know a hell of a lot about why pandemics are such a scary threat and what we can all do about them.\\n',\n",
       " '\\n',\n",
       " 'We�ve put an index in the show notes and the associated blog post so you can look through and skip to any section you�re particularly interested in.\\n',\n",
       " '\\n',\n",
       " 'So, enjoy!\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Today, I�m speaking with Howie Lempel, who, until recently was a program officer for the Open Philanthropy Project, where he�s worked on problems including biosecurity and pandemic preparedness. For those who don�t know him, the Open Philanthropy Project is a foundation whose goal is to make philanthropy go especially far in terms of improving lives. It�s the main philanthropic vehicle for Cari Tuna and her husband, Facebook co-founder, Dustin Moskovitz, who are expected to give about $8 billion over the course of their lives. And for disclosure, 80,000 Hours� biggest donor is actually the Open Philanthropy Project. But prior to working at the Open Philanthropy Project, or OpenPhil, as we call it, Howie studied at Yale Law school and worked for the Brookings Institution and various criminal justice reform organizations. Thanks so much for coming on the show.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Thanks, Rob. I�m really excited to be here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So we plan to talk about the nature of the risks from pandemics, what can be done about them, and how listeners can best use their career to reduce the threat. But first, what did you work on at the Open Philanthropy Project?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Sure. So when I started at the Open Philanthropy Project, it was a brand new organization. It was like a mix between being a large foundation and having the feel of a smaller startup. That meant that at the beginning, everybody was a bit of a generalist. So when we started, our first major project was deciding what areas of philanthropy, what problems and causes seemed especially promising for OpenPhil�s giving. So from our first year or so, my main work was doing research to that end, and the questions that we asked to choose our program areas were: could we find causes that had a really big scope or scale of the problem that we were trying to solve, that really affected a lot of people and had a big effect in their lives, where it didn�t seem like there was enough philanthropy that already existed? And also where it seemed like there were concrete things that more money could do to help.\\n',\n",
       " '\\n',\n",
       " 'So we started with this huge list of all the causes that seemed like it could be potential candidates, and I was one of the people at the start who did really shallow looks at each of those causes. So, talking to maybe two or three experts and getting their sense of helping us understand what the problem was in that area, what people were already working on, what sort of important possible solutions there were that people weren�t working on yet, and start getting a sense of what philanthropy could do in the area. So my job at first involved both reading up, quickly getting to know a lot of different areas, and then going out and reaching out to the real experts in the field, and trying to get their sense of what more philanthropy would do in each of those areas. And over time, as we started to specialize, the job changed because we dug in more.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So this is extremely similar to what 80,000 Hours does, or is part of what we do, and I think we actually copied the framework that you were using initially. So our problem framework is to look for issues in the world that are really large in scale, that not many people are working on, and where it seems like you could easily make a difference by spending more resources, so like scale, neglectedness and tractability we call it. We�ve relied on Open Philanthropy Project�s research quite a bit in forming our own list. So I guess the openness has been pretty useful for us.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think the same types of questions that 80,000 Hours is asking, when they�re thinking about how someone can do as good as they can with their career, that�s the type of question that Open Philanthropy is asking, but thinking about how a foundation can do as much good as they can with donating money.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: And I suppose that there are some differences there between where you need talent to go and where you need money to go, but because they can convert between one another it�s a pretty similar problem.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Absolutely.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: A lot of what OpenPhil did in the early days is what we call our global priorities research, trying to figure out working on which problem you can have the largest impact with your money or your career, and I think in the future we�ll probably have a full episode devoted just to that topic. It�s one of our recommended areas because there�s very few people doing it and it seems like you can have a very large impact by shifting resources from problems that are somewhat less pressing for the world toward the problems that are much more pressing.\\n',\n",
       " '\\n',\n",
       " 'And potentially we�re not allocating resources in a very effective way overall because there�s just not many people who are trying to make these prioritization decisions, trying to figure out do you accomplish more good by focusing on health or education, or on these global catastrophic risks or something else completely different. So OpenPhil has been able to make quite a lot of progress in a short amount of time just by looking at something on which not a lot of people had already tried to work. Is OpenPhil a place that you would recommend people work?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, absolutely. I think working at OpenPhil is one of the best career decisions I�ve made and I think it�s one of the best places in the world to work on cause-prioritization-type issues because I think it�s pretty unique in that it is open to so many causes and really cares about cause prioritization, but also has the resources to actually � Once it chooses a cause, make a real bet on that cause, and I think that that really is a good learning opportunity when you�re working on cause prioritization. So knowing that you�re eventually going to actually have to make a bet, and that the organization is going to make grants, I think it imposes some discipline on�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Focuses the mind.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think that that�s right, and then also I think there are certain questions that it forces you to ask about, �How much money could this field absolutely absorb?� Like, �Are there good reasons why X area is neglected? That you would only know if you talk to potential grantees.� That type of thing, that it�s pretty hard to get if you�re not in the position of grant-maker. So I think it�s a pretty unique place to do that type of work and, yes, I�d definitely recommend it for people who are interested.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you know if they�re hiring at the moment?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I don�t.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Okay, yes, well we�ll stick up a link to their vacancies page. I don�t know whether they�re hiring right now but they might well be at some point in the next six months, so yes, you can potentially get on their mailing list and find out about that. What was your research work at OpenPhil like day-to-day?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: It depended on exactly what task I was working on, but a lot of the work was first spending as much time as I could really talking to experts in the field and learning from them. So it�s a lot of phone calls reaching out to top people, reading and trying to get to know a field well enough that I knew, and advise quickly as possible the main open questions that would affect whether or not we wanted to enter a cause area, and then try and figure out mostly through Internet research, academic research, who the people were who could answer those questions, and then trying to reach out to them and look at their answers.\\n',\n",
       " '\\n',\n",
       " 'Then once we identified areas as particularly promising, the work changed a little bit, and there were some cause areas where the next step was really to go out and find grant-making opportunities. That meant a lot of going to conferences, getting to know everybody in the field, trying to understand what role every organization played, whether there were things that seemed like gaps to us, types of organizations that really ought to exist and didn�t, or organizations whose work we would really like to expand, and then go out and make friends ourselves. Sometimes, for program areas where we really felt like we needed an expert or a specialist to scale up our grant-making who had � Instead of whatever amount of expertise I can get in a few months of learning about an area, there are people who have spent years and years building up their networks and expertise.\\n',\n",
       " '\\n',\n",
       " 'So sometimes my job is to find that person and go out and trying to recruit someone. And again, that actually involved relatively similar work. The difference between taking a bet on an individual who you�re going to then give the responsibility to do overt grant-making, versus taking a bet on the organization you�re going to grant to is pretty similar. So it�s a lot of going out into the field, making sure everybody knows my face, going to conferences, getting to know people, and getting them to � building up trusting relationships that you could ask them for their advice on who we ought to hire to go and run a program area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So did people suck up to you a lot because you had a lot of money to give away? Was it hard to know who to trust?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes. I think that�s one of the hardest parts of working for a foundation, is there�s a pretty awkward relationship where you have a lot of power, and people really do want to please you, and it takes a lot of work to build a trusting relationship where people feel like they can give you negative feedback, people feel like they can be honest with you, and yes, I think that�s a major challenge working at a foundation.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So do you think the work had a large impact?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I really do think it did. I think that there are relatively few foundations like OpenPhil that are large and have that level of resources, and that also have OpenPhil�s openness to really going into almost any cause area where it feels like it could make the biggest difference. So I think that OpenPhil really was able to find some areas where there was nobody available to give the kind of philanthropy that was needed and really make a big difference. So one area that�s really important to me is working on reducing the suffering caused by factory farming. And OpenPhil was able to enter that area and become really one of the largest donors in that area.\\n',\n",
       " '\\n',\n",
       " 'And similarly, in the area that we�re talking about, pandemics and biosecurity, it�s an area where there�s a lot of funding by governments, but there are certain activities that really can�t be done by governments, so something like political advocacy or policy advocacy often can�t be done by governments. It�s sort of certain really sensitive questions that governments often can�t really work on, so I think it�s really important to have private sector money in there to hold governments accountable. I think that�s something that OpenPhil really was able to do in an important area and it was really clear that there weren�t many other foundations in the area available to give that kind of funding.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You�re saying in a sense that the field isn�t neglected in terms of what governments can do, or not so neglected, but then in terms of what independent actors, nongovernment actors can do, that those things often aren�t getting done because there�s just no one there?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Well, I would say that there are two issues. One of them is exactly what you said. It�s an area where � Public health is one of the big priorities of a lot of governments, so there is a lot of spending in that area by governments. There is almost no spending on things like policy advocacy, think tanks. The whole NGO sector that usually exists in important policy fields was really neglected, then there�s a second issue, which is my � The main thing that I care about in this area, and OpenPhil�s priority, was in particular, reducing the chance of the worst-case scenarios among pandemics. Things that can really be what we called global catastrophes. I think that that�s something that governments often aren�t quite able to focus on as much as we might like.\\n',\n",
       " '\\n',\n",
       " 'They often work in very short time cycles. Congress has to renew funding every couple of years for most agencies so it�s really hard for governments to focus on the long run, focus on things that might not be likely, but that would be a really big impact if it were to occur. I think that if you want governments to be able to work on those types of things, you really need some advocacy to back them up, sort of subsidize that work from the private sectors. I think we�re both able to fill that gap in the private sector, that there wasn�t that much philanthropy, and then also I�m hoping that Open Philanthropy will be able to support the government through advocacy and allow it to really prioritize some of the stuff that we thought was most important.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Does OpenPhil work on any other problem areas where the situation is similar?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I actually think that this was pretty unique among OpenPhil�s cause areas, at least while I was there. Because OpenPhil really does prioritize areas that are both very important and also neglected, often that meant it was areas that were neglected even by the public sector. So biosecurity and pandemic preparedness stood out to us as an area that was a little bit different, because it does have relatively large amounts of spending by governments but that still seemed like there were big gaps. The other thing that�s sort of unique about this area, is that it�s a little hard to define exactly what counts as spending through these pandemics.\\n',\n",
       " '\\n',\n",
       " 'So there�s a lot of work on public health in general that would be helpful from the perspective of pandemic preparedness, but that maybe isn�t the top thing that you might prioritize if all you cared about was using risk from pandemics, or in particular reducing risks from the types of pandemics that could really be global catastrophes. So if you look narrowly at interventions that are specifically focused just on lowering risk from that type of pandemic, then even governments I would say really neglect that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So there�s just general public health spending designed for disease control that is better than nothing, but it�s not the thing that you�ve been most interested in doing if you were just focused on preventing a pandemic that could kill a really large number of people and was completely new.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Exactly.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What kind of grants did you make in the area while you were at OpenPhil? Or what grants did OpenPhil make?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: So the main priority for most of the time I was there was actually not grant-making, but it was getting to know the area really well, start to set our strategy, and we made our hire, somebody who�s an expert in the field, Jaime Yassif, who�s now running the program area. But we did make a couple of grants while I was there. One of them was to a Blue Ribbon panel on biodefense in Washington DC. We gave them, to start, for their first year about $300,000 of our funding. They also had a bunch of other funding from other organizations. What they did was they got a panel of real policy luminaries, former policymakers in DC, to convene a set of meetings and identify real priorities in United States biodefense policy.\\n',\n",
       " '\\n',\n",
       " 'So figuring out what the biggest improvements to be made to prevent risks to the United States from epidemics. Not necessarily just things that can be a global catastrophe, but natural epidemics and also the potential use of bio weapons or potential biological accidents. So they put out a big report identifying priority areas and their goal was really both to identify priority areas, just difficult because there are so many different types of work that can be done in this field, and to really call attention in the internal DC world, so this is an important topic. We supported their work and that was one of the first grants that we made in the area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How do you think it went?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I haven�t been there for all of the follow-up and it would be great for some folks to, at some point, talk to the new program officer at GMA about it, but our initial question is number one, this type of thing is setting the stage for hopefully having policy changes in the future, and it was largely trying to convene the biodefense community and call attention. So we didn�t really expect policy change to immediately happen, but I think that the initial signs were incredibly successful. So they were able to get a meeting with the vice president; Vice President Joe Biden at the time, and had some follow-up with his staff.\\n',\n",
       " '\\n',\n",
       " 'They were able to get several hearings in front of Congress and there was at least some members of Congress who seemed like they might really have the potential to become champions of this cause, and really bring it on as one of their priorities, who seemed to have brought a lot from the hearings, so those seemed to be really good initial signs of success. And OpenPhil actually, since then, has renewed the grant and given them more funding to continue to work and to sort of do follow-up work saying, �We identified these priorities. Has the government made any progress since then?�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Are there any other grants that it would be interesting to talk about?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think the second one that was really interesting was a grant to an organization called iGEM, which is actually a student competition where undergrad students work on really cutting edge synthetic biology projects, which essentially means that they are combining DNA themselves to modify organisms to do really useful things. So one example of that was neat is that you can modify bacteria and basically make that bacteria into a sensor, so you can have it � An example that was really cool was a project where students created a drug-testing bacteria, so someone could find out whether or not some drug; heroin, was pure, or whether it had been adulterated, by basically having the bacteria react differently to different types of the drug.\\n',\n",
       " '\\n',\n",
       " 'And it�s really exciting work, some of the really cutting edge, synthetic biology is happening there and also a lot of those undergrads are going to grow up and be some of the top synthetic biologists down the road. We were really excited about the work that they�re doing, but it�s pretty easy to see how in the long run, this ability for more and more people to be able to modify organisms could also come with safety and security risks. So OpenPhil gave a grant to iGEM�s safety and security team to give them more resources to do things like train those students in how to work safely with the organisms they were working with, because right now the iGEM students are not working with anything particularly dangerous, but those same students down the road could be working with more dangerous organisms.\\n',\n",
       " '\\n',\n",
       " 'And then give them resources to teach the students about security culture, and teach them to be guardians of science, and synthetic biology as the field can only really work if you have a culture where everybody realizes that this is a technology that could have huge benefits, could also have huge risks. So the goal of the grant was both to give iGEM resources to have a really good safety and security program, but also to be able to experiment a little bit with that program, because down the road, what we really want is not just this one competition to have a really good safety and security program, but to know in general, what types of things work for creating a culture of safety and security in the DIY bio community in general.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you know whether that grant worked out yet?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I think we�re still too early to tell. I think we�ll find out down the road.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So we�ve touched on it, but now let�s really dive into the nature of the pandemic risks we face. Why is pandemic preparedness such an important thing to work on? Like possibly the most important thing for people to be working on?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Sure. I think the way to start to approach that question is to ask � And the way that OpenPhil originally approached that question was to first ask, why do we care so much about working on reducing global catastrophic risk general? and that ended up being a real priority for the Open Philanthropy Project because we care a lot, both about the well-being of current people, but also about the well-being of people down the road in the future generations, really protecting all of the good things that civilization�s brought for people down the line. So we started out by asking � Right now we have this trajectory where global poverty has been going down for a long period of time.\\n',\n",
       " '\\n',\n",
       " 'It seems like the well-being of most people on earth has been getting much better and there�s a lot of hope that in the future that will continue to happen. Are there things that could really destroy that, set us off track? So we went through a process of looking at the main candidates for events that could really disrupt that progress, and if we look at everything from climate change, to pandemics, to risks from emerging technologies to asteroids, war, yes, nuclear conflict, and try to first ask, �What would it take to really get civilization off track?� And that sort of heuristic that we came up with, is we tried to ask, �What are the events that could lead to something like hundreds of millions of deaths in a short period of time?�\\n',\n",
       " '\\n',\n",
       " 'When we asked that, and then limit it to areas where there wasn�t enough philanthropy yet and where there were apparent things that philanthropy could really do about it, biosecurity and pandemic preparedness was really towards the top. And I could go through a few reasons why that was the case. When we talk about biosecurity and pandemic preparedness, we usually split it into two categories. One of them is risks from natural pandemics. It could be everything from the flu, to smallpox, to HIV and AIDS. And the other is risks from pandemics that might be caused by humans. So that kind of splits them into two categories. One of them is potentially the use of biological weapons.\\n',\n",
       " '\\n',\n",
       " 'A second is the possibility of an outbreak that starts from a lab accident. So somebody doing research, working with potentially dangerous organisms, and accidentally allows some of that to get out and starts an outbreak that way. We looked into both of those as part of the cause area, and natural pandemics, at least in the short run, seem much more likely to occur. We have a long history of natural pandemics occurring. We had several natural pandemics over the last century. Most recently was the flu pandemic in 2009 and new outbreaks are emerging all the time. So if we just want to ask, �What�s the most likely source of a big outbreak over the next several years?� That�s going to be where it comes from.\\n',\n",
       " '\\n',\n",
       " 'Then the second question that we asked is, �Could a natural pandemic really get to that level of a global catastrophe?� And that�s a more difficult question for us. We have a lot of examples of outbreaks that were definitely tragedies, where there were thousands of deaths that could have been preventable, but we also have some pretty good evidence of society coming back and being fairly resilient and making it through those events. So we want to ask, �Could a natural pandemic really rise to the level of our sort of threshold of hundreds of millions of deaths?� The main thing that we looked at for evidence was the Spanish flu, which occurred back in 1918. That pandemic killed about 3 to 5% of the world�s population, which if you projected that out to today, ends up at�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: � million, 200 million.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, exactly � deaths. So that at least gives us a sense that a one-in-100-year-type pandemic might reach that threshold. Those are really difficult questions about if that occurred today, would it be similarly bad? I think it�s hard to know, and having talked to a lot of the experts in the field, opinions really differ. On the one hand, healthcare�s gotten better. On the other hand, we�ve had globalization and outbreaks can spread a lot faster. So when we look at, at least the possibility, it seems possible that you really could have a pandemic of that size from natural causes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: There�s also just a lot more people in some very dense cities, so of course, it�s very difficult to control diseases, right?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes. That�s a factor too. And then there are phenomena like global warming, which some folks at least think could really increase pandemic risk in the near future as it forces people to migrate a little bit, and move to areas, and come into contact with animals and disease reservoirs that they haven�t touched before.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So, the Spanish flu in 1918, �19 killed 3 to 5%, which today would be 200 million, 300 million, but that wasn�t even the worst pandemic in history, right? Because it also had the Black Death and smallpox when it got to the Americas. I�ve read stats where as high as 50% of the population have died, or we think historically they died in the very worst cases, when the Black Death came through some parts of Europe. Is that right?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, so I�m not sure if we�ve gotten up to 50% of the global population�\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove timestamps and speaker names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regex library \n",
    "no_time = re.sub('\\[([0-9]*\\:[0-9]*\\:[0-9]*)\\]', '', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m6/hdvcc7791jx1bd9d61b94drc0000gp/T/ipykernel_47796/3927456817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mno_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[a-z]*\\s[a-z]*\\s\\:]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/regex/regex.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags, pos, endpos, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     object and must return a replacement string to be used.\"\"\"\n\u001b[1;32m    277\u001b[0m     \u001b[0mpat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_unused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m def subf(pattern, format, string, count=0, flags=0, pos=None, endpos=None,\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "no_name = re.sub('[a-z]*\\s[a-z]*\\s\\:]', '', lines, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mod_text.txt', 'w') as f:\n",
    "    f.write(no_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mod_text.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lines: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robert W: Hi, I�m Rob, Director of Research at 80,000 Hours. Today I�m speaking with Miles Brundage, research fellow at the University of Oxford�s Future of Humanity Institute. He studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.\\n',\n",
       " '\\n',\n",
       " 'Miles�s research has been supported by the National Science Foundation, the Bipartisan Policy Center, and the Future of Life Institute. Thanks for making time to talk to us today, Miles.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage:  Thanks for having me.\\n',\n",
       " '\\n',\n",
       " 'Robert W: First up, maybe just tell us a bit about your background and what questions you�re looking into at the moment.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Sure. I used to work on energy policy when I was living in Washington DC, and I�ve gradually moved into AI policy over the past few years, because it seems like it is more of a neglected area and could have a very large impact. I recently started at the Future of Humanity Institute and I�m also concurrently finishing up my PhD in human and social dimensions of science  and technology at Arizona State University. In both capacities, I am interested in what sorts of methods would be useful for thinking about AI in a rigorous way, and particularly the uncertainty of possible futures surrounding AI. I have a few more specific interests such as openness and artificial intelligence and the risks related to bad actors.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Are you trying to predict what�s gonna happen and when and what sort of effects it will have?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage:  Not necessarily predict per se, but at least understand what�s plausible. I think that it�s very difficult to predict with any high confidence what�s gonna happen and when, but understanding, for example, what sorts of actions people could carry out with AI systems today or in the foreseeable future in different domains like cybersecurity and information operations, production of fake news automatically  and autonomous drones and so forth. I think understanding the security landscape of those sorts of things doesn�t necessarily require a very detailed forecast of when exactly something will occur so much as understanding what�s technologically possible given near term trends. Then you don�t necessarily have to say this is when this particular accident or malicious behavior will occur, but just that these are the sorts of risks that we need to be prepared for.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Does that bring you into contact with  organizations that are developing AIs like Google or Open AI?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, absolutely. Those organizations have an interest in making sure that AI is as beneficial as possible, and they�re keenly aware of the fact that they can be misused and that there might be accident risks associated with them. For example, I held a workshop fairly recently on the connection between bad actors and AI and what sorts of bad actors we might be concerned  about in this space, and we had some participation from some of those groups.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of bad actors are you thinking of?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: You can look at it from a number of different perspectives. Depending on the domain that you�re interested in, you might end up with different sorts of actors. In the case of physical harm associated with AI, you might look at autonomous weapons and the weaponization of consumer drones, and in that case it�s something that would be  particularly appealing to state actors as well as terrorists, and we�re already seeing the weaponization of consumer drones, though without much autonomy if any, in the case of ISIS overseas. There are cases like that where we can foresee that particular groups when given more advanced capabilities such as higher baseline levels of autonomy in consumer drones, would be able to and would like to carry out damage on a higher  scale.\\n',\n",
       " '\\n',\n",
       " 'In other cases, it�s less clear. For example, there are concerns around mass surveillance, and it�s often not totally clear whether one should be more concerned about states or corporations, depending on what sorts of risk you�re concerned about. For example, corporations currently have a lot of data on people, and there have been a lot of concerns raised about the use of AI to make decisions that have critical impacts on  people�s lives, such as denying loans and court decisions and so forth that are being informed by often not very transparent algorithms. Those are some concerns, but there�s also a potentially different class of concerns around authoritarian states using AI for oppression.\\n',\n",
       " '\\n',\n",
       " 'I think there�s a range of risks, but generally speaking there�s going to be a steady increase in  the floor of the skill level as these AI technologies diffuse. That is, there will be more and more capabilities available to people at the bottom of the scale, that is individuals as well as people with more access to computing power, money, and data at the higher end. It�s not totally clear how to think about that because on the one hand you might expect that the biggest actors are the most concerning because they have the most skill and resources, but on the other hand they also  are sometimes, in the case of democratic governments, held accountable to citizens, and in the case of corporations held accountable to stakeholders. You might also think that you would be more concerned about individual rogue actors. That�s an issue I�m still trying to think through in the context of the report that I�m writing based on that workshop.\\n',\n",
       " '\\n',\n",
       " 'Robert W: People often distinguish between the short term risks from AI, like self driving cars not working properly or algorithmic  bias, and then the longer term concerns of what we�re gonna do once artificial intelligence is as smart as humans or potentially much, much more intelligent. Which one of those do you spend more time working on?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I try to focus on issues that span different time horizons, so I think that AI and security is something that we�re already seeing some early instances of today. It�s already being used for detection of cyber threats, for example, and the  production of vulnerabilities in software. There�s a lot of research going on, which an example being the DARPA cyber grant challenge recently, where there�s automated hacking and automated defense. There�s already stuff happening on the front of AI and security as well as the economics of AI and issues surrounding privacy, but it�s not totally clear whether the issues in the future will just be straightforward extensions of  those or whether there will be qualitatively different risks. I think it�s important to think about what�s happening today and imagine how it might progressively develop into a more extreme future, as well as to think about possible discontinuities when AI progresses more quickly.\\n',\n",
       " '\\n',\n",
       " 'Robert W:I should say this point, that we have a profile up on our website about the potential upsides and downsides of artificial intelligence, where we go through all of the basics.  Miles, you have a forthcoming guide to work on AI policy and strategy that we�re gonna link to. Here, we�re gonna try to go beyond that. If you find yourself a little bit confused, then potentially just go on to read one of those documents first.\\n',\n",
       " '\\n',\n",
       " 'Which policy or strategy questions do you see as most important for us to answer in order to ensure that the development of artificial intelligence goes well rather than in a bad way?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think there are a lot of questions, and some of the ones I was talking about earlier surrounding security  and preventing the weaponization of AI by dangerous actors is one. There are also other issues that have been widely discussed in the media and by academics, such as accountability of algorithms, transparency, the economic impacts of AI and so forth, but one that I think is particularly important, particularly over the long term, this is a case where there might be a need  to think about possible discontinuities, is coordination surrounding AI safety. There�s been a lot of attention called to AI safety in the past few years and there�s starting to be a lot of concrete and successful research on the problem of avoiding certain AI safety failure modes, like wire heading and value misalignment and so forth, but there�s been less attention paid to how do you actually incentivize people to act on the best practices and the good theoretical frameworks  that are developed, assuming that they will be developed.\\n',\n",
       " '\\n',\n",
       " 'That�s a potentially big problem if there�s a competitive situation between companies or between countries or both and there�s an incentive to skimp on those safety measures. There�s been a little bit of work, for example, by Armstrong et al at the Future of Humanity Institute looking at arms race type scenarios involving AI where there would be an incentive to skimp because you�re concerned about losing the lead in an  AI race. We don�t really know what the extent will be, but it might turn out that there are significant trade offs between safety and performance of AI systems. One might be tempted to, in order to gain an advantage whether economic or military or intelligence-wise, ramp up the capabilities of an AI system by adding more hardware, adding more data, adding more sensors and effectors and so forth, but that might actually be dangerous if you don�t understand the full  behavioral envelope of the system, so to speak. And how to constrain human actors from doing that is a very difficult problem.\\n',\n",
       " '\\n',\n",
       " 'I, along with others at the Future of Humanity Institute and other different organizations have started thinking about what is an incentive compatible mechanism to ensure that people do the right thing in that case. That is to say, we don�t want to ask people to do something that�s totally against their interests if  they actually do have an interest in developing these systems and protecting themselves, militarily or otherwise, but we also want to ensure that they�re constrained in some sort of way. One example would be, this is not necessarily a fully fleshed out example, but just to illustrate the sort of thing I�m talking about, would be some sort of arrangement between AI developers such that in order to gain access to the latest breakthroughs or the latest computing power, you would need to submit to some sort of  safety monitoring and adhere to certain best practices.\\n',\n",
       " '\\n',\n",
       " 'That would create an incentive if you want to be on the cutting edge to participate in those safety protocols. Again, that�s not a fully worked out example, but that�s an example of the class of things that I would like to see more of. Developing specific proposals for how to ensure that the right thing that�s being developed on the technical front actually gets implemented.\\n',\n",
       " '\\n',\n",
       " 'Robert W:  Do you look much at, say, what governments should be doing in this area? Whether there�s regulations that we should be putting in place or at least preparing to put in place in future?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. I think that that ties in with what I was saying a little bit, in that one might envision that as AI capabilities develop, that it�ll be increasingly seen as a matter of national security that a country be on the leading edge. I think it�s not super clear how  to navigate that yet. I think there�s some low hanging fruit. Clearly, in my opinion, it would be beneficial for countries to have some experts in house in their governments who actually know something about AI and are able to deal with crises as they arise, if they arise, and to be able to think carefully about the impacts of AI on the labor force, for example.\\n',\n",
       " '\\n',\n",
       " 'I think there�s some low hanging fruit and some policies that have been developed  for dealing with some of the near term issues, but for the longer term issues, it�s not super clear that we want to rush into a situation where governments are leading this, if that would turn out to only accentuate the arms race dynamics that we should be trying to avoid if AI is seen even more as a national security issue in an unhelpful way. I think it�s incumbent on those who are thinking about the long term policy issues around AI to develop a more positive  proposal that involves not just the US government getting involved for the purpose of accelerating American AI capabilities, but a more collaborative approach.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I suppose with previous dangerous technologies like nuclear weapons or chemical or biological weapons, there�s been agreements to try to slow down their development and deployment. Is that something that could potentially happen here at the international level?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think it�s possible  to imagine some sort of slowing down among a small number of actors, and we�re fortunate that there is a high concentration of computing power and talent and skill in a fairly small number of organizations. There�s pretty much no chance of some random person out competing the top organizations in AI in a surprising way. It�s possible that with some of the top companies and  nonprofits as well as countries, they could coordinate in some sort of way. Not to postpone AI across society for a long period of time, but at least to be cautious in the later stages of development and to allow time for mutual vetting of safety procedures and things like that. I think it�s possible to imagine some sort of coordination, but the technical and the political factors interact to a large extent.\\n',\n",
       " '\\n',\n",
       " 'To the extent that we actually have good safety measures that  are efficient and that don�t introduce a lot of overhead, computational or otherwise, into these systems, then we�ll be in a better place to actually get people to coordinate because it won�t be imposing a lot of costs on them. Likewise, to the extent that we�re able to coordinate better, we�ll be in a better position to actually get people to implement those mechanisms if they do impose some performance penalty.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Can you think of any major bits of progress we�ve made on Ai strategy  questions over the last couple of years?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: The example I mentioned of the paper by Armstrong et al called Racing to the Precipice is one example, and it presented sort of a stark version of the arms race scenario, that I think there are reasons to be more optimistic than are presented in that paper because they look at only a certain set of assumptions, as with any model. More generally, I think there�s been progress in  the development of principles and criteria for good policies in recent years. For example, with the Asilomar conference and the set of Asilomar AI principles, I think that was a good step towards developing some shared understanding around the need to avoid arms races and concerns over AI weaponization and so forth.\\n',\n",
       " '\\n',\n",
       " 'I still think that there�s a lot of room for progress to be made. There was a paper  put out by the Future of Humanity Institute by Nick Bostrom, Carrick Flynn, and Allan Defoe recently that looks at policy desiderata for the development of machine superintelligence. There�s a lot of good material there, but I think they would be the first to admit that we�re still at the high level principles stage of developing these policies. We have a better understanding of what we want to avoid than we did a few years ago, and what a good policy proposal  would look like, but we don�t yet have anything super actionable.\\n',\n",
       " '\\n',\n",
       " 'I think the situation is a bit analogous to where AI safety was a few years ago, where there was starting to be an articulation of what the problem was with the book Superintelligence and various other publications. People were starting to take the problem seriously and started to have a vocabulary for what a solution would look like with value alignment and  other terms being coined, but there wasn�t yet any concrete research agenda. Subsequently, there have been a lot of concrete research agendas, as well as technical progress on some parts of those research agendas with work by Open AI and Deep Mind and others leading the way. I think we�re potentially in a similarly exciting phase on the AI policy front where we have a decent understanding of what the problem is.  With the example of avoiding arms races being forefront in my mind, at least, but we don�t have clear models and case studies that we can point to as ways forward. I think that�s the next step, is moving into more concrete proposals and trying to balance some of the trade offs that have been identified in recent papers.\\n',\n",
       " '\\n',\n",
       " 'Robert W: It seems like a lot of the latest thinking in this area isn�t  written up in public yet, partly because people don�t want to publicize their views at this point or just because it takes a long time to get papers published. What do you think�s the best way for someone who�s interested in working in the area to get up to speed on the issues?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think, again, it�s somewhat analogous to the situation with AI safety a few years ago. I think the book Superintelligence was a big step forward on that front in terms of having a single reference to point people to.  It�s less clear if there�s one single reference to point people to on AI policy issues, though I�m hoping that the career guide that I�m working on will be somewhat useful in that regard. Another resource that comes to mind is a syllabus for a class on the global politics of AI developed by Allan Dafoe at [Oxford] and the Future of Humanity Institute. That�s a very detailed list of resources. I�m sure there�ll be a link  provided for this interview somewhere.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: That�s another set of resources. It�s essentially a long list of both formal academic publications as well as things in the more gray literature, such as blog posts, which is, I think, the sort of thing that you�re referring to as things that aren�t necessarily written up academically, but are somewhat accessible. Even there, there�s still some things like  Google Docs that are used internally and so forth. If, based on reading those sorts of things, this is clearly something you�re interested in, then the obvious next step is just to get in touch with people who are working on these issues and indicate what your interests are. There might be things that aren�t available online that they can point you to.\\n',\n",
       " '\\n',\n",
       " 'Robert W:You mentioned the Asilomar statement of principles. Do you just want to describe that?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Sure. Two  years ago, there was a conference on beneficial AI held in Puerto Rico. That led to an open letter on AI, and subsequently some investment by Elon Musk in the Future of Life Institute, which supported a lot of grants in this space. I think that that led to a lot of attention to the issue and there were thousands of people, including a lot of AI scientists, who signed onto that letter, and then subsequently  the Asilomar principles, which were developed at a conference two years later, this year at Asilomar in California, developed a more specific set of proposals for what sorts of things AI scientists should be thinking about. Not just the fact that there should be more research, but also things like capability caution, so being attentive to the fact that we don�t know for sure what the upper limits of AI capabilities  will ultimately be, and the things I mentioned about avoiding arms races and being concerned about AI weaponization more broadly.\\n',\n",
       " '\\n',\n",
       " 'I think that�s a good example of developing a consensus view on what we want to see and what we don�t want to see. We want to see AI benefiting society as a whole, and we don�t want to see it leading to the accruing of benefits to a small number of people or to large scale war  or anything like that. I think it�s encouraging to see not just that, but also the development of other sets of principles through the IEEE and their effort on ethically aligned AI design.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I read those principles and they seem quite strong on paper. Do you think people are likely to follow through on them?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think there�s somewhat of a gap between the level of abstraction  of the principles and concrete steps that people can take. For example, it�s not totally clear what any individual can do to stop an arms race, for example, but I think it�s a step in the right direction to know where you�re headed, and then figuring out exactly what to do about that is the next step.\\n',\n",
       " '\\n',\n",
       " 'Robert W: In your guide, you suggest that people could potentially work on AI strategy and policy questions at places like Google Deep  Mind or Open AI, where artificial intelligence is actually being developed. What concretely could you see people doing at places like that if they took a job there?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: To some extent, people at those organizations are doing something fairly similar to what I and my colleagues are doing in academia, which is thinking about what the problems are and trying to develop solutions. I think the benefit of being on that side of things is to have  more direct exposure to what�s happening on the ground, so to speak, in AI development, and I think that can be useful for developing a set of what sorts of problems are actually cropping up as a result of the development of capabilities that actually exist as opposed to just ones in the future.\\n',\n",
       " '\\n',\n",
       " 'I think it�s also notable that people at these organizations have been very active in the efforts that I�ve mentioned, such as raising attention to AI safety, which was an initiative that  involved people at Google Brain and Open AI and the concrete problems in the AI safety paper, and people at DeepMind have been very influential in calling for more AI safety work and they�ve been publishing fairly early on this matter relative to other organizations. I think there�s a lot of reason to think that these organizations are playing a positive role and I think it would be a good place to be involved in thought leadership on  these topics, as well as doing direct research.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How hard it is to get a job at a place like that? I would imagine they not only have a few people working on AI strategy or policy. Are these extremely competitive roles?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I would say they�re pretty competitive, and I think that�s a general phenomenon in anything related to AI these days, whether on the policy side or the technical side. I think that this is a growth area for  sure. If one develops expertise in this topic and has something to contribute, then I think there will ultimately be opportunities available to you in the next few years. Again, I want to draw an analogy to AI safety where there was a lot of concern about will there be jobs in AI safety three years or so ago, and I think that deterred some grad students from working in this space. Now the situation seems to look a bit better in that there�s a fair number of advertisements for  postdocs and people being hired at top labs to work on these issues. I think we�ll probably see something pretty similar, not necessarily it becoming not competitive, but there being a little bit of slackening over time as there�s a need for developing larger teams to work on these topics.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Another path you�ve talked about is going into politics or policy roles, for example as a Congressional staffer  or perhaps at a think tank like the Brookings Institute or the National Science Foundation, or potentially just going into party politics in general. What useful things do you envisage people could do there?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think in the same way that working at an AI lab would give you a better sense of what�s practical on the technical front, working for Congress or Parliament or a political party would give you a better sense of what�s practical on the political front. In addition to that,  developing a better intuition for the system that you�re dealing with and what the constraints are, I think it�s also potentially a good way to actually influence what�s actually done by those bodies. For example, in the US Congress right now, there was just recently announced an AI caucus to organize the discussion of members of Congress on issues related to AI, with particular focus on issues related to the  future of work. I think over time, we�ll see more discussion of the longer term sorts of issues that we�ve been talking about in those fora and they would be useful to have people who are concerned about making sure that the right thing is ultimately done working in those places and able to actually act on the best current understanding developed by researchers and by practitioners.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I think a big risk of going into politics or policy  in general would be that your career progresses, but you�re not specifically in one of the places that ends up having a say in how these things go. You just end up working in a Congressional committee that turns out not to be that relevant. Are there any roles that you can take early on, or what can you do to position yourself well so that you don�t just get sidelined in the end?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think if you intend to work in politics, then you have to be somewhat opportunistic about  jobs and the ebb and flow of political opportunities. For example, if one Congressional committee is declining in its relevance in terms of AI, then one should consider about jumping ship towards somewhere that�s more relevant. I don�t think that taking one particular job will lock you in forever, but building up some political capital and some human capital in that area could be useful,  even if you ultimately want to switch to a different organization.\\n',\n",
       " '\\n',\n",
       " 'I would push back a little bit on the idea of not being relevant just because you�re not at the organization where people are developing the latest AI. As I mentioned before, I think that�s a very exciting opportunity and along with academia, it�s a great place to work on these issues. Governments still play an important role in framing these issues  and in convening discussions. An example of this would be the preparing for the future of artificial intelligence report put out by the White House Office of Science and Technology Policy last year, which was the result of four workshops that brought together experts in a wide variety of areas: in law and economics and AI technology and safety. I think you still have an opportunity to  frame the discussion and move the ball forward, even if you�re not working in the latest labs.\\n',\n",
       " '\\n',\n",
       " 'Robert W:What would be some of the best places to apply to in that area?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: It�s a bit tricky to answer that in general. It depends on factors like what your citizenship is and what particular policy issues you�re interested in, but some fairly robust recommendations would be to  look � For a particular class of people that have technical backgrounds, one area to look at would be AAAS fellowships. The American Association for the Advancement of Science has a fellowship program where they essentially put people in rotations in organizations like Congress and the executive branch, where they can draw on their technical  experience, and someone with a background in AI would probably end up in an AI relevant organization. I have heard a lot of good things about that being a good way to develop career capital and experience and understanding of how the political system works, but that�s not necessarily going to result in being in the exact right location for solving AI policy problems the next few months.\\n',\n",
       " '\\n',\n",
       " 'I think it�s really hard to say where that would be or even if there is such a place because I think we�re still,  despite all the progress that�s being made, in a relatively early stage. There isn�t even yet a nomination for the Director of the Office of Science and Technology Policy in the White House. Otherwise, I would say that that is a good place to go. Likewise, we don�t really know what the agenda of the Trump Administration is on the AI front, and there�s a lot of discussion,  but not a lot of institution building in governments at the moment. It�s hard for me to answer that in general, but I think that getting experience is a pretty robust thing to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: A situation that I encounter reasonably often is I meet someone who�s really smart, who is very interested in this topic, who might be able to make a great contribution. At the moment, there aren�t that many groups that are hiring for these roles.  You�re at the Future of Humanity Institute, and I guess there�s also the Future of Life Institute, but there aren�t that many places that someone can potentially apply. What advice should I give someone in that situation? Should they continue studying, perhaps, or building expertise so that they�re in a better position to apply in the future when the number of positions grows?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Again, it�s hard to answer that in general. It depends on what background the person has and what sorts  of jobs they�re interested in. I think if one casts a fairly large net in terms of what AI issues one wants to work on, there�s a fairly large range of organizations. It�s not just FHI. There�s also the Center for the Study of Existential Risk, the Leverhulme Center for the Future of Intelligence, the Tech Policy Lab at the University of Washington, and various faculty programs.  Various academic programs around the country, in the US, and around the world are looking to hire people, for example, as postdocs or a faculty member in areas related to AI and policy if you�re someone with an academic background. There are probably a fair number of policy positions at tech companies that aren�t necessarily specific to AI, but that would lead to getting valuable experience in the broad area of tech policy.\\n',\n",
       " '\\n',\n",
       " 'To  generalize a bit, I would recommend casting a fairly large net and working in adjacent areas if you can�t immediately work in the exact right place. This is not the sort of area where people have been working on AI policy for decades and it�s hard to break in for that reason. It�s more just that it�s still a field that�s growing. I don�t think that that growth will stop. I think there will be more opportunities in the future, but if you can�t find the right opportunity  now, then talk to people in the field about what sorts of opportunities are on the horizon and try and work in adjacent fields. That might be one thing to consider.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What do you think would be the biggest challenges for someone starting out, trying to start a career in this area?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Depending on one�s background, I think the main challenge would be boning up on areas of weakness. For example, if you have a policy background, you might want to focus on  learning more about AI, and if you have a technical background, you might want to focus more on learning about policy. I think one of the challenges is what you mentioned before about there not being a super clearly accessible literature on the topic, and I think there�s some effort being going into addressing that with the career guide and the bibliography that I mentioned.\\n',\n",
       " '\\n',\n",
       " 'I�m not really sure that there�s any specific  set of pitfalls that I would recommend that people avoid besides not neglecting the areas in which they�re weak. Don�t try and rest on your laurels in, say, technical areas or policy areas, because it�s a fairly interdisciplinary area, and you would need to think about what sorts of disciplinary perspectives would be beneficial to solving the problem you�re interested in, and not just what is your background, though of course you want to draw on your strengths.\\n',\n",
       " '\\n',\n",
       " 'Robert W:  Because it�s a field that is in its fairly early stages and is growing quickly, it�d be a good fit for someone who was really able to set their own direction and meet people and potentially attract funding to do what they want to do, someone who�s gonna create the opportunities that they want to go into.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, it�s an area that�s evolving and I think a lot of the organizations that exist today and I have mentioned as  good places to work and so forth didn�t exist five years ago.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Some of them didn�t exist one year ago.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, some of them didn�t exist one year ago. It�s an area in flux, and I think as with politics, you should be opportunistic about thinking about what�s the right place for you to be in. Yeah, I think it�s a very exciting area to work in and I think that a lot of people will find that it�s an area that would benefit from  their background.\\n',\n",
       " '\\n',\n",
       " 'Robert W: If you had someone that could go into any organization or role and that�d be a good fit, what kind of person would you think is most valuable out of everything.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Most valuable person. I don�t really have an answer for that.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You don�t have an answer for that.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Is there any, say, vacancy out there that you just wish that someone could fill it?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: One vacancy that comes to mind is for the policy researcher  position at DeepMind. They�re currently hiring for someone to specifically look at AI policy. I think the case is pretty clear that that would be a good place to work and a good role to have if you�re interested in influencing the conversation around AI policy and being exposed to the latest developments. Yeah, if you�d be a good candidate for that and you�re interested in that, you should definitely apply.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Sometimes I find people who want to work on AI and policy because they don�t see themselves as  quite cut out for technical research, perhaps because their math skills aren�t quite good enough. Is that a sensible way to go, or is in some ways the AI strategy work harder just because it�s less clear, it�s less precise than technical work?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I wouldn�t say it�s harder or easier. It�s just different. Even on the technical side, there�s a lot of fuzziness. AI safety wasn�t very crisply defined a few years ago, and it�s starting to move more in the direction  of technical rigor. I think the same thing will happen to AI policy to some extent, but there will also always be some element of relationship building and qualitative analysis and so forth. That certainly is not exactly the same as solving technical problems. Yes, I do think that having a different background, it would make sense to choose different areas to work with based on your background, but I wouldn�t say that working  on the safety problem is totally technical and the policy problem is totally non-technical, but certainly there�s some correlation there.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kind of level of technical understanding do you have to get to in order to be able to make a useful contribution?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I�m not sure that you need any specific level in order to make some contribution. For example, there are a lot of roles that can be played by people who are just good at distilling the literature on a particular topic. For example, if you�re interested  in understanding the role that AI could play in authoritarian states, then you don�t necessarily have to have any technical background to read up on the literature around surveillance and coercion and things like that. I think there�s a lot of room for good synthesizers and people who are curious and interested to dip in areas they�re unfamiliar with.\\n',\n",
       " '\\n',\n",
       " 'I think there�s also a fairly high ceiling for what would be useful. I think it would also be  good to have people with very strong technical backgrounds thinking about the nitty gritty game theory issues related to arms races and so forth. I think it�s a pretty broad range of possible skill sets that would be useful.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You�re thinking if you can just understand what, you can speculate about what an AI might be capable of doing in the future. Then if you know economics or if you know social science, then you can think, what implications would that have  for politics and that kind of thing, even if you don�t understand specifically how the AI would work.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. Ideally, you know a little bit about each of those things, but you don�t need to be an expert in everything. It�s impossible to be an expert in everything. It�s not feasible to learn about every single discipline, of which I�ll list several in this policy career guide that you mentioned. Clearly, you cannot be a deep expert in  technical AI, economics, sociology, political science, etc., but knowing a little bit and being able to collaborate with people with complementary skill sets is also very valuable. I think a lot of the most exciting work in the next few years will involve collaboration between people on the technical side and on the policy side.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How can someone tell which kind of paths they�re most suited for?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I don�t think that there�s any clear algorithm for  this, but talking to people in the area and reading some of the literature and thinking about where you think the gaps are, what areas you think that you could shed light on given your background is definitely something you should do. You should look at what sorts of job opportunities are available and cast a fairly broad net because you don�t necessarily know what�s going to strike your interest until you stumble upon that opportunity, and networking is also super helpful.  I think not just in terms of finding out about job opportunities and figuring out where there�s a nice fit, but also finding out about different organizational cultures and where you might fit in is also useful. I don�t think there�s a single algorithm, but networking and reading more of the literature and looking at job postings are all good things to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of places can people network? Are there any events that are open to people who don�t yet have roles?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage:  All of the AI conferences are open to anyone who is capable of paying for the trip and the ticket. Some examples of big AI conferences are NIPS, ICML, IJCAI, AAAI, and so forth. NIPS is probably the biggest  machine learning conference right now with an emphasis on deep learning. ICML is also a very large one. Actually, I�m not sure which one of those is bigger. I haven�t been to ICML, but I�ve been to NIPS a few times. ICLR is also a deep learning specific conference. I think going to conferences like that and specifically going to workshops and symposia that are relevant to policy questions, which often happens at these conferences.\\n',\n",
       " '\\n',\n",
       " 'For example, at NIPS last year, there was  a symposium on the AI and the law. There are opportunities like that to network and find out more about how people in that area are thinking and to meet people who have similar interests. That�s just on the AI side. It�s also we�re thinking about more policy oriented conferences. There�s some conferences like the Governance of Emerging Technologies Conference, the We  Robot conference series, and probably a few others that would be of interest to people who are either on the technical side and want to move in a more thinking about policy side, or just they�re in the policy side but they want to specifically seek out people interested in AI. Those would be good places to look.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Are any of the organizations involved open enough that someone who�s interested in the area can drop by and get to know people?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah. I think  particularly on the academic side, though to some extent also at corporations, there�s a fair amount of openness. For example, at FHI, we host a lot of visitors who just want to learn more about our work. The same is true of other organizations I�m familiar with.\\n',\n",
       " '\\n',\n",
       " 'Robert W: It sounded like in your guide, you thought the best undergraduate major was something like combining a technical subject, like maths or  computer science with a more social science topic like politics or economics. Assuming that post people listening to this have already chosen their major, what should people do at the postgraduate level? Should they do a PhD or just go and work directly at Google if they can?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I�ve probably said this a million times, but I don�t think there�s a clear answer for everyone.  Certainly it doesn�t hurt to have an advanced degree. Of course there are opportunity costs, but there�s certainly more job opportunities available for someone who has a PhD. For example, I don�t know if it�s a hard requirement, but certainly it would be a benefit for people applying to a fair number of industry jobs if they had an advanced degree, as well as in academia. It�s hard to avoid the fact that for academic jobs, as much as  we would like to hire people solely based on their demonstrated skill, it�s often also the case that there are university policies surrounding what sorts of degrees you need for different roles and pay grades and so forth.\\n',\n",
       " '\\n',\n",
       " 'There�s definitely value to having a graduate degree, despite the non-trivial opportunity cost if you have an opportunity. As with all these things, it depends on someone�s background and if you have a clear opportunity to make an impact, and  you could perhaps get some of the best of both worlds by also taking classes online or enrolling at a local university and being a part time PhD student or master�s student. There might be ways to have your cake and eat it too, but it would depend on the particular person.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I guess the most obvious options at the postgraduate level are things like machine learning or computer science or economics or I guess public policy? Is that useful?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage:  Just to give a particular example, at Oxford we have a handful of people who are interested in AI safety and AI policy, as well as broader issues related to the future. Just at the organization that I�m at, FHI, there are people who either are currently enrolled or who soon will be enrolled in programs in mine, which is human and social dimensions of science and technology, as well as machine learning, cybersecurity, and zoology. There�s  a wide range of possible areas that one could study and find an advisor that is suitable and interested in supporting interesting work.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Some people worry that doing work in this area could be harmful if not done well. For example, it could be that, as you were saying earlier, that regulation in this area could be done poorly and would actually be harmful overall, or that bringing greater attention to the issue could reinforce the arms race dynamic, or tentatively, having people  get involved who don�t have enough technical expertise could bring the area into disrepute. How seriously do you take those kinds of concerns? Should people be cautious about jumping into the area if they don�t feel like they�re really on top of things?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think that is a reasonable concern, and there is, in fact, some risk of discrediting yourself or your cause by being too alarmist or whatever the particular  charge might be. On the other hand, there�s a lot of noise in the system no matter what. Whatever you do in the near term, there will be Terminator pictures and people who dismiss AI safety and all sorts of other things you can�t really control. I think they�re also risks of not acting, which is not being able to help out when there�s an important program and you have something to contribute. I would say if that�s something that concerns you, then talk to people about your beliefs and  what your beliefs are, and look at surveys of experts and see what the reasonable distribution is among people who have studied the topic more. I don�t think it�s an all or nothing sort of thing where you either have to stay ignorant or be a super expert. I think there�s a middle ground where you develop your confidence little by little as you go, and try and remain as modest as possible in areas that you have little expertise.\\n',\n",
       " '\\n',\n",
       " 'Robert W:  I suppose you could always track the things that people like you or Nick Bostrom are saying and how you speak about the issues, so that you don�t come across as alarmist and you have a measured and precise tone.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Right, right.\\n',\n",
       " '\\n',\n",
       " 'Robert W: You mentioned earlier that AI could potentially be used in military applications, and that�s one way that things could go badly. Given that, do you see much value in people going into military or intelligence roles with the hope of limiting this in the future?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Possibly.  It�s really hard to say. It would depend on the particular role, so probably a role higher up in the chain and more on the policy and strategy sides of things, as opposed to the more operational or logistical or just execution side of things would be more likely to give you an opportunity to affect those sorts of things. There might also be benefits to that beyond just having a direct impact on the issue while in that  role. You could also then develop valuable connections and know the right people to influence from the outside down the road or have a better understanding of the dynamics of the problem and what needs to be done to solve it. I definitely wouldn�t dismiss it, but I also wouldn�t necessarily recommend it across the board as the best thing to do.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I guess the intelligence services and military are just such enormous bureaucracies, it�s just unlikely  perhaps that you would be in the right place at the right time, or have enough discretion in those roles to be able to make a difference. As you say, it could help you to build up your career capital so you can get other good, useful roles in the future, and you know how the system works, so you could potentially influence it from outside and talk to the right people.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, exactly. Again, it would depend on the particular person and what other options they have, but it�s certainly worth considering.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What about foreign policy? Could it be useful to go into Department of State or something like that and work on arms  control agreements? Even just current arms control agreements so you understand how they work and can think about how they might work for AI in future.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I think that could be very useful. Looking at arms control and foreign policy, both from an academic perspective and from a practitioner perspective, I think, is very valuable, and it�s something that�s not our strong suit in the AI safety and policy community right now. More could be done on that front, but I think some  of the same caveats apply as to the military and intelligence side of things, that the State Department, for example, is a very large bureaucracy.\\n',\n",
       " '\\n',\n",
       " 'Robert W: How do you feel about more general attempts to make the world better, such that when AI is developed, things are more likely to go well? For example, just improving the quality of government in general, trying to get the right kinds of people elected, or improving our ability to forecast the future across the board. I guess one worry would be that that�s just not  sufficiently leveraged on this specific problem, and if you think AI is really likely to be the key issue, then you want to work on that specifically. Maybe if you could improve our ability to forecast technological changes in the future across the board, then that could be a better option. Also, you just have a lot more options if you�re open to broader ways of improving the world.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: I don�t think there�s a clear decision for everyone, but both at the group level and at the individual level, it often makes sense to take a sum it up a portfolio  approach. There are people at FHI who also are affiliated with the Center for Effective Altruism, and involved in building the EA community like Owen Cotton-Barratt and Toby Ord. There are people who straddle that boundary and look for opportunities to benefit both sides, and there are also people who focus on one thing or the other. I don�t think there�s a clear right answer, but certainly the sorts of trade offs you mention sound reasonable.\\n',\n",
       " '\\n',\n",
       " 'On the one hand, one would want  the world to be in a better place, for there to be more people who are thinking carefully about making good decisions and benefiting the future people in various important decision making roles. It�s quite uncertain what impact you could have on AI, and likewise, even if you work directly on AI, it�s not totally clear that that�s always the best thing you should do. I have a slight bias towards thinking that direct work needs  more attention at the moment, but I think that you should take that with a grain of salt, because I would like more people to help solve the specific problems I�m working on.\\n',\n",
       " '\\n',\n",
       " 'Robert W: We should potentially continue trying to get people involved. We�ll just become AI researchers right away.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Right.\\n',\n",
       " '\\n',\n",
       " 'Robert W: That�s all of my questions. Was there anything you wanted to add? Any inspiring message?\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Yeah, I�ll just circle back on what I was saying earlier,  which is that AI safety was in a very nebulous stage of development a few years ago, and it took the work of Nick Bostrom in Superintelligence and Stuart Russell in giving a lot of talks and writing op-eds to call more attention to it and give it more legitimacy, and then subsequent work was done to refine the issue and develop research agendas by people, including the authors of he �Concrete Problems in AI Safety�  paper. Now we have a lot of postdocs and graduate students working specifically on this.\\n',\n",
       " '\\n',\n",
       " 'We have people in industry specifically working on well defined problems in this space, and we have the opportunity to make a similar transition in AI policy over the next few years. We�re [moving] from fairly high level desiderata and problem framings to specific proposals and formal models and good white papers and so forth  over the next few years. It�s an area that would benefit from a lot of people�s expertise. I would definitely encourage people who think they might have some relevant expertise to seriously consider it.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I�ll just reiterate that. Since I wrote our problem profile on positively shaping artificial development, we�ve heard from a lot of people who think that this is the most neglected area within AI safety. I�m very keen to get more people working on these problems and getting more organizations, hiring for them.  If it�s something that you�re interested in, I think you should definitely be looking into it more. We�ll put up links to guides where you can find out more and meet the right people.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: Great, thanks for having me Robert.\\n',\n",
       " '\\n',\n",
       " 'Robert W:Yeah, thanks so much and have a great day, and look forward to talking in future.\\n',\n",
       " '\\n',\n",
       " 'Miles Brundage: All right. Thanks. Bye.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Here at the Centre for Effective Altruism, we�re interested in finding ways to compare what it means to do good, and to figure out which ways of doing good do the most good. We ask questions like, �Which charity should you donate to if you want to help as many people as possible? What should careers should you follow if you want to improve the world? Which cause areas have the largest impact?� These are the sorts of questions that we think it�s really important to get clear so you know how you can make a real difference.\\n',\n",
       " '\\n',\n",
       " 'Robert W: To help us answer some of these questions, we�re joined by Professor David Spiegelhalter, the Winton Professor for the Public Understanding of Risk at Cambridge University. Professor Spiegelhalter spent much of his life trying to improve public understanding of statistics, science, and risk in ordinary life. He regularly appears in the UK media and writes on his blog, Understanding Uncertainty. It�s great to have you with us today, David.\\n',\n",
       " '\\n',\n",
       " 'David S: Hi.\\n',\n",
       " '\\n',\n",
       " 'Robert W: We�ve got so many things we�d like to talk about. We�ll see how far we get, but first, tell us a bit about who you are and your position.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, I�ve got a strange job, really. I�m in the maths department at Cambridge, and I teach statistics to undergraduates, but I�m actually funded by a hedge fund, Winton Capital Management. I�ve received my chair in order to improve the public understanding of statistics and risk. What I have done since I�ve had that job, for the last eight years, is I suppose to try to join the general community of people who are trying to improve the way numbers in particular are discussed in society.\\n',\n",
       " '\\n',\n",
       " 'Robert W: What kinds of questions do you research and what outreach do you do to help the public understand numbers and uncertainty better?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, I get asked to do a huge amount of stuff. I suppose helping in various agencies, communicating risk, about cancer risk, about the risk of screening for example, risks and benefits of screening. My background is in medical statistics, so that�s what I get to do quite a lot, but I�ve done TV programs about climate change, I�ve done radio stuff about all sorts of threats to society, from Fukushima and so on. Everything to do with trying to get a handle on what are the important threats to us individually, and how we might go to make comparisons between those.\\n',\n",
       " '\\n',\n",
       " 'I�m very interested in rather than the great global existential risks, I�m more interested in the things that affect us all individually, about how we eat, our exercise, transport, and so on.\\n',\n",
       " '\\n',\n",
       " 'Jess W: One of the things that I find really interesting in this area is that there�s a lot of evidence that people tend to be very poor at estimating lots of different probabilities and so end up overestimating the scale of some problems or risks that they face, and underestimating others. From your experience, what kinds of problems do you think that people tend to be most prone to overestimate or underestimate and get wrong in very harmful ways?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, there�s been a lot of research on this by psychologists as you know, and of course when we talk about people, I always include myself in this. I�m a subject to the winds of my emotional gut reactions as much as anybody else. I�m not making any claim about some superior knowledge and rationality compared with everybody else, but we just know that the way we respond to things, to simplify, we can think in two different ways. We can respond with our guts or we can try to engage our brain and think slowly about stuff. This is so relevant when it comes to risk.\\n',\n",
       " '\\n',\n",
       " 'Particularly this idea that what�s available to us, this availability heuristic is very strong, that what we hear in the news, we hear about Ebola, we hear about terrorism, we hear about the latest threat that might be in what we eat and the way we travel, and we get very concerned about this, whether it�s a plane crash or whatever. Because that�s what�s in the news, that�s what is available to us. That�s what�s so prominent, but of course, so many of these risks are actually very small indeed.\\n',\n",
       " '\\n',\n",
       " 'This is not a threat to us, particularly at all. The things that are much more familiar, and we don�t hear much of, for example, heart disease, cancer, all the sort of stuff that we have get largely amount of that is because of the way we live, our lack of exercise, our crummy diet and so on. Of course, people get a bit bored with that, and don�t get so concerned about it.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Do you feel you�ve had any success improving public understanding of risks and getting people to focus on the stuff that really matters?\\n',\n",
       " '\\n',\n",
       " 'David S: Oh, I don�t know. It�s very [inaudible 00:04:23]. I mean, it�d be great to think so. I�ve been involved in some good projects I�ve been very proud of. For example, the redesign of the cancer screening leaflets in the UK which present the benefits and risks of cancer screening in a very balanced way. They�re hugely innovative in that they don�t actually recommend people go for screening. They just say, �Well, these are the possible benefits, these are the possible harms, make up your own mind.� I believe that�s the right way to go about communicating risk, is not to say, �Oh, you�ve got to watch, you�ve got to watch out, this is terrible.� Say, �Well, if you do this, this might happen, or it might not happen, and weigh it up,� and actually give people credit for some intelligence which I think people basically are. Don�t think people are so stupid.\\n',\n",
       " '\\n',\n",
       " 'What I am proud of is being part of a general community that�s very strong in Britain, to do with public engagement in science, which I�m just a small part of that because it covers material on the radio, stuff on television, stuff in some newspapers, and in various agencies. For example, in Statistics Authority, which is just trying to take a much more critical attitude to the way that numbers and evidence are used in society. I think it works. In Britain, we�re rather good compared with most people about, I don�t know, we don�t have these massive fears of vaccinations and nuclear power, of even GMOs. I think this is a sign that we in this country have developed quite a good public engagement with science community.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, I was actually going to ask you about what potential you think there is for young people who want to have an impact in their career, going into a career path similar to your own and trying to get into public outreach and engagement to have a broader impact, rather than necessarily just doing research and academia. How hard do you think it is to do this kind of thing? It sounds like this is definitely something you wish more academics did work on.\\n',\n",
       " '\\n',\n",
       " 'David S: Absolutely. I mean, I really came to it quite late as a real part of my career. As I said, again, because a philanthropist in a hedge fund provided the funding to be able to do this full time, but I was doing some before as part of my job. I increasingly feel that it�s actually a duty of academics who after all, are publicly funded, for a certain proportion of them to really spend some time on public engagement. It doesn�t suit everybody, it�s not everybody at all, although I think everybody should have a website explaining what they do. They should be also supported and there should be incentives within the career structure for academics to do this.\\n',\n",
       " '\\n',\n",
       " 'I�m quite pleased, in Cambridge for example, when we�re looking at promotion of people, their public engagement is taken very seriously indeed. It is something I strongly support. It�s not everybody, but I think it is a very important for academics to do, as they�re doing their work, in whatever area, it has a relevance to society and it can potentially improve society. They should be working on it. I personally think statistics is a particularly important area for improvement of society. I feel this very strongly, many statisticians do. In terms of public engagement, the Royal Statistical Society has now got an initiative of training up statistics ambassadors, young statisticians who want to do this, who really want to get out there and communicate the importance of their work and try to improve the way things are done with numbers and evidence. I think this is so exciting, such fun.\\n',\n",
       " '\\n',\n",
       " 'Robert W: A lot of young people we meet are thinking of going into academia. Do you feel like that�s a good place to be, to have a big impact in the world? Of course, there are also some drawbacks that you might be aware of from your experience of being in academia?\\n',\n",
       " '\\n',\n",
       " 'David S: Yes, yeah. I wouldn�t necessarily say to recommend, �Yes, you really should go into academia.� It doesn�t suit everybody and frankly, [inaudible 00:08:17] lifestyle is it [inaudible 00:08:18]. It�s very tough now. In most careers, there seems to be a lot more pressures on than there used to be, so it can be quite a tough call, I think. But there is potential to do a lot of good stuff and people I work with and know in other areas, whether they�re working on natural threats, natural disasters, whether they�re working in botany, whether they�re working in any area, plant sciences, whatever, are really dedicated to trying to improve a lot of the world. I�m so impressed by the dedication.\\n',\n",
       " '\\n',\n",
       " 'But it is a tough job, and you don�t necessarily see in the push to publish the work, you�ve got to do all this, you�ve got to go through all this business in order to build one�s career. It doesn�t necessarily appear at first that you�re actually doing great job in what you do. Very little of it you can see a direct impact. However, as I know what�s very good about it, is it�s building a lot of transferable skills that you can use. I find that in my statistical skills for example, are in massive demands, by all sorts of agencies. I could point in particular to the importance of the millennial development goals and the sustainable development goals which are replacing them, in the monitoring of the state of welfare in different countries around the world has become absolutely vital.\\n',\n",
       " '\\n',\n",
       " 'Just being able to measure things and know what�s going on and to develop good ways to do surveys, to actually look at what works and how you can improve these measures has become enormously vital, enormously important. I think, well again, I�m coming back to � I think statistics is a particularly valuable area to go into.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, absolutely, and just a little bit more broadly, the key idea behind effective altruism is that we want to try and figure out which altruistic activities, which charities, which career paths, which causes do the most good. We think that being able to measure things, or at least trying to measure things is incredibly valuable in making progress on those questions. But obviously many of these questions are incredibly difficult to answer, so a common criticism that we often get is people saying something like, �Oh, it�s impossible to figure out which charities or careers do the most good so you�re wasting your time trying, and you should just go with your gut or help personal causes.� What do you think of this objection and do you think there�s anything to it? Concern about there being too much uncertainty.\\n',\n",
       " '\\n',\n",
       " 'We often respond by saying that it doesn�t mean that we shouldn�t try and estimating this clearly the best we have, but what are your thoughts on this challenge and how to respond to it?\\n',\n",
       " '\\n',\n",
       " 'David S: I mean, this is not a comment that�s just restricted to the area that you�re talking about, it happens all the time. It used to happen in healthcare. People used to say, �Oh, you can�t put a value on a human life. We�ve just go to do all we can to do good and we have to just go with our guts, essentially.� No, that view now, certainly in the UK, has been completely discredited. You can put a value on a human life. We do all the time.\\n',\n",
       " '\\n',\n",
       " 'Robert W: I�m not sure everyone would agree.\\n',\n",
       " '\\n',\n",
       " 'David S: Not sure everyone can agree, and of course �\\n',\n",
       " '\\n',\n",
       " 'Jess W: But you have to.\\n',\n",
       " '\\n',\n",
       " 'David S: � But NICE, for example, has been doing this for over a decade, putting the value on the marginal benefit of particular healthcare intervention and assessing what should be supported under National Health Service according to reach all the criteria. Now, we know this is not perfect. We know this does not measure everything, but it�s explicit, it�s transparent, and they�ve done their best, and I think it�s a massive success, and a huge example of other countries, how you can go about it. Now, we know it�s never perfect, it�s never perfect, but because people then trying to take that approach which I think has been extremely beneficial in healthcare, and move it into other areas.\\n',\n",
       " '\\n',\n",
       " 'Now, how can you do it � So people are trying to do it with the environment. �How can we measure the value of a tree?� Well, people are trying to measure the value of a tree, and I know what the value of a tree�s worth, and it�s quite a lot. But working out all these different, trying to measure the benefit of a or how [inaudible 00:12:31] sustainable environment, et cetera. What�s the value of a species? What�s the value of this? I mean, it seems it�s too easy to sit back and say, �Oh, you can�t do it. You can�t measure that.� Well, you can have a good go, you can have a good go. As you say, it�s always a balance between trying your best, realizing you�re never going to be able to do it, but not to be put off having a good go.\\n',\n",
       " '\\n',\n",
       " 'It�s like a lot of things we do in statistics, trying to measure things, trying to model things, that we always know what we do is inadequate, but that doesn�t mean we can�t do useful things.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Can you tell us a bit more about NICE and how they prioritize health treatments within the United Kingdom?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. I mean, that�s something I�ve worked on, a medical statistician. I�ve been a huge supporter of NICE, and essentially what they do is when they decide whether to have a new vaccine or whether decide to have a new recommend for drugs to be paid for under the NHS, for example, they will look at what�s the expected benefit of that intervention, and what�s the expected cost. Then, they look at how much it�s going to cost to provide an extra, or they call it �QALY,� a quality adjusted life year. One year life, it�s discounted if it�s poor quality, you will discount it, it won�t be worth a whole year.\\n',\n",
       " '\\n',\n",
       " 'Then, essentially they can make the comparison and they look, if something comes in at least than 20,000 pounds of QALY, just paid for, just, �Yep, fine, we�ll pay for it under the NHS.� If it�s more than about 30,000, then they really try to say, �Well, we don�t want to pay for this� and try to go back to the drug company to renegotiate a price, for example. Between 20,000, and 30,000, well, that�s more in the gray zone. This has been enormously beneficial, partly in order to go back to drug companies and get them to reduce their prices, and also to see that some interventions, for example, cancer screening, come in extremely cheap. They�re just worth doing.\\n',\n",
       " '\\n',\n",
       " 'It also means you have to be explicit, for example, in how much you value the future rather than the present, because they put in a discount rate, currently three and a half percent. Now, that means that a year of life in 25 years� time, for example, is only worth about half what a year of life now is worth. That�s taken into account. Now, that of course is controversial and if we were talking about big policy decisions, for example about climate change, that�s far too big a discount rate. We wouldn�t care less about the world in 100 years time if we used that discount rate, so in different areas you might be wanting to use a different discount rate. But these are taking economic ideas and moving them into very human decisions, and I think they are enormously helpful and illuminating, but they�re never perfect.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. I�ve actually written an article with Professor Toby Ord about why we shouldn�t discount health, even though we should discount financial returns, about why\\n',\n",
       " 'it�s appropriate in one case and not in the other. We�ll put up a link to that on the website. A lot of people who are planning out their careers, they feel just overwhelmed by uncertainty. They don�t know how much they�re going to earn in one career or what�s the chances of getting into academia, or being elected to parliament. How do you think people ought to make decisions when they�re just surrounded by so much uncertainty?\\n',\n",
       " '\\n',\n",
       " 'David S: Well, I mean you could say we�re always surrounded about � This again, this is common to everything we do. We never know what�s going to happen in the future, which is great. People don�t want to know what�s going to happen in the future. People don�t even know what they�re going to get for Christmas, let alone what�s going to happen for the rest of their lives. So, this is just a common problem, and we need to distinguish what we might call risk and uncertainty.\\n',\n",
       " '\\n',\n",
       " 'The risk is when it�s a very well defined problem, there�s short-term issues and buying lottery tickets. You know what the chances are, and then we can take a rational approach. When we�re dealing with deeper uncertainty, when really are not even sure what the options are, of how our lives might develop, then it�s very difficult to take a completely formal approach. I wouldn�t try to say, �Oh, we should be able to do all this mathematically� at all, however, the basics of qualitive ideas, of thinking through a rational decision are still very valuable. Because you think of, �What are the options available? What are the possibilities? What are the possible consequences of what I might do?�\\n',\n",
       " '\\n',\n",
       " 'Now, that�s always inadequate. You�re never going to be think about the thing, and however, by thinking through that, some things might come immediately apparent, some immediate rankings of what is preferable and not preferable might become apparent. Otherwise, you might need to fall back and think on broader strategies for making decisions in the face of what we call �deeper uncertainty.� Now, this is a deeply contested area. When governments make policies, very often they�re in these situations where they can�t even think of all the things that might happen, so they�re in the situation of deeper uncertainty and people have tested various ways to go about it.\\n',\n",
       " '\\n',\n",
       " 'The first thing is to not to think that you can optimize. You can�t be perfect, you cannot be perfect. The sort of suggestions people make in terms of general decision-making in these contexts are to do with flexibility and resilience. You don�t want to commit yourself to something you�re not going to be able to change because you can�t predict everything that�s going to happen. You want to build in flexibility and adaptivity to your decision, and that�s the same in any business, any government, any project at all. The other thing is building resilience in which you are essentially can cope with the unexpected and things.\\n',\n",
       " '\\n',\n",
       " 'That means both the possibility of really good things happening as well as the possibility of really bad things happening, that you are making sure that you haven�t put all your eggs in one basket, essentially. So if something goes wrong, you haven�t completely gone up a total blind alley that you can�t renegotiate, which means operating from a most robust basis as possible. It means not trying to optimize the single path, but building in robustness, which means I think in terms of career of course, building in transferable skills you�re going to be able to use in a variety of circumstances. I think these are standard tactics to respond to deeper uncertainty, which can be used in any situation.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, that�s definitely something that the organization [inaudible 00:18:46] does now, is to advise people on how to have a big impact with their careers really focus on is especially earlier in your career, keep your options open, choose things that are robust under lots of different possible things happening. One other question that we find we come up against a fair amount in thinking about careers and charities and all kinds of things is how do you go about choosing between a small chance of some really huge outcome, like saving the world from a pandemic that kills millions, against some high probability of a very moderate or a more moderate outcome, helping people in your community?\\n',\n",
       " '\\n',\n",
       " 'This seems like a very difficult thing to deal with. Do you have any thoughts on how people can go about making these kinds of decisions?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, very difficult but again, these are common decisions always in our lives. Do we go for the high risk option or the safer option? I personally think that actually, a mixed strategy is something that businesses I think would recommend. Although, that�s not what I do. I�m not involved in any investment decisions, but often I think people take an idea of portfolio of risk taking, because these are all risks. You can�t guarantee with anything that you do. These are all risks, and so you might have a certain amount invested in rather safe things with a safe return, you�re pretty sure that what you�re going to do is going to have a reasonable return, and that you really can predict the consequences of what you�re using your career or your money for, what you�re getting for.\\n',\n",
       " '\\n',\n",
       " 'On the other hand, on more speculative, where you might be supporting something that actually might not come to anything. Either because the solution it�s promising won�t work or the threat won�t arise in the first place. I think actually a mixed strategy in these circumstances, again, I wouldn�t want to just put all the eggs in one basket. I think it wouldn�t be appropriate. I also personally feel that � This is completely my personal reaction that I give, is that I quite like a mixed strategy in which some things are the big [crosstalk 00:20:54]. You might slowly anonymous, where you�re giving money to large organizations that�s doing something which is contributing in a probably fairly predictable way to improving things with people, versus the more personal ones where you do have the personal contact.\\n',\n",
       " '\\n',\n",
       " 'You might get slightly more emotional feedback from them, partly because they might encourage you to perhaps take on a more � Feedback from the altruism might encourage you to take on a bigger [inaudible 00:21:24], a bigger commitment in the future.\\n',\n",
       " '\\n',\n",
       " 'Robert W: There are various ways of thinking about how to quantify the risks or possible gains associated with an activity. Can you explain what a micromort or a microlife is?\\n',\n",
       " '\\n',\n",
       " 'David S: Oh yeah, yeah. These are units that � Well, we invented the microlife. Someone else invented the micromort. It�s just a way of trying to get a common scale, particularly because a micromort is acute risks, things that might kill you on the spot, but otherwise you�re healthy. It�s a one in a million chance of dying because of an action you might take. Skydiving or something is approximately 7-10 micromorts. Microlife, which is something to do with chronic risks, which is the risk of � I think [inaudible 00:22:07] quite interesting, which is bad lifestyle. Lack of exercise, or smoking, or drinking, or bad diet, whatever, which isn�t going to kill you on the spot but is likely to shorten your life. Not definitely at all. Maybe not, but probably will.\\n',\n",
       " '\\n',\n",
       " 'It�s a reduction of half an hour in your life expectancy, and I find these quite useful, because it enables you to compare all these different things like diet and smoking and exercise and things, and makes me for example, obviously smoking is where you�re the biggest return or the biggest in terms of not smoking, the biggest return. Drinking at low doses, well actually, it doesn�t make much difference. [inaudible 00:22:53] it�s a rather huge concern, unless you�re really swigging it down. But exercise, again, I think is extremely important, and diet�s important. It just gives you a feeling of where the priorities are, which of course, it�s all a bit obvious. They�re the priorities that people do in public health, but it makes slightly unconcerned about other things that people might be really concerned about.\\n',\n",
       " '\\n',\n",
       " 'Robert W: These concepts were invented to help people get a better understanding and make better decisions. Do you think that we could invent new concepts to help people think about how they can do more good in the world?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. A microaltru or something. It�s difficult. I mean, a QALY is a good one. If your area of cause is to do with saving lives, improving health, lethal risk to people, then QALY�s a good one. It�s already established, you can look at how many years of life are you going to gain from your intervention, and we know that if you�re giving money to some particular charities, which I know that you promote, they�re extremely cost effective in that way. It�s more difficult if you�re the broader idea, for example, of supporting education, preserving the environment, the actual environment, and so on. Quite hard to put a number on that, within a particular area of cause, then you might be able to do it, but having metrics that go right across different areas is difficult.\\n',\n",
       " '\\n',\n",
       " 'I know that people criticize the attempts to do it. I still think it�s worth trying to do, and people struggling to do this within the environment, and so on. I don�t think I�ve got any great idea to be honest. I mean, one possibility of course is looking at general measures of wellbeing, which again, people are doing. It�s measured by national governments now to try to measure wellbeing, in terms of how people feel about their lives. General wellbeing indices which are not just health related, or length of life related, I think have got great potential. You�re not going to be able to get the perfect measure on any of them.\\n',\n",
       " '\\n',\n",
       " 'Robert W: People often ask, �How would you rate how good your life is going from 1 to 10?� Typically they give answers between six and eight, at least in the UK.\\n',\n",
       " '\\n',\n",
       " 'David S: [crosstalk 00:25:27].\\n',\n",
       " '\\n',\n",
       " 'Robert W: Maybe we could have a micro happy, which would be something as good as moving someone from six to seven, or seven to eight on this welfare scale.\\n',\n",
       " '\\n',\n",
       " 'David S: Yes, exactly. I think those scales, well actually, I find them very revealing when people do mention them, particularly across cultures, as you said. Everyone says seven, [inaudible 00:25:49]. Maybe because they like the number seven, but it also reflects this idea, �Well, it�s not bad. Mustn�t grumble. Could be better, but mustn�t grumble� so slightly British, stoic attitude. I think it�s almost predictable what people are going to respond, but I find, for example, the age distribution of these responses and how the age distribution varies across cultures extremely interesting.\\n',\n",
       " '\\n',\n",
       " 'The idea of whether one could move groups of people on that scale would be very important. When I think of charities, and some of the ones I give to, which are not directly I don�t think any of them actually make people live longer, but would hope to improve people�s wellbeing in the sense of happiness considerably.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah, it is a fascinating area of research, and there are significant international differences. I think in the UK and America and Australia, people do tend to give seven or maybe eight if they�re feeling particularly good, but in South America, people are just for some reason, more exuberant and tend to often give eights, even when they�re relatively poor. By contrast, in Eastern Europe and Eastern Asia, like Japan and China, people tend to give relatively low scores relative to how their life seems to be going from the outside.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. No, no, people in different cultures, I think in Africa they�re even lower. People are very �\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. There are some countries where the average is as low as 4 out of 10, which maybe half of people are giving less than four which is a bit unfortunate.\\n',\n",
       " '\\n',\n",
       " 'David S: Again, I mean, they�re very culturally, I think, specific. But I think what I see is relative changes perhaps compared with the average in that community that one would [inaudible 00:27:41] for. You can�t just get everyone up to eight or whatever. I mean, there are very different cultural attitudes to do with, I think, that are reflected in those questions.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Yeah. By trying to estimate risks or the size of problems in this quantitative way, do you think we risk being biased against approaches that are harder to quantify?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, it always is a problem that people start believing the model, the measure, which is only a very inadequate thing. Starts becoming the thing that you are in fact then trying to, you�re focused on to the detriment of all others. There�s a real problem with over metricizing any activity, pretending that you can measure the benefits of everything. The academic world, of course, is a real example of that where people start getting obsessed with impact factors and nonsense like that. There�s a real danger with believing these things too much\\n',\n",
       " '\\n',\n",
       " 'Also in time, I think we should have a damn good go at trying to do it. Just because something is difficult and cannot be done perfectly doesn�t mean that you can�t at least have a stab at it, because you may be � Often these things can get really clear around things. Great precision might not be required in order to determine that actually this is not an effective use of the sources compared with [inaudible 00:29:07].\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah and I think that rather giving us a perfect answer to all questions using models and looking at different factors, can as you say just help you to identify clear differences or areas where you need to get more information because you just have no idea to [fight 00:29:23] all that in or something, so it can definitely be very useful. Finally, just to what extent that the average person can benefit from a better understanding of statistics, learning more statistics? Either in their everyday lives, or in being more effective as altruists. To what extent do you think, I guess, relating back to the past question, it could potentially limit us?\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah, I think of course I�d say this, wouldn�t I? But yeah, I do believe that people would benefit from having a greater idea of stats and measurement, and number, and quantity, and the frailties of that as well. Because the point about understanding statistics is both understanding their strength and their weaknesses. The two are absolutely hand in hand. What people tend to do at the moment, they�re I suppose not confident with numbers and how they are constructed, because they are in their whole, are always constructed. Someone has chosen what to measure, and they tend to either accept them as if they�re God given truth and that�s the number and that�s so vital, and that is it, that�s what we�re looking for, or reject them out of hand. �Oh, you can�t put numbers� � Just damn lies and statistics and, �This is nonsense.�\\n',\n",
       " '\\n',\n",
       " 'Those two extreme views are both equally idiotic. But if you�ve got a slightly more nuanced view, you�ll realize that statistics, numbers, and measurement are an incredibly valuable tool, but they�re just a tool, and they have their frailties and inadequacies. They both need to be able to teach and understand the strength and the limitations. What I�m involved in a number of educational projects in which we�re trying to do just that.\\n',\n",
       " 'Robert W: It�s really a matter of trying to assign the appropriate weight to each different kind of evidence or each different piece of evidence that comes your way?\\n',\n",
       " '\\n',\n",
       " 'David S: I mean, in the end, what statistics is is to do with is quantitative evidence. That�s what we want, what we�re talking about, and evidence can be good, bad, or along a whole scale. It�s not a true or false. It�s just evidence, and we�re trying to weigh up our evidence, which we do all the time in our lives. We do this. Our guts tend to be a bit fallible when it comes to weighing up evidence. We tend to be too influenced by certain salient things that attract our attention. As you�re standing back and trying to be a bit cooler about weighing up evidence, I think it�s a really valuable thing to try to do in all areas of life, it doesn�t matter what.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah, absolutely, which is not to say that you should completely discount that gut feeling, but often just supplement it with some extra evidence source statistics.\\n',\n",
       " '\\n',\n",
       " 'David S: Yeah. I don�t trust them a bit, but I still go with them.\\n',\n",
       " '\\n',\n",
       " 'Jess W: Yeah. I think that�s about all we�ve got time for now, but thanks so much, David. This has been very informative and really interesting.\\n',\n",
       " '\\n',\n",
       " 'David S: Okay, thanks very much.\\n',\n",
       " '\\n',\n",
       " 'Robert W: Right. If you�d like to hear more from Professor Spiegelhalter, then you can find his blog Understanding Uncertainty online, and he�s often on the radio in the UK. In particular, on �More Or Less� on BBC Four.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Hi! I�m Robert Wiblin, Director of Research at 80,000 Hours and welcome to the podcast. If you want to make sure you never miss an episode from us, you could subscribe by searching for 80,000 hours in whatever app you use to get podcasts. That way you can also speed up the episode, which is how I much prefer to listen to interviews. Next week, I�m scheduled to speak with Alex Gordon-Brown about working in quantitative trading in order to and to give, which I expect to be very engaging.\\n',\n",
       " '\\n',\n",
       " 'Today�s conversation really goes into the weeds and I learned a great deal from it. If you�re looking for personal advice on how to pursue a career in technical AI research, stick around because we get to that in the second half. I apologize for the audio quality on my end. I think we�ll have that fixed up by next time. If you�d like to offer any feedback on the podcast, please do email me at rob at 80000hours dot org. We�re still figuring out how we can best use podcasts to help our readers and I�ll try to respond to everyone. Without further ado, here�s my conversation with Dario Amodei.\\n',\n",
       " '\\n',\n",
       " 'Today I�m speaking with Dario Amodei, a research scientist at OpenAI in San Francisco. Prior to working at OpenAI, Dario worked at Google and Baidu and helped to lead the project that developed Deep Speech 2.0 which was named one of 10 breakthrough technologies of 2016 by MIT Technology Review. Dario holds a PhD in Physics from Princeton University where he was awarded the Hertz Foundation Doctoral Thesis prize. Dario is also the co-lead author of the paper, �Concrete Problems in AI Safety�, which lays out in simple terms the problems we face in making AI systems safe today. Thanks for coming on the show, Dario.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Hi.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We plan to talk about the motivations behind technical AI safety research, the Concrete Problems paper, and how someone can pursue a career in it for themselves. First, we�re at the OpenAI office here in SF, tell us a bit about OpenAI and how you ended up actually working here.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: OpenAI is a non-profit AI research lab. It was originally founded by Elon Musk, Sam Altman and a few other folks. Generally, we�re working on following the gradient to a more general artificial intelligence and making it safe. I joined around, what was it? July of last year, so about a year ago which was a few months after it started. I came here because there were a number of � I thought there were a number of really talented researchers here and it was a good environment in which to think about safety in the context of AI research that�s already being done.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI was only founded about 18 months ago? Is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It was about 18 months ago, yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How many staff does it have now?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think there�s, last I counted, about 55 people here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Has it been difficult hiring that many people that quickly?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�ve actually never worked at a startup before. Our CTO, Greg Brockman, was previously CTO of a startup called Stripe, which now has around a thousand people or so. It�s definitely hard. He�s really good. It is not something that I�ve been super involved in except on the safety side.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess it�s the Bay Area way to explore some growth in organizations. Who�s backing it? What�s the budget like?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t know if I can give exact numbers in the budget. The main donors at this point are Elon Musk, Sam Altman and Dustin Moskovitz through Good Ventures .\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s what you do pretty similar to what�s going on, at DeepMind? Are there important differences?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I would say that the general research agenda at OpenAI and it�s focused on reinforcement learning and in learning across many environments and trying to push forward the boundaries of what�s done instead of just focusing on supervised machine learning. I would say that�s very similar to DeepMind and probably it�s one thing that sets OpenAI and DeepMind apart from other institutions. We both have a similar focus on safety. We both have safety teams. I would say OpenAI is trying to be a smaller organization that focuses on hiring just the people that we want the most. That�s been one of the big difference. There�s probably some differences in culture as well that are a little bit intangible and hard to describe. I think generally our view of AI works and what to build in AI and the focus on safety are actually pretty similar between the two organizations.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You studied Physics, right? Had a PhD in Physics? Then you switched into AI, or was your Physics-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My Physics work was, in particular, specialized in biophysics. I was thinking about models coming from statistical physics and applying them to models in the brain. Then also using the techniques from Physics and electronics to make measurements to try and validate those models. I come from a Physics background but I�ve been thinking about intelligence for quite a while and how intelligence worked. I think, when I get my PhD, I wanted to understand that by understanding the brain. By the time I was done with it and by the time I did a short postdoc, AI was starting to get to the point where it was really working in a way that it hadn�t worked when I started my PhD.\\n',\n",
       " '\\n',\n",
       " 'I felt like maybe it was starting communication the best way to understand intelligence would be to actually directly work on building parts of it rather than studying the messiness of the brain. That was what led to that switch.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you want to give a quick pitch for why, what kind of artificial intelligence is so important?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think you can give a standard argument that though a lot of people are familiar with which is that if you think about any technology that humans have created, what�s allowed us to create that technology since the sanitation, flight, medicine, improvements in human health, improvements in the ability to feed the world. All this has been generated by our intelligence. Our intelligence is relatively fixed.\\n',\n",
       " '\\n',\n",
       " 'If we�re able to build something that was able to match or exceed our intelligence, then that would really be increasing the engine, produces a lot of the great things that we do and ultimately maybe I�m, maybe it would take a long time, would give us a much more complete control over our own biology and neuroscience could make us whoever and whatever we want to be, could end conflict, war or diseases, that stuff. That sounds a little utopian but I think if we push this technology far enough and all goes well, then that will lead to a result either immediately when we build it or over a somewhat longer period of time. I don�t see any reason why those things can�t happen. I think that�s the basic reason to work on AI.\\n',\n",
       " '\\n',\n",
       " 'As I�ve written, there are these safety issues where we can imagine situations in which it doesn�t actually go well to the extent that that�s a risk, that�s also risk that we can reduce. We can also have leverage by focusing particularly on reducing that risk. On both the positive side and the negative side, it seems like a-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Quite a lot of leverage.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, there�s a huge amount of leverage to be had. The previous stuff I was doing was in biology. It�s great, you can help people, you can try and try and cure some disease but this feels like it�s more getting to the root of problems.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What does the name �OpenAI� mean? Does that relate to the parts the organization is taking?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I wasn�t actually present at the time that OpenAI was founded or the name was chosen. I wasn�t the one who picked the name. I think there�s been fair amount of misunderstanding. I think there�s one group of people who think it�s all about open source, and releasing open tools. There�s another set of people who, I don�t think many people think this anymore but who for a while thought that it was about making an AGI without any safety precautions and just giving a copy to everyone and that this would somehow solve safety problems. These were too early misconceptions that were around long before I joined OpenAI. My understanding is that it�s meant to indicate the idea that OpenAI wants the benefits of AI technology to be widely distributed. Assuming-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Rather than only going to the owners.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Assuming that safety and control problems are solved and we build AGI, there�s then a question about who owns it, what happens with it, what world do we live in after it�s created. Again, this is � I wasn�t the one who named this or set the specific mission statement but I think Elon�s intention with it was trying to think ahead given that we built an AGI and it�s not wildly unsafe how were its benefits distributed throughout humanity. I think openness is intended to indicate the idea that these benefits should accrue to everyone. That�s my understanding.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI is a non-profit.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It is a non-profit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If you developed a really profitable AI, how does that work? OpenAI become incredibly rich and then it gives out the money to everyone?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Personally, I�ve no interest in getting rich from AGI. I think it would do so many interesting and wonderful things to humanity that I think-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The question of getting a larger share is silly.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: The meaning of money would change quite a lot and even maybe the psychological motivations that would want me to get a larger share or things I could change and my want to change. In many ways, shares in terms of money are maybe not the right way to think about it but I think there�s all kinds of stuff that could happen when AGI happens. Some of the things I think about are where that could go or what that could mean. The summary is we don�t know very much because it�s something we haven�t done yet. A lot of it�s speculation.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What do you research here at OpenAI?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I mainly work on safety. We have a safety team that�s so far, myself and Paul Christiano. Paul was co-author of mine on the Concrete Problems paper and has also written a lot online on his blog about AI. He�s probably one of the people who�s I think done the most to promote clear thinking about the problem and tie it to current AI. We have a third person joining in a few weeks who I�m super excited about. We�re trying to build up a team that focuses on technical safety. We also do a little bit of strategy stuff which is how do we get different organizations that are working on AI to cooperate with each other, how do we cooperate with policy makers on questions like these. We�re also thinking a little bit about those issues but mainly technical safety.\\n',\n",
       " '\\n',\n",
       " 'I also do some stuff that�s not strictly technical safety but is generally done to stay up to date on where AI is currently going. I did some work on a transfer learning a while ago that was really a little bit safety motivated but trying to make environments that are broad enough that it�s possible to see distributional shift to our distribution problems. That�s the range of stuff I work on.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What was the organizational culture like here? What kind of people does OpenAI attract?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think we�ve generally been very selective in who we pick. Generally it�s people who are very talented machine-learning researchers but also people who, I would say not everyone, but a large faction of people here really do think in terms of eventually getting to AGI. At least some people, a significant fraction are quite interested in or at least supportive of safety work related to that or related to what we do. Now there�s a wide range of beliefs on how to work on safety, how possible it is to work on safety from our current vantage point. There�s a wide distribution of use but broadly people are pretty supportive.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: OpenAI recently moved away from software development, is that right? To more focus on machine learning?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: That�s not quite right, it�s more � I think what you�re referring to is we had a project called Universe, which actually I was somewhat involved in the machine learning side. The idea of that project was to make a lot of environments that agents could learn using. The way we did was using something called the VNC protocol to connect directly to a browser through pixels and so that would allow you to play thousands of flash games and navigate weba tasks. It turned out to be a case. I was actually really excited about this because I saw this as a test bed to study safety. If you have hundred flash racing games, you can train the agent on one flash racing game and then see how it behaves badly when you transfer it to another flash racing game. You can study some of these open world problems where an agent has a very wide space to explore and a wide range of actions it could take.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: This is one thing that, and our researchers have been working on is teaching computers to play computer games really well like a superhuman level.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this is � DeepMind has worked on this with ATARI games. We were taking it to another level with any game you can find in the internet. This ended up being a project that I think could probably be described as a little bit ahead of its time. It turned out that in order to connect this way, we needed all the different workers who are applying our algorithm to be asynchronous with one another and for reasons that were complicated when we figured out � we only figured out later. Actually such asynchronous communication was really hard to make it play well with ML. it led to a lot of complexity.\\n',\n",
       " '\\n',\n",
       " 'We�re, to some extent, de-emphasizing that project now. We�re actually trying to move to doing the same thing with a more synchronous environment. Basically, the same idea but more in a way that more amenable to ML benchmarking and to measuring how well we�re doing and doesn�t have this kind of hard to interact with property. It�s more like we made a tool and it was a good first attempt that add something ambitious but it wasn�t quite the right tool so now we�re working on changing it to a version that�s better, I wouldn�t say we�ve gone away from software engineering so much as we�ve been experimenting with how to produce tools and takes a few iterations to get that right.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Turning now to the broader issue of superhuman AI development, what do you see as the potential dangers here? Why should anyone be worried about this?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My attitude, to start off with, has always been, although I do think about AGI, which is a term I prefer to use than super intelligence because I think no one knows whether a machine will rocket past human level or not so that�s something that could happen or not. AGI is something that I think definitely will eventually happen. I prefer to talk in terms of that. Even within safety, in Concrete Problems, I explicitly try to think in terms of not how powerful the systems are but conceptually what can go wrong with them.\\n',\n",
       " '\\n',\n",
       " 'The same kind of thing could go wrong with an AGI as could go wrong with a very simple agent playing a video game or robot cleaning your house. If it has the wrong objective function, if you don�t specify its goal correctly, it can do something unpredictable and therefore dangerous. In general, when I talk about safety, I talk about safety generically whether it�s in powerful systems or very weak systems. All that said, with respect to powerful systems in particular, I think there is a possibility that if we either do a bad job specifying the goals of complex systems or just they�re unreliable in the way that self driving cars are unreliable.\\n',\n",
       " '\\n',\n",
       " 'A self-driving car has to have a very high standard of safety in order to trust it to drive on the road. For almost a decade now, we�ve had self-driving cars that are 99.9% safe. But that�s not enough, we need them 99.999% safe. With AGI which is something that � it�s going to take a lot more novel strategies than self-driving cars, the space in which it operates is a lot broader than self-driving cars. If you just transpose that kind of safety testing from self-driving cars to general intelligence, even with all the controls you put on and even with all the safety standards, it�s clear that at the very least we�re going to have a big challenge in making sure that something doesn�t go wrong. If something does go wrong, it would be easy for or it might be easy for a large amount of harm to be done relatively quickly.\\n',\n",
       " '\\n',\n",
       " 'You have your AGI controlling the stock market or the economy or something and it just doesn�t know how to do it very well yet and something goes wrong. It takes a long time to unwind that. There�s a long tail of things of varying degrees of badness that could happen. I think at the extreme end is the Nick Bostrom  style of fear that an AGI could destroy humanity. I can�t see any reason and principle why that couldn�t happen.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If it was sufficiently powerful or sufficiently good at accomplishing its goals.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If it was sufficiently powerful and like safety had been handled sufficiently badly, that is definitely something that can happen. I think there are folks at places like MIRI who say that this is the default outcome or this is likely to happen or there�s almost no way to avoid it, or you have to solve some incredibly hard math problem to avoid it. I don�t generally agree with any of those things but I think this is a possible outcome and at the very least as a tail risk we should take it seriously. I think another thing I�m worried about is that the wrong, even if we manage to make a super human AI safe or an AGI safe, then it might be used for the wrong ends.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Deliberately.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Deliberately used for the wrong ends by a disturbed individual or an organization whose views are not aligned with humanity or nation state whose views are not aligned with humanity. That�s in my mind, the range of risks.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you think there�s much of a chance that the risks are being overblown here and in fact it�s just going to end up delaying something that could be incredibly useful and make life a lot better?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It may very well turn out. Maybe it�s more than 50% chance that as we get close and closer to AGI then it becomes clear how to make something safe. Maybe it�s just like the goals are specified in a way that�s a very corded off from the task that are done that there are certain problems of nature like scanning brains or something that we need AIs to do for us in order to gain control over our biology or control over resources. Then there are human values and maybe there can be an efficient division of labor where there isn�t much confusion or maybe safety problems are just a bunch of research and � they�re just a corner of machine-learning research where we haven�t done much yet and so we haven�t tried. I can think of lots of ways and maybe it�s even the most probable way where things turn out totally fine. But I wouldn�t-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You don�t want to count on it.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I wouldn�t in any of those cases  say the risk was overblown. It�s like, supposed you have a fire alarm and someone�s cooking a barbecue and it�s smoke, you wouldn�t call like installing the fire alarm overblown. It�s just sometimes you�ll have a fire and sometimes you won�t.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You want to take precautions.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Installing the fire alarm is the right course of action. I think of this as a precaution. I don�t really think of it, I don�t think of anything I do as slowing down the rate of AI progress or at least I�m not trying to do that. I think of it as broadening the scope of AI progress and thinking about AI in a more interaction and human-centered way. If anything maybe it accelerates progress a little bit although that�s probably a minor effect. If people are worried about progress being slowed down, I don�t believe anything I do is close to that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Closing that, yeah. How much of OpenAI�s work is focused on these kinds of problems? It sounded like 5% of the staff but I guess other people are worried about too?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think, broadly, most of people at OpenAI are worried about or at least think these issues are worth thinking about. That�s different from who is actively doing their technical work on it. I would say it�s three or four people now and hoping that that grows somewhat. We�re actively looking for really talented people. I think OpenAI as an institution has the general idea that in order to work on AI safety, you have to be at the forefront of AI. Also if you are at the forefront of AI, you have a better ability to implement AI�s safety in the final system that�s built.\\n',\n",
       " '\\n',\n",
       " 'Many people are interested in safety in the long run but I think until recently and even so now, I think many people here don�t know if there�s a way to work on safety right now. They�re skeptical that you�re able to work on safety right now with concrete work. I�ve been trying to change that with Concrete Problems and with this recent paper that Paul and I wrote on learning complex human preferences. We�re trying to show that there�s concrete work that can be done and that�s had a variety of reactions. Some people are like, yes, this is exactly what you meant with safety work now I see how it can be done. Some people are like, well that�s machine learning work, I don�t actually see how it connects to AGI and so then we�ll try and write another paper and say, �Okay, this is the line we�re drawing and this is how we think it gets us there.� It actually could turn out that this is mostly just ML work and the final systems we build are different enough that for whatever reason this ends up not being relevant to safety.\\n',\n",
       " '\\n',\n",
       " 'Again, I�m pretty happy in that world. If there was nothing concrete it was possible to work on safety and I instead ended up doing a different direction in machine learning, then that ends up being fine. Then it will turn out we couldn�t have worked on safety until later and then we�ll work on safety later. Whereas in the world where it does matter, it�s really great and really impactful to get a headstart on it.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�m curious to get your view on this debate that I�ve seen online. You have this contrast, some people like perhaps Bostron could be a case of this in the book SuperIntelligence of talking as though once we have a super human AI then it will get very much smarter very quickly and it could potentially just solve all of these problems, it could solve aging, like solve all of our health issues. Since the people criticizing this online saying you just think this because you�re a bunch of nerds and you think that thinking is the way to change the world, the way that everything gets done, but it�s not going to be so simple even if you had a very intelligent machine it wouldn�t necessarily be able to solve those problems. Do you have a view on that debate?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�d rephrase the debate a little bit. I think there�s an interesting technical question of like, let�s say I built an artificial general intelligence tomorrow and because it�s software, let�s say I made a hundred thousand of them. How much does that fundamentally change our society and our technological capability? A lot of it is just, you can look at individuals throughout history that manage to discover a lot more than other individuals. You look at-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Von Neumann.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Von Neumann or Einstein or one of these figures who just manage to be leaps and bounds ahead of others. The question is like what�s the ceiling on that. If we invented AGI tomorrow, would it take a couple of days to scan all of our brains into software, upgrade us, give us indefinite life extension, or would it just be like, �Oh, it�s more humans to talk to�. I think it�s actually complicated. I think some people act like it�s obvious one way or another but it�s not really something, I have a lot of certainty on in part because I think modern science has experienced a lot of diminishing labor like-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Diminishing returns.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Diminishing return like depletion of low hanging fruit. It could turn out that like solving biology is just this exponentially complicated combinatorial problem or it�s limited by data and experiment. Of course, maybe the machines will allow us to do the experiments much, much faster. Then there�s some limit on the physical reaction time of the biological systems. When you put it all together, do we get zoom to do something much much faster than we ever could or do we get just some mild acceleration of what humans can be doing? I feel like many people act as if the answer�s obvious but as someone with a background in Biology, even thinking about all the directions in which machines can optimize it, my guess is machines could probably make things happen pretty fast but I think there�s huge uncertainty here and I don�t really think anyone knows what they�re talking about on this question.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: My background is in economics. I imagine if you had an incredibly smart AI and it was trying to figure out macroeconomics, like understand recessions and booms and bust cycle, I suppose it could have lots of conceptual breakthroughs but you take the measurements so quickly and you can�t really run experiments so you could end up being the, processing of the data that we get is extremely good and very fast but then the data only comes in so quickly and there�s only so much you can actually learn.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: There are some simple stuff, which is like, I wouldn�t be surprised if for example a really powerful AI wouldn�t be able to understand our macroeconomic systems because this data issue but it would be able to design a better macroeconomic system. It�s weird. There are some stuff I feel like you just redesign it and you and do it much better. There�s other stuff that it�s just really difficult. I find this puzzling. I�m pretty agnostic on it. I don�t really have a good answer on the kind of like nerds think AI can solve everything question. I think there are some deep set problems in human nature and so just solving resource constraints isn�t going to solve war or probably in some ways already solved resource constraints. But maybe having true AGI will allow us to redefine what it means to be human and we�ll ultimately-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Elevate ourselves above conflict.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Will elevate ourselves above our petty human bickering or maybe the petty human bickering will prevent us from being able to elevate ourselves so we�ll be stuck. I don�t know about that either.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ll reach superhuman levels of petty bickering perhaps.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t actually know. It�s actually very hard to know.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ve mentioned a few times this paper, �Concrete Problems in AI Safety�, let�s dive into that. Before we discuss those problems, what was your impetus for writing it?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I had been aware of the work of the AI safety community for awhile but had in general felt that � I wasn�t particularly happy with the way they were phrasing things. It didn�t seem like what they were describing was actionable and there wasn�t a lot of ties to like � AGI was generally discussed in these very abstract terms like having the utility function and having incentives to do this or that, discussing things at these very abstract level, I couldn�t help but feel there were a lot of implicit assumptions that were not really being discussed.\\n',\n",
       " '\\n',\n",
       " 'At the same time, the mainstream machine learning community, which I�ve been a part of for about a year and half, having a lot of experience with speech recognition systems, one thing that I found about neural nets is that they�re very powerful but they�re very unevenly powerful. The key example I gave early on was you can train a speech recognition system on 10,000 hours of American accent of data. For someone with an American accent that gets it perfectly, then you give it someone with a British accent or an Indian accent or something, and it just does terribly on it. Of course, if you train it on enough diversity of accents then start generalizing better. Generally, when we build engineering systems, that silent, random failure, it�s not something that we see as a desirable property in systems we build particularly safety-critical systems.\\n',\n",
       " '\\n',\n",
       " 'The idea that fixing those problems was not just a one-by-one thing where we�re like, �Op! We�re using neural net again in this self driving car, what statistical test for everything we can get.� We�re using a neural net now in a drone, let�s make sure it doesn�t shoot someone. That we could have principles behind what gives us guarantees on the behavior of a system or at least what gives us statistical guarantees. That seems super interesting to me and it really didn�t seem like a � it seemed like very few people were actually working on it.\\n',\n",
       " '\\n',\n",
       " 'Me and some of my colleagues, Chris Olah at Google, Paul Christiano, who�s now at OpenAI, Jacob Steinhardt at Stanford, John Schulman here, and Dan Man� who is another Googler had all thought a little bit about this problem. We decided to get together and write down all of our ideas in a paper that would lay out an agenda for what, why we think this is a thing. In particular, I think � I felt that the machine learning community as a whole was a little bit confused. I think that they largely thought AI safety was about fears that AIs would malevolently rise up and attack their creators. Even when they didn�t think it was about that, they worry that the people who talk about AI safety will feed into fears that it�s about that.\\n',\n",
       " '\\n',\n",
       " 'I felt like this was a silly state of affairs and that of course we can do research on making systems safer and more reliable that doesn�t prey on these fears. In particular, we can even do research that ultimately points towards AGI. I think the important thing is that we shouldn�t go around with every other word we say being AGI in particular like the research itself shouldn�t be specific to AGI. You can�t really research AGI now because we can�t build an AGI. I think the very standard technique when doing research on a topic is if you want to think about a topic that�s abstract or in the future then come up with a short term bridge to it that lets you think about something conceptually similar in a way you can empirically test now. That was the general philosophy behind the paper and the philosophy behind the follow-ups that we and others have done to implement the research agenda described in the paper.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What are some of the concrete problems? Do you want to tackle one or two here?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah sure. I can go into them briefly. I think we made a distinction between problems that relate to, what happens if you don�t have the right objective function and what happens if you do have the right objective function but something goes wrong in the process of learning or training the system. Not having the wrong objective function, the extreme version of that is what�s talked about and the classical AGI safety stuff which is you want to specify a goal and you, for whatever reason, you know you have some simple instantiation of the goal and it ends up not quite being the right thing. We call that-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The genie problem?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. We call that �reward hacking�. Few months ago, using an environment in the now de-emphasized Universe program. I had an example of like a boat race where the boat is supposed to go around and a few laps and what it�s trying to do is finish the race as fast as possible. The only way to be able to get points and you can�t change this because it�s the way the game is programmed is you get points as you pass targets along the way. But it turns out there is this little lagoon with all these targets and the targets also give you turbo so they make you go faster and faster. You can just loop around in this little tiny lagoon and not finish the race. In one sense, you shouldn�t be surprised it�s the correct solution, it�s how you get the most points. The idea is that the mapping from, well, this is the reward function to this is the behavior that leads to it is a very twisted mapping and so the point is that it�s-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s not what you would have intended it for it to be maximize it.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s not what you would have intended it to be. The lesson of that is that it�s very easy to make small changes in the reward space and have that lead to big differences in the behavior space and also for the mapping to be very opaque for you to look at a reward space think you know what it means and in actuality it leads to something very different than what you would have expected. We call that generalized reward hacking. Then there was another problem called negative side effects, which is a little related to that, which is just that if your reward function relates to a few things in your environment and your environment is very big, then there�s a lot of ways for you to do destructive things. It�s one particular way in which it�s easy to specify the wrong reward function.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Because you haven�t put in side constraints?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. You haven�t explicitly put in the 10,000 other things.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: All the things you care about.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: 10,000 other things you care about. Then there was this thing called scalable supervision, which is, if you�re a human trying to specify a goal to a machine learning system even if you have a clear idea of what it is that needs to be done, then you don�t have enough time to control or give feedback on every action that an AI system does, and therefore limits to your ability to control and supervise can lead to a system behaving in a way you hadn�t intended because it interpolate in the wrong way. Those are the problems with like the classical AI safety type problems like you have the wrong � you somehow, you gave yourself the wrong goal in a way that was hard to understand. Then the more technical right problems that relate to, you know, your system was trying to do the right but something went wrong, it�s things more like this thing we called distributional shift which is when your training set is different from your testing sets.\\n',\n",
       " '\\n',\n",
       " 'The classical example of this is � when I was at Google there was an incident where Google�s photo captioning system had been � they had this photo captioning system that was trained on a lot of photos. It turned out that most of the photos were statistically biased to be photos of Caucasian people. There were also a lot of animals and monkeys in it. Unfortunately, this system reacted when a black person took a picture of themselves and the photo it tagged them as a gorilla because it has only seen humans with white skin so this was, of course, incredibly offensive and Google had to apologize for it. They even had � they even thought of this a little bit ahead of time but the neural net ended up being so screwed up that it didn�t even warn them that it was in a region of the state space it was dangerous.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Because the algorithm has no concept of what�s offensive and what was not.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, the algorithm, it�s just-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: But it can produce a pretty horrifying outcome.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Exactly. It�s just a statistical learning system. It doesn�t know about racism, it doesn�t know about racial slurs. It doesn�t know about what�s offensive. It�s just a learning algorithm and it just learns from the data it was given. There turned out to be some problems with the data that it was given and there turned out to be some problems with the algorithm. It just innocently produced this extremely offensive result. This is � the world of neural net is full of this. I think something related to this distributional shift is adversarial examples, which my colleague, Ian Goodfellow, works on a lot, which is when you intentionally adversarially try to disrupt an input to a machine learning system and make a very small change to it that causes something bad to happen.\\n',\n",
       " '\\n',\n",
       " 'They were a little complimentary. Adversarial examples is like a small but carefully chosen, like perturbation to it, whereas [inaudible 00:39:34] of distribution is this holistic perturbation to it. Resistance against those two is � it�s separate. You�re talking about two orthogonal directions in the perturbation space. These are all issues with making sure that when you train something that it behaves in a new environment the way you would intend it to behave or if it goes wrong but it fails gracefully. We haven�t put a lot � when we put some work into this area, we cite a lot of papers in the Concrete Problems paper. I think relative to the stampede of work in mainline AI, I�d like to see more of this stuff.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I think you did an interview with the Future of Life Institute where I think you talk about this paper for about half an hour so people who are interested can go and listen to that and you get more details on each of those different five problems. How do these problems tie together the long term concerns with the short term ones that we have today?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think the attempt was to come up with some conceptual problems that relate to both that have long term and short term versions. With something like a distributional shift, the short term version of it is something like the gorilla. The long term version would be something like, well I�ve trained in AGI in a simulation and then I put it in the real world and a lot of things are different. Does it break a lot of stuff without meaning to? The super intelligent version of it is like whatever, it�s like the marks-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s just the same but too extreme.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: More extreme, it�s building a Dyson sphere, it�s never built a Dyson sphere before where like if something go wrong, whatever outlandish thing you can think of. I think the point and the explicit strategy was that people often contrast long term versus short term approaches as if working on short term safety and long term safety and like different topics and like they trade off against each other. What I�d rather do is have a thread running from long term to short term things where you identify what the fundamental problems are. Then you work on them on short term problem. Then as the systems get more powerful, you update your techniques. It creates this more symbiotic where you�re following along.\\n',\n",
       " '\\n',\n",
       " 'I think safety shouldn�t be anything different from reinforcement learning. Reinforcement learning is a general paradigm for learning systems. You can do something as simple as walk across a grid, all the way up to playing Go, all the way up to perhaps building a system that�s as intelligent as humans. I probably wouldn�t literally use reinforcement learning but � the reinforcement learning is a general paradigm that runs from things that are very simple, the things that are very complicated. I guess the idea was to do the same thing for safety, come up with some general principles that will carry across towards very powerful systems. I wouldn�t say these problems tell you everything that could go wrong with powerful systems. I think there are almost certainly things that are very specific to powerful systems.\\n',\n",
       " '\\n',\n",
       " 'My general view is I�m much less confident in our ability to identify those problems. Maybe we can, some people are trying but let�s � my view is just, it seems like there�s a lot seen on the table. Let�s identify the problems we can identify, let�s work on them, and then whatever is left, we either have to work on them very late in the process or maybe someone can identify them but that seems like the higher hanging fruit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: The hope is that in order to solve the long term problems, you want to find cases that are similar today where you can get feedback on whether it�s actually helping.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, exactly. I think that there�s a magic of empiricism because it�s very easy to engage in long chains of reasoning about a topic that don�t get tied back to reality. Of course, the risk of working on short term stuff is that it doesn�t matter or it doesn�t generalize. The compromise I�ve come up with is try and think of things that are conceptually general and then try to tie them into empirics .\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: To that end, has OpenAI made any noticeable progress on these problems or other problems?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think about three weeks ago, Paul Christiano and I and Tom Brown here, and three people at DeepMind including Jan Leike, Miljan Martic and Shane Legg came out with a paper called �Deep Reinforcement Learning from Human Preferences�. This works on the reward hacking, scalable supervision side of things. The way this paper works is normally you have a reinforcement learning algorithm it has a goal or a reward function. The agent acts to maximize that reward function. This works pretty well for something like chess or go where the behaviors are incredibly complicated but evaluating the goal is pretty easy. With go, it�s like, are you in a winning position, you have more territory? With chess, it�s have you checkmated the king or have you been checkmated?\\n',\n",
       " '\\n',\n",
       " 'It�s really easy to evaluate these simple goals with the script and so you can run the algorithm through millions or even hundreds of millions of games and the goal evaluation is easy. Most of the stuff that we do in real life, the goal was complicated. It�s like carry on the conversation, or be effective personal assistant to a human, which means scheduling things for them, making their life easier but not enabling all their private information to their boss or whatever. There�s a lot of like context sensitive stuff, which is part of what makes, it�s part of what leads to safety problems. If I take a complicated set of goals like that and I try and forced it into the framework of a hard-coded reward function, it�s going to lead to something that makes everyone unhappy because the two things don�t fit together.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You said that it�s maximizing on one dimension and then fails on all of the others?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. Or just that the intrinsic number of bits of complexity in something like hold the good dialogue is very high. If I try and program that in, I might be going to be programming for a very long time in which case I�ll probably make an error. Or, if I try and make what I programmed simple, then there�s just not going to be enough bits of information to fit the actual complex nature of the goal. I�m either going to be very error prone or I�m just not going to be capable of learning what I need to learn. That�s why people talk about like strategies for absorbing values and things.\\n',\n",
       " '\\n',\n",
       " 'What our paper basically does to address this is it replaces the fixed reward function with a neural net based model of the human�s reward. The idea is you have a reinforcement learning agent that�s learning, and in the beginning, it starts acting randomly, and every one in a while, it gives some examples of its behavior to a human. It will come out with two video clips. The human looks of the video clips and says is the left better or is the right better? The human says left is better or right is better. If it�s playing pong something, then if the left is point got scored on you and the right is you scored a point, then the human will say the right is better. Then, the agent builds a model of what reward function would lie behind the human�s expressed preferences. The reward function becomes something implicit and learned, observed from the human�s behavior.\\n',\n",
       " '\\n',\n",
       " 'Then the RL  agent gets to work saying, �Yup! This is what I think the human�s goal is. I�m going to go and try to maximize this.� But then it comes back to you and it gives you more examples of behavior, and then, the human decides in those. Over time, the human is given more and more subtly different examples of behavior. The reward predictor in response learns to discriminate them and gets a more refined understanding of what the human prefers, and in the RL algorithm then tries to maximize that. The consequences of its behavior are then given back to the human. It�s this kind of-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s kind of three steps, like the human, and then AI that�s trying to figure out what the humans are optimizing for and then the thing that does that. But then, most of time it�s asking the intermediate AI, is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Both is to have [crosstalk 00:48:20] intermediate model of what the human wants.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, you have three parts. You have a model of what the human wants. You have the RL algorithm that�s maximizing that model, and you have the human that feeds-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That trains the-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: That trains the model. But also the RL algorithm feeds back to the human so that it basically, whatever the RL algorithm has learned to do, it goes back to the humans. It basically says, �Okay, is this what you wanted? Of the things I�m now doing, which do you want more?�\\n',\n",
       " '\\n',\n",
       " 'It�s this gradual preference elicitation, which helps to get around the � if you get things wrong by a little then you get the wrong behavior. It�s unfolding behavior in real time and incrementally showing you the consequences of the behavior that you�re seen. By no means does this solve all safety problems. It�s just one little bit of progress on one.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: One brick in the wall.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, one safety problem. This is an example of this thing I�m talking about. We use this both to solve ML task that you couldn�t solve before because the reward functions were too hard to specify and then impact on safety is obvious because it allows us to specify goals more easily. There�s all kinds of other problems, you can have with it, it has to scale, there are other safety problems you don�t want AI systems tricking you. There�s so much. This thing is � there�s an example of what we did. I think we�re going to try and do a lot more it.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How many times do you have to get feedback from the human to solve this problem? Is it a reasonable number?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. It depends on the task. On some of these ATARI games, which take about 10 million timesteps to learn, usually a human has to give feedback a few thousand times. Less than 1% or a tenth of a percent the human actually has to pay attention to. We managed to train this simulated little neural robot to do a backflip with a few hundred times steps. It�s the human clicking for about 30 minutes or so. We�re trying to get that number down because-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: This is the learning from human preferences paper?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We�ll put up a link to that. You can take a look at this little worm thing here that learns to, let us jump progressively from this flailing around than focusing on the ground.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It got a little bit of media coverage. My favorite headline was �What this back flipping noodle can teach you about AI safety�.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It seems quite a bit.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think it was �Here�s what this back flipping noodle can teach you about AI safety�.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That�s some good click bait. Apart from those five issues that you talk about in the paper, what do you think are some other important problems or open problems in the field?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: One thing we didn�t discuss in the paper is the issue of transparency of neural net. This is trying to figure out why a neural net does what it does, which you could eventually extend to why is a reinforcement learning system taking the options it takes. It just has a policy. It�s in a situation it runs a bunch of things through its neural net and it says, �I�m going to move left�, or �I�m going to bend my joint�. It doesn�t really have much explanation for what it does. If we could explain why, break down the decisions made by neural nets, then that could help with feedback, could help with making sure that systems do what we want them to do and that they�re not doing the right thing for the wrong reasons, which might mean they would do the wrong thing in another circumstance.\\n',\n",
       " '\\n',\n",
       " 'I think that�s a pretty important problem. My co-author on the paper, Chris Olah, did a lot of work in that area with Deep Dream, which is all of the back propagated images generated by neural nets that was originally designed to be a way to visualize what maximally activates a given neuron within a neural net. It was initially a transparency technique. That�s an area that Chris is very excited about. That�s another area I think we should work on. I mentioned before adversarial examples. I think that�s an area that�s already getting a decent amount of attention but probably should get more like everything in safety should get more. I think that�s an area we should work on, and also that has like short term safety implications. Someone could sabotage a self-driving car with adversarial examples. We certainly wouldn�t want that to happen.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We can�t have that.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Interesting. Is that a problem for a rollout of self-driving cars now? That someone might put up a word sign that confuses them?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I�m not the expert on it. I definitely don�t want to give anyone any ideas about how to do that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess it would certainly end up being criminal I would think to do that in the same way as hacking a computer system.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�d be extremely illegal. I don�t actually know the details of whether that�s feasible or not and wouldn�t discuss them if I did.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Of course, yeah. As we progressively work towards being able to control AIs that we�re developing, do you think it�s going to be possible for people [53:31] to understand the solutions that we have developed? You�ve discussed this three step process by which you train a machine or a reinforcement learning algorithm to understand the humans and then that trains the machine learning algorithm on the other side. I can understand that. There�s other big breakthroughs in history that you can get, like quantum physics, like it�s a particle and a half a wave and probably grasp it. Do you think it�s going to look like that? Or would it be just impossible technical details that-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: The AI itself or the safety?\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess the way that we�re going to get machine learning or like other AI technologies to do what we want rather than flip out in some way we don�t expect.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I guess there�s two possible questions. One is are we going to understand at a very granular level every decision that�s made? Then there�s, are we going to understand the principles by which the system operates? I think we better understand the principles by which the system operates. If we don�t understand those, I don�t know how we can build them. If we did build them, I would definitely worry about their safety. I think it�s realistic to understand the basic principles on which something is built. But then there�s a question of on what level of extraction do we understand it right? The principles on which a visual neural net are built are very simple. It�s back propagation and alternating linear and non-linear components. That�s pretty much all there is to understand. Then the question is how much do we know about what goes on inside the neural net. That�s the question of transparency.\\n',\n",
       " '\\n',\n",
       " 'I�m optimistic that we�ll gain a better understanding of transparency inside neural nets. The question is how does that actually help us on safety, how do we actually use it? There�s a lot going on inside neural nets even if we could individually understand every piece of them. How does that actually help us? There�s more units than I can read and understand. I have to have some way of translating  that into something actionable like correcting bad behaviors or something. Somehow, that component of it has to fall into place as well. I don�t know yet how that�s going to happen. I don�t know if it�s possible. I think it�s urgently important research area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Let�s turn now to how someone might be actually able to pursue a career in AI safety. What are the natural paths to getting a job at OpenAI or other similar organizations?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think my advice is going to be focused on the kind of AI safety work that I�m excited about. For example, MIRI does some safety work that�s more based on mathematics and formal logic. If you wanted to do that, you�d need a different background. The safety work that I�m most excited about, I think, it sounds obvious but the two things you most need are an extremely strong background in machine learning and a real deep interest in AI safety. I think to break those down, I think the first one is, certainly at OpenAI we really try hard to have a really high bar for hiring people. Just because someone wants to work on safety doesn�t mean that we lower the machine learning bar at all.\\n',\n",
       " '\\n',\n",
       " 'We have a lot of people here who are very good so going to get a PhD in machine learning, going to get a PhD with trying to work with the best people you can work with, doing the most groundbreaking work. There�s no ceiling to how much of this helps. My sense has been that people who have a deeper understanding of machine learning, if they�re interested in AI safety, also tend to really grasp AI safety issues better provided they think about them. That�s the second component, which is, I want people who really have a deep interest in safety not just, �Oh. it would be good if systems didn�t� � It would be good if self-driving cars didn�t crash but have a broad view of where we�re going with AI, which could be totally different from my vision, might not involve AGI but this general idea that we want to build machines that do what humans want and carry out the human will. I think that idea is a broad one and I want people working on safety to have a broad, broad view of that issue.\\n',\n",
       " '\\n',\n",
       " 'In the AI community, I don�t think the second one is lacking. There are many people who are passionate about the second one. I think the limiting factor is just a very strong machine learning talent.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: We just wrote a career review of doing a machine learning PhD, which we�ll put up a link to and then you can have a read. Is it machine learning or bust? Are there other options like other PhDs that people could do that it could be relevant like Computer Science or Philosophy or Data Science?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My PhD wasn�t in machine learning. We have a number of people here who have backgrounds in neuroscience or another area of computer science or mathematics or physics. It�s entirely possible if you happen to be educated in another area to go into this field. But I think going forward, if you�re a young student, I don�t particularly see a case for doing a PhD in another field if what you want to do is machine learning.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Grab the bull by the horns.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I guess I�m saying it�s pretty easy to convert skills in related areas. Sometimes it gives you perspectives that you don�t have. If you want to do machine learning, you should get a PhD in machine learning. I think another thing I�d add is we do have some people working here who don�t have PhDs. My co-author, Chris Olah, actually never even went to college. He just straight went to Google. He had to do a lot to prove himself. The level of technical ability you need to show is not lowered, it�s even higher when you don�t have the educational background, but it�s totally possible.\\n',\n",
       " '\\n',\n",
       " 'I would say the most important thing is just being able to do a lot of impressive and creative machine learning work. I would even go so far as to say it�s not my expertise but even the people doing safety work that doesn�t involve machine learning, I get pretty nervous when we don�t have a strong background in machine learning because even if they think that a machine learning system can�t be made safe, they should know enough to understand why they think that�s the case and what they think the alternatives are.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That includes, I guess, people doing mathematical research or philosophy research?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If it relates to AI safety. I would say that even those people, I would encourage them to learn as much machine learning as possible if only because they should understand approaches that they�re partaking.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it fair to say that you think that the approach you�re taking where you study machine learning and try to actually improve AGIs is the best way to make AGI safe that you�d rather see someone do that than go into these other adjacent areas?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s a little complicated because I think that as systems get more complicated, there may be ways in which we combine neural nets for formal reasoning. There�s been some work by my friend, Geoffrey Irving, and some of his colleagues on doing theorem proving. Basically, using neural nets to select the lemmas to be used for the next theorem. If you take that far enough, you can imagine versions of reasoning systems that basically, they traverse some well-defined reasoning graph. They make logical conclusions that are tractable but it�s all driven at the bottom by neural nets driven intuition. The neural nets decide what conclusions you draw and where your thinking goes.\\n',\n",
       " '\\n',\n",
       " 'I think this is how humans do symbolic tasks like physics or math or anything like that. We�re neural nets at the bottom and then we have a layer on top of that that is � we use those neural nets to represent symbolic reasoning. Computer could probably do that even better because it can make sure that it never makes a mistake in this symbolic reasoning. The symbolic reasoning engine is there. You can imagine having formal guarantees on that formal reasoning. I think when we get to that it�ll look different from the way things are currently being done. I�m really not against using formal reasoning methods and using mathematics but I think it�ll be possible to do that work more productively once we understand how it fits in with current systems.\\n',\n",
       " '\\n',\n",
       " 'I actually don�t know. Maybe there�s stuff that�s being done now is productive but I�m pretty suspicious of anything where you can�t get that type empirical feedback loop because I think it�s really easy for people to fool themselves.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It sounds like the key piece of advice is do a PhD in machine learning. What universities can people go to? What supervisors for a Phd?  Do you have any suggestions then?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I do want to repeat again that there are ways not to do a PhD. In particular, a number of people go to PhD for a year or two and then doing internship here or at DeepMind or somewhere, and then get hired so you can partial PhD can work. That said, I think the usual suspects are places like Stanford, Berkeley, Cambridge or Oxford in the UK, Montreal, Yoshua Bengio�s Group is pretty well-known for doing a lot of good stuff. Then, there are kind of number of other places. Definitely, the PhD pools are where we hire. A lot of our folks, because we know a lot of the relevant professors.\\n',\n",
       " '\\n',\n",
       " 'Again, we have some people here who didn�t do the PhD work. I think the most important thing is being able to keep up with the literature and make, creative, original discoveries that are novel and that stay on pace with what everyone else is doing. If you can do that then that�s the best thing. When I talk to people who I want to switch into machine learning from another field, the advice I always give them is just get every possible model you can. If you�re trying to learn supervised learning, get all the image now, models like implement them yourself, read the paper, implement it, read the paper implement it. Same for supervised learning, same for generative models. You just get this knack for it after you�ve done it for a while. It�s really just practical hands on experience. You just get a sense for these things once you�ve done them for a while. You also find out quickly how good you are.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I might be showing my naivete to ask this but machine learning is only way that you can approach AI right? There�s like other paradigms on how you produce another artificial intelligence, is that right?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this has historically been the case. It�s gotten a little bit, I think complicated. In the old days, we had things like expert systems that were based purely on logical reasoning. Generally, they found that those systems were very, very brittle because they couldn�t represent the high dimensional space that we see. For example, vision system that�s based purely on rules, it�s difficult because if I�m trying to identify a face or an object, I�m trying to identify these blobs in distribution space. In some sense, these problems are inherently statistical. The rule based systems actually don�t end up working all that well. Historically, there were statistical system and there were rule based systems.\\n',\n",
       " 'I think we can say, now the statistical systems have pretty decisively won. I sometimes hear people say things like, �I don�t think AGI could be built using machine learning or I don�t think AGI could be held safely using machine learning. I don�t think AGI could be held safely using machine learning or something. I think when people say that they aren�t really thinking carefully about the alternatives, I�m quite sure that a pure rule-based system is just not going to work because of the thing I said that you have to ground what you�re doing and sensory information and the sensory information is just inherently statistical and fuzzy. Pure rule-based systems I think are not going to work. What could happen is the thing I described before where you have a machine learning systems, deep neural nets being used to drive logical reasoning systems.\\n',\n",
       " '\\n',\n",
       " 'That will be a hybrid of the two but people are working that, that will be considered within the field of machine learning. I think we�ll often, in the future, machine learning may include logical reasoning processes but there�ll be at a higher layer. What ends up happening, it will involve reasoning in the same sense that goal-playing systems involve a goal but it won�t really be like those rule-based systems that we had before. You can�t be 100% sure of this but I think just the basic argument that percepts are these statistical blobs and so you have to use the statistical system at least at the beginning to measure them. Then whatever concept you draw from them end up being fuzzy statistical concepts. If you want to bring those back to logical reasoning, the reasoning have to exist on a plane that�s subtracted from that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So there�s not some other AI paradigm that people should be doing if deep studying?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Again, in the history of AI, there were a lot of rule-based systems. Then there were critiques written of them. I forgot the guy�s name, Hubert Dreyfus or something who basically, he was like this continental philosopher who wrote this critique that people found really hard to understand but what it was really saying was just percepts of these statistical blobs. If you make a rule-based system, you�re not going to � you�re always going to make mistakes and your system is always going to be brittle and wronged a lot.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Assuming someone has been studying machine learning or one of these other related areas that�s potential path in, is there a natural path from studying to actually getting a job at OpenAI or another organization? Are there intermediate steps that people have to take?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I think, again, PhD students who do impressive first author work on papers are people we are generally very excited to interview. If you�re in a good PhD program and you do some work, then I�d definitely do some good work. I�d definitely encourage you to apply here. For people who are relatively early in their careers or come from another background, there is a program at Google called The Brain Residency program, that allows you to study machine learning with the experts for a year. Then, that allows you to know to train your skills. We�ve had a number of residents who have applied here or elsewhere. That ends up being a good thing.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Speaking of which, there�s a bunch of different organizations, right? There�s OpenAI, there�s Google DeepMind, Google Brain, Vicarious was another company or maybe more in the past. There�s the human compatible AI group at Berkeley. You can like go through a couple of these that you might recommend working at?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: As I said, OpenAI and DeepMind are probably the most focused on reinforcement learning. They probably spend the most time thinking about AGI. Not everyone here does but it�s a focus here more than it is elsewhere. Google Brain is where I was before coming here. That was the original research group at Google. I would say a more kind of decentralized group that works on a wider set of topics. Chris Olah there thinks about safety. There is � you mentioned the Centre for Human-Compatible AI which is Stuart Russell�s group. We collaborate with them some. We have some interns from there come here. I think Stuart�s been someone who�s been thinking about safety for a while. That�s another good place to work on it. Vicarious to my knowledge, doesn�t think about safety. I could be wrong.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Are there other groups that you missed here? Are there any other government research projects? Anything in China?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Not that I�m aware of. Of course, there�s MIRI and FHI.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Right the Future of Humanity Institute and the Machine Intelligence Research Institute (MIRI). They�re less doing machine learning where it gets more strategic and-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Their focus is less on machine learning although I think Stuart Armstrong at FHI collaborated a bit with DeepMind, which something that I think was broadly machine learning related. This was like studying interruptibility and corrigibility or something like that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: At 80,000 Hours, we talked to people reasonably open who would be interested in doing a job like what you�re doing. Is there any way that they can get indicators early on about whether that�s possible or whether they just wasting their time and I�m sure looking at other options because if it�s not going to be a good fit, either they don�t maybe have the machine learning chops so cultural they�re not going to a good fit or some other reasons.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think the thing I mentioned about implementing lots of models very quickly. If you want to know where you�re good, a way that is a good proxy for how well you�ll in grad school at well at for the test we give when people apply here is find the machine learning model that�s described in a recent paper, implement it and try and get it to work quickly. This is a painful process for you and you really don�t like doing it, then you aren�t going to like any of the research that of either on AI safety or other AI stuff.\\n',\n",
       " '\\n',\n",
       " 'If you find you can do this quickly and/or, you really, really like doing it. You find it addictive, then that�s an indicator that this is something, this might be something you really want to do. I wouldn�t worry about the cultural stuff. If you�re skilled in this area and passionate about this area, I don�t think you�ll have-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That�s not going to be a barrier.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I don�t think it will be a barrier. I don�t think you�ll have any problems. You try and be ask open and welcoming as we can, we don�t have the luxury of selecting people and, anything other than their.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You don�t like our favorite TV shows.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I mean. That�s just wasteful and pointless.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah, absolutely. How well you can people to do that? Cannot they do that as an undergraduate? Is it more like a PhD level thing?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: People can do it in high school like, if you meet a 17 year old, how do you get them into machine learning. I�m like, just go home and implement these models. You actually don�t need any kind of formal education. You probably need a thousand dollars to buy a GPU. I have considered various times, shall we have a like for grand program where it�s like, if you�re like 17 years old and you want to get into machine learning, I�ll just buy you a GPU.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah. Yeah why not.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: If you�re interested in AI safety, that was a thousand dollars. Most adults living in the developed world can afford a thousand dollars but most 17-year-olds might not be able to. If they don�t already have access to one it might be a good way to get people started early.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If there�s a 17-year-old listening here who wants to go and build their machine learning model, what should they Google for, what should they start reading?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Because I�m from OpenAI/DeepMind direction research, thinking about reinforcement learning, trying to-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What�s the difference between Reinforcement learning and Machine learning? Do you want to-?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Machine learning is the broader topic, within there are several different areas. There is supervised learning which is where you try and predict some data that�s been labeled. An example of a supervised learning problem would be like you�re given images and they correspond to objects. This is an image of a dog, this is an image of cat, this is an image of a computer. You train the network on lots of pairs of here�s the image, here�s what it is. Then it learns over time to map the two to each other.\\n',\n",
       " '\\n',\n",
       " 'Supervised learning has this static quality where it�s like a one off. You�re trying to like, predict one thing from another. Reinforcement learning is more a setup where you�re interacting in a more intertwined way with an environment. The game of go is like this. You make a move, and then the opponent or your environment makes a move, and then you make a move again and overall you�re trying to win the game and the reward or figuring out whether you�ve won the game or how well you�re doing can be delayed by a long time. The reason I focus a lot on reinforcement learning and why OpenAI focuses a lot on it is that reinforcement learning and things like it, the extended versions of reinforcement learning seems like a better fit for what intelligent agents do in general.\\n',\n",
       " '\\n',\n",
       " 'Often, I have very long range goals, I�m trying to get an education, trying to get a PhD, trying to have a career, trying to start a family or something. These are all things that unfold over years and involve interacting with my environment in this very complicated way. Reinforcement learning is the only paradigm we have that even close to capturing this,\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Sorry. I cut you off. We�re figuring out what the seventeen year old should read to get that foot in the door.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Lots of papers in reinforcement learning. I read about what�s called the DQNs � it�s Googlable. It�s not common acronym. Deep-Q learning that this was a paper done by DeepMind in 2013. Policy gradients, particular A3C and just follow the tools of recent reinforcement learning things that have showed up on arXiv. Just go to /r/machinelearning on Reddit and look at some recent papers in the deep neural net literature. Look at them try to re-implement it, see if you can get results as good association the results that others get. It�s really pretty self-contained and you don�t need that much help. If you�re having trouble getting started implementing them, then you can start by fermenting popular papers like DQN, you can find it an existing implementation that start with that and try to fiddle with it to see if you can make it better.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What kind of program or you running, because you�re moving out doing this in Excel. What�s the-\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No. It�s typically, you�ll use a, the typical is a Python with Tensorflow. Tensorflow is this tool that Google Brain team made for doing general computations but in particular deep neural net computations. You�ll find a large fraction of this office implemented in Tensorflow or some similar framework so python�s pretty easy to learn, Tensorflow is pretty easy to learn. Read some Tensorflow code as some stuff that�s been implemented, learn Tensorflow and implement some stuff yourself.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What�s the range of roles available? How do they vary?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: You�re talking about within machine learning or within safety-related stuff?\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess mostly within safety I think.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Okay. I can talk about what�s being done here at OpenAI. I would say there is two main directions. There�s the technical safety stuff, and there�s the policy side of things. On the technical safety, there�s not a lot of people working on it yet but the Human Preferences paper that I showed you is a good example of it. A lot of the papers we cite in Concrete Problems are good examples of this work. DeepMind just have some recent good examples of safety work. I think the skill set, as I�ve said, is very similar to the typical machine learning skill set. You should also be willing to work in the field that has relatively sparsely populated literature, which means coming up with your own ideas or working very closely with someone who�s one of the people generating the ideas in the field.\\n',\n",
       " '\\n',\n",
       " 'That has some downsides in that. You have to set more of your own direction but also has some upsides that you can be one of the first people in a to totally new field. That�s what excited me about writing Concrete Problems. I can work on something that 200 other people will work on or I could try and set a new direction. Maybe it won�t be exciting at all and you know, �Oh well! At least I did something interesting.� Maybe it�s turns out to be really exciting and that�s like a bet that I�m perfectly happy to take.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What would you recommend to someone who is considering entering the AI safety industry doing the machine learning work. They�re worried that they�re not going to have such great long term career options elsewhere, especially compared perhaps to doing machine learning work in a more commercial way with less of a safety focus, of just going into what pays the most or has the best career.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think ML work is so hot right now that anyone who goes into it, particularly on the fundamental research side, it�s easy to transition to applications. I think the kind of safety work that we�re doing has many of the same skills as any other area in machine learning, even though the subject matter is very different. I think some who does that is going to be in a very strong position to do very well in the future. I think it�s probably, even if we�re going into for altruistic reasons, it probably just also having to be on the most secure career, high paying, financially secure career areas. You could go into � we recently had someone leave OpenAI who became head of AI at Tesla, head of all of AI reporting directly to Elon Musk.\\n',\n",
       " '\\n',\n",
       " 'I myself want to stay at OpenAI and work on safety. I want to keep working on the research end all the way until we get AGI whenever that is. If I didn�t want to do that, if I wanted to leave, there�s like plenty of wonderful things that I can do and the same would be true of other people who come here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it more of a concern from people who would be working at MIRI doing non-machine learning safety research.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I mean I think most of the people there are smart people who either had or could have really great careers as software engineers. They probably have great, great, great options as well. I generally get the sense, people who go to MIRI are really passionate about MIRI�s mission and tend to worry about this less. The amount of buzz and hype is definitely not as high as it is for the machine learning bolt.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Pretty often, we talk to people who are saved in the mid 20s and they did a fairly quantitative degree, maybe like economics or logic. I do know machine learning particular. Is it possible for them to great to get into this? Was it all just over for them at 25?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: My own example is, until I was 28 or 29 or something, like I hadn�t any machine learning. It�s definitely possible to do this. My main advice is the same advice that I�d give to you know, the 17 year old that we talk about earlier, which is just implement as models as you can as quickly as you can. Just to see if you have the knack for it you really enjoyed doing it because this is going to be greater than 50% of what your job consists of. Just knowing how to have the real intuition to implement neural net models and have them work, how to put together new architectures that do new things.\\n',\n",
       " '\\n',\n",
       " 'First, implementing these papers and then tweaking them. That�s a really cheap to give out whether this is a career that you�re good at and that you�ll enjoy. I wouldn�t recommend by going back and doing another PhD in machine learning. I think once you have a PhD like there are some positions where Google wants you for instance, wants you to have a PhD when they hire you. They don�t really care what area it�s in. They do want to know that you�re like committed to some new area that you want to go into but it�s more important for the places that care about whether you have a PhD, which we don�t care that much. Even though at places that care, things more important, you have a PhD and that you have it in some particular field.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: If you already have a PhD in Philosophy, then you should go and learn ML directly or do some internship somewhere.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I would say learn ML, implement a bunch of models then go do an internship or the Brain Residency program at Google, come do an internship with us or at DeepMind, all these are viable options and each step gives you a better ideas of whether this career path is really for you.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Really, it�s just a case of someone who did an undergraduate degree in Economics can jump in and try to learn machine learning, try to train machine learning on my computer.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I think there�s � if you know how to program Python and you can learn Tensorflow quickly that it�s a very empirical field. Of course there�s lots of hidden knowledge that researchers know that they tell each other but that it�s hard to express in the papers. You won�t pick up on everything but you can certainly get started this way. Then talking to people about models, you�ve implemented talking to professional researchers to get a sense of what�s an exciting to work on next. That�s enough to get you started.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How does working at OpenAI or Google compare to machine learning role in academia?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I generally tend � I�m a bit biased but I generally tend to give people the advice to come to industrial labs. I think one reason is the industrial abs have gotten, by industrial labs, I mean OpenAI even though it�s not for profit. Just the large non-academic research center, they tend to have more resources, more compute, and in part because of this, I think they�ve been winning the talent war recently.\\n',\n",
       " '\\n',\n",
       " 'I think it still makes sense to go do a PhD. I think staying in academia your whole life, I mean, I guess if you become a professor then it becomes a lot easier to collaborate with the industrial labs. Both we in DeepMind have people who were our professors and spent part of their time there and part of their time here. It�s all very feasible and there�s a little of mobility between the two. But in general it felt many people will disagree with me but I felt that the most groundbreaking work has tended to arc the industrial labs. Over the last couple of years at least.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Yeah, interesting. Is there any effort to change them? Are you universally is trying to catch of it just too expensive to get the research projects.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, there�s Yoshua Bengio�s group in Montreal as you know is quite larger, it�s one of the few major figures in deep learning who�s resisted the pressure to go into the industrial world. His lab does a lot of great work, Pieter Abbeel at Berkeley, Percy Liang at Stanford, and just a number of others including folks who do work that�s not necessarily related to deep learning. There�s a lot of interesting work everywhere. At least the kind of safety that I work on tends to play best with the cutting edge of ML work and explicitly tries to keep up with the cutting edge of ML work.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: That reminds me, given the cost, how is OpenAI funded? Is it just the donations or are you also like selling products at things point?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No, we�re non-profit. I think I mentioned earlier the major donors are Elon Musk, Sam Altman and Dustin Moskovitz at this point.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s just the donations.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: At this point it�s just donations, yes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Okay, interesting. Do you think, is it possible for you to sell things to go extra computational power if you need or like it starts selling services. It�s a legal issues, it�s outside the area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, I�m not an expert in this. I think � I�m not sure you can sell stuff if you�re non-profit.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is the work frustrating because you�re not sure whether solutions actually exists and you beat your head against the wall for quite a while before you figure out, well maybe that there isn�t a way of solving it the way that you thought?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think actually that�s the case in any area of machine learning where you�re trying to do original research. If you�re trying to do something worthwhile, they you don�t already know if it can be done and you have to try stuff that seems crazy. It might not � it�s true if any area, especially true of an area that�s very new like AI safety. Yeah, I definitely agree that one of the trade offs for working with AI safety is that on one hand you have this exciting ability to work on a new field that�s just starting. It could be very impactful but at the same time no one�s defined what successful work looks like.\\n',\n",
       " '\\n',\n",
       " 'We�re still laying out what the problems are and what the work is that needs to be done. I think it definitely requires an attitude of being willing to do more to define problems yourself. You need to be more creative instead of doing something that�s an incremental improvement on the thing, the thing that was done last. To me that�s a good property.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Is it a good role for someone who a lot of grit and willing to persist with things despite adversity. They�re pioneering in your area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think that quality is useful in any important or original work and it is here as well.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Turning now to non-machine learning approach I to tackling the problems in AIs safety. What kind of non technical approached do you see as promising? I interviewed Miles Brundage of the Future of Humanity Institute, recently. Do you have a view on any of the AI policy topics that we spoke about?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah. I don�t know precisely what you guys spoke about. I spend a little bit of my time speaking about the relevant policy issues. I think if the humanity at some point builds AGI then we�re going to have to think about both how to handle safety issues as we�re building it. Some of the coordination issues that going to come up with respect to safety and also the question of who uses it, what it�s used for. One example is this, you can imagine that maybe if it�s possible, a really good way to build again would be to build an AGI, and instead of doing anything with it in the world, try and, if it�s possible, first developed the develop the capability to have it advise you on this situation you put humanity in by building it. Look, we just opened this can of worms by creating you. Can you analyze our strategic situation and say what we should do because we�re aware that if we don�t use you in the right way, or we hand you to the wrong person, then it could be really bad for humanity.\\n',\n",
       " '\\n',\n",
       " 'If we�re able to turn the problem in on itself that way, they would be really good. That�s partially-\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Get the AI to make the world safe for AI.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, it�s partially a technical question and it�s partially a policy question, which is how do we get ourselves in a situation where we can do that. I think that there�s a lot of players. There�s going to be more AI organizations, government actors, will someday have something to say about Ai. They already have something to say about AI. Someday they�ll have something to say about AGI. When we�re more in the world where AGI is going to happen. What strategies should we take towards all of the actors. How do we make sure that when everything is put together, it leads to a good outcome. Is there anything we can do today to deal with these distant problems.\\n',\n",
       " '\\n',\n",
       " 'Those are the set of policy issues that we tend to think about. There�s also some more thought on short term policy issues. How can we get people to think about more mundane issues of safety, should the government regulate things, what should policies be on self-driving cars and stuff, what should policies be on automation and job creation. We do some of that stuff but a lot of people think about that so we tend to focus more on the long range stuff. It�s less actionable business there aren�t that many people thinking about it. We might as well do whatever thinking we can on it. Which might be there�s nothing that actionably be done, but we want to at least consider.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you have any thoughts on how we can ensure all of the players cooperate and avoid having an Arms Race where they just try to incorporate and avoid having an arms rice where they just try to improve their machine learning techniques really quickly without regard to safety. It seems like you�re collaborating a great deal with DeepMind.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yeah, this was in part motivated by the idea of the orgs working together. It helps that myself and some of the founders of DeepMind have known each other for a while. We all think about AGI and think, that safety issues are important. When people at the major organizations are friends with each other and work to actively collaborate, then that reduces the probability of any kind of conflict because people know each other. There isn�t fear or uncertainty. If there�s a disagreement, we can work it out. Then the question is, how does that scale to there being a lot of organizations. How is that scale to others who get involved once they see how powerful AI is. Can we make them cooperate as well? My hope is that we can. No, it�s not an easy thing.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you think it would be good or bad thing if AI were developed sooner. There�s been a kind of this explosion of investment in machine learning and improvement. Is this something that we should be pleased about or concerned about, or just neutral, we�re not sure whether it�s good.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It�s hard to say, the obvious bad thing is if you�re like Really afraid that there�ll be safety problems with AGI then you might think it was a bad thing. A lot of people think it�s bad thing for that reason. My view is we�re relatively early in the game and I think there�s a substantial probability that the gloomy analyses are really misunderstanding the safety problem and how a safety problem works. Some counter wrist to worry about that something bad happens to the world in the meantime while we�re trying to develop AGI, or that AGI is used in a bad way. I guess a couple of years ago, I often made the argument that we were in a relatively peaceful geopolitical time so it would be good that AGI will be built. I�m starting to wonder that in the last year we�re not in such a peaceful geopolitical state.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s a little bit less clear.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Last year so that maybe we�ll not in such a peaceful geopolitical state.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: As we�re recording everyone is flipping out about North Korea developing intercontinental ballistic missiles.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: No, these things are pretty deeply concerning. There�s been a lot of political instability in the western world in the last year. Aside from the usual reasons why this might make me unhappy, it�s made me unhappy because it creates a less stable political environment in which AGI would happen. I don�t know. I will say, I think we�re better off if AGI is developed in a stable political environment with leaders who are intelligent and have reasonable views. I�d like that to happen. I no longer know whether that means that AGI can happen soon or that it should in a long time. I guess it depends whether the current trends that we�re seeing in the last year continue or if they�re only a blip. If they�re only a blip, then that doesn�t matter. In a few years, we�re back to where we are before. But if we�re on a general trend in bas way direction, then maybe it�s bad to wait to long.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I guess that�s the difficult thing to time.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: I think it�s pretty complicated. I think pure safety considerations tell us that it�s always good to have more time although at the same time, some of the hardest safety problems to solve maybe problems we can�t solve until the last couple of years, until we build AGI. In that case, delay doesn�t really help us. It just delays the crunch period.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: It�s like someone trying to finish an essay by a particular deadline. If they know they�re only going to do it the night before, then it doesn�t much matter when the deadline is.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: It doesn�t much matter when the deadline is. I think it�s a complicated question. It�s not a variable that I have a lot of control over. It�s happening at the field level. I prefer to try to control variables that I have some control over. One thing I have control over is that it seems like there�s at least some safety work that can be done now and so I�d like to do it. It seems like there are some ways that different AI organizations are not collaborating now that we can encourage them to collaborate. I�ve also been working on that. I think those efforts have been successful and so I feel like it�s been good to cause things to happen that wouldn�t have caused otherwise. Then there are all these other things that I feel like I have no control over whatsoever.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�ve taken out an awful lot of your time here. I�m sure you have to get back to your research, hit these deadlines. Anything you�d like to say to people who are considering following your example and doing this research before we finish?\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: We�re of course hiring for very talented, machine-learning people who care a lot about AI safety. We welcome applications at OpenAI. We collaborated a lot with the DeepMind safety people. I�m always, as part of this collaborative spirit, I think that�s a really great team as well and people should apply there as well. It�s convenient to have a place that�s in Europe and a place that�s in the US. I think there�s a lot of good work going on at several different places.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: I�ll just add 80,000 hours has been doing a whole lot of research into his question of how can we positively shape the development of artificial intelligence and we�re coaching some people to try to help them get jobs at places like OpenAI. If you feel like you�re in a really good position to do that, then fill out the application on our website. We think it�s one of the most high impact roles that someone could take if they�re able to do it, which is one of the reasons why we�ve looked into it so much. Hopefully, over the next few years, we�ll see quite a lot more people going into this field and it wouldn�t be so neglected. But it�s been fantastic to have you on the show, Dario.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Yes, thanks for having me.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Hopefully, we can check back in a couple of years and find out what OpenAI has been up to, and hopefully you�ve found lots of new talented people to work in the area.\\n',\n",
       " '\\n',\n",
       " 'Dario Amodei: Hopefully.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Fantastic. All right. Thanks so much.\\n',\n",
       " '\\n',\n",
       " 'Hi I�m Robert Wiblin, Director of Research at 80,000 Hours. Thanks so much for joining. The last few episodes have been really well received and it�s been gratifying to see our regular audience growing quickly.\\n',\n",
       " '\\n',\n",
       " 'If you haven�t subscribed yet, search for �the 80,000 Hours Podcast� wherever you get podcasts. That way you�ll never miss an episode about something you�re interested in, can listen in while you�re cleaning or travelling, and can speed the conversation as you like.\\n',\n",
       " '\\n',\n",
       " 'Today�s episode is the longest so far � two and a half hours. But if you listen through you�ll know a hell of a lot about why pandemics are such a scary threat and what we can all do about them.\\n',\n",
       " '\\n',\n",
       " 'We�ve put an index in the show notes and the associated blog post so you can look through and skip to any section you�re particularly interested in.\\n',\n",
       " '\\n',\n",
       " 'So, enjoy!\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Today, I�m speaking with Howie Lempel, who, until recently was a program officer for the Open Philanthropy Project, where he�s worked on problems including biosecurity and pandemic preparedness. For those who don�t know him, the Open Philanthropy Project is a foundation whose goal is to make philanthropy go especially far in terms of improving lives. It�s the main philanthropic vehicle for Cari Tuna and her husband, Facebook co-founder, Dustin Moskovitz, who are expected to give about $8 billion over the course of their lives. And for disclosure, 80,000 Hours� biggest donor is actually the Open Philanthropy Project. But prior to working at the Open Philanthropy Project, or OpenPhil, as we call it, Howie studied at Yale Law school and worked for the Brookings Institution and various criminal justice reform organizations. Thanks so much for coming on the show.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Thanks, Rob. I�m really excited to be here.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So we plan to talk about the nature of the risks from pandemics, what can be done about them, and how listeners can best use their career to reduce the threat. But first, what did you work on at the Open Philanthropy Project?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Sure. So when I started at the Open Philanthropy Project, it was a brand new organization. It was like a mix between being a large foundation and having the feel of a smaller startup. That meant that at the beginning, everybody was a bit of a generalist. So when we started, our first major project was deciding what areas of philanthropy, what problems and causes seemed especially promising for OpenPhil�s giving. So from our first year or so, my main work was doing research to that end, and the questions that we asked to choose our program areas were: could we find causes that had a really big scope or scale of the problem that we were trying to solve, that really affected a lot of people and had a big effect in their lives, where it didn�t seem like there was enough philanthropy that already existed? And also where it seemed like there were concrete things that more money could do to help.\\n',\n",
       " '\\n',\n",
       " 'So we started with this huge list of all the causes that seemed like it could be potential candidates, and I was one of the people at the start who did really shallow looks at each of those causes. So, talking to maybe two or three experts and getting their sense of helping us understand what the problem was in that area, what people were already working on, what sort of important possible solutions there were that people weren�t working on yet, and start getting a sense of what philanthropy could do in the area. So my job at first involved both reading up, quickly getting to know a lot of different areas, and then going out and reaching out to the real experts in the field, and trying to get their sense of what more philanthropy would do in each of those areas. And over time, as we started to specialize, the job changed because we dug in more.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So this is extremely similar to what 80,000 Hours does, or is part of what we do, and I think we actually copied the framework that you were using initially. So our problem framework is to look for issues in the world that are really large in scale, that not many people are working on, and where it seems like you could easily make a difference by spending more resources, so like scale, neglectedness and tractability we call it. We�ve relied on Open Philanthropy Project�s research quite a bit in forming our own list. So I guess the openness has been pretty useful for us.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think the same types of questions that 80,000 Hours is asking, when they�re thinking about how someone can do as good as they can with their career, that�s the type of question that Open Philanthropy is asking, but thinking about how a foundation can do as much good as they can with donating money.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: And I suppose that there are some differences there between where you need talent to go and where you need money to go, but because they can convert between one another it�s a pretty similar problem.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Absolutely.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: A lot of what OpenPhil did in the early days is what we call our global priorities research, trying to figure out working on which problem you can have the largest impact with your money or your career, and I think in the future we�ll probably have a full episode devoted just to that topic. It�s one of our recommended areas because there�s very few people doing it and it seems like you can have a very large impact by shifting resources from problems that are somewhat less pressing for the world toward the problems that are much more pressing.\\n',\n",
       " '\\n',\n",
       " 'And potentially we�re not allocating resources in a very effective way overall because there�s just not many people who are trying to make these prioritization decisions, trying to figure out do you accomplish more good by focusing on health or education, or on these global catastrophic risks or something else completely different. So OpenPhil has been able to make quite a lot of progress in a short amount of time just by looking at something on which not a lot of people had already tried to work. Is OpenPhil a place that you would recommend people work?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, absolutely. I think working at OpenPhil is one of the best career decisions I�ve made and I think it�s one of the best places in the world to work on cause-prioritization-type issues because I think it�s pretty unique in that it is open to so many causes and really cares about cause prioritization, but also has the resources to actually � Once it chooses a cause, make a real bet on that cause, and I think that that really is a good learning opportunity when you�re working on cause prioritization. So knowing that you�re eventually going to actually have to make a bet, and that the organization is going to make grants, I think it imposes some discipline on�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Focuses the mind.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think that that�s right, and then also I think there are certain questions that it forces you to ask about, �How much money could this field absolutely absorb?� Like, �Are there good reasons why X area is neglected? That you would only know if you talk to potential grantees.� That type of thing, that it�s pretty hard to get if you�re not in the position of grant-maker. So I think it�s a pretty unique place to do that type of work and, yes, I�d definitely recommend it for people who are interested.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you know if they�re hiring at the moment?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I don�t.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Okay, yes, well we�ll stick up a link to their vacancies page. I don�t know whether they�re hiring right now but they might well be at some point in the next six months, so yes, you can potentially get on their mailing list and find out about that. What was your research work at OpenPhil like day-to-day?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: It depended on exactly what task I was working on, but a lot of the work was first spending as much time as I could really talking to experts in the field and learning from them. So it�s a lot of phone calls reaching out to top people, reading and trying to get to know a field well enough that I knew, and advise quickly as possible the main open questions that would affect whether or not we wanted to enter a cause area, and then try and figure out mostly through Internet research, academic research, who the people were who could answer those questions, and then trying to reach out to them and look at their answers.\\n',\n",
       " '\\n',\n",
       " 'Then once we identified areas as particularly promising, the work changed a little bit, and there were some cause areas where the next step was really to go out and find grant-making opportunities. That meant a lot of going to conferences, getting to know everybody in the field, trying to understand what role every organization played, whether there were things that seemed like gaps to us, types of organizations that really ought to exist and didn�t, or organizations whose work we would really like to expand, and then go out and make friends ourselves. Sometimes, for program areas where we really felt like we needed an expert or a specialist to scale up our grant-making who had � Instead of whatever amount of expertise I can get in a few months of learning about an area, there are people who have spent years and years building up their networks and expertise.\\n',\n",
       " '\\n',\n",
       " 'So sometimes my job is to find that person and go out and trying to recruit someone. And again, that actually involved relatively similar work. The difference between taking a bet on an individual who you�re going to then give the responsibility to do overt grant-making, versus taking a bet on the organization you�re going to grant to is pretty similar. So it�s a lot of going out into the field, making sure everybody knows my face, going to conferences, getting to know people, and getting them to � building up trusting relationships that you could ask them for their advice on who we ought to hire to go and run a program area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So did people suck up to you a lot because you had a lot of money to give away? Was it hard to know who to trust?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes. I think that�s one of the hardest parts of working for a foundation, is there�s a pretty awkward relationship where you have a lot of power, and people really do want to please you, and it takes a lot of work to build a trusting relationship where people feel like they can give you negative feedback, people feel like they can be honest with you, and yes, I think that�s a major challenge working at a foundation.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So do you think the work had a large impact?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I really do think it did. I think that there are relatively few foundations like OpenPhil that are large and have that level of resources, and that also have OpenPhil�s openness to really going into almost any cause area where it feels like it could make the biggest difference. So I think that OpenPhil really was able to find some areas where there was nobody available to give the kind of philanthropy that was needed and really make a big difference. So one area that�s really important to me is working on reducing the suffering caused by factory farming. And OpenPhil was able to enter that area and become really one of the largest donors in that area.\\n',\n",
       " '\\n',\n",
       " 'And similarly, in the area that we�re talking about, pandemics and biosecurity, it�s an area where there�s a lot of funding by governments, but there are certain activities that really can�t be done by governments, so something like political advocacy or policy advocacy often can�t be done by governments. It�s sort of certain really sensitive questions that governments often can�t really work on, so I think it�s really important to have private sector money in there to hold governments accountable. I think that�s something that OpenPhil really was able to do in an important area and it was really clear that there weren�t many other foundations in the area available to give that kind of funding.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: You�re saying in a sense that the field isn�t neglected in terms of what governments can do, or not so neglected, but then in terms of what independent actors, nongovernment actors can do, that those things often aren�t getting done because there�s just no one there?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Well, I would say that there are two issues. One of them is exactly what you said. It�s an area where � Public health is one of the big priorities of a lot of governments, so there is a lot of spending in that area by governments. There is almost no spending on things like policy advocacy, think tanks. The whole NGO sector that usually exists in important policy fields was really neglected, then there�s a second issue, which is my � The main thing that I care about in this area, and OpenPhil�s priority, was in particular, reducing the chance of the worst-case scenarios among pandemics. Things that can really be what we called global catastrophes. I think that that�s something that governments often aren�t quite able to focus on as much as we might like.\\n',\n",
       " '\\n',\n",
       " 'They often work in very short time cycles. Congress has to renew funding every couple of years for most agencies so it�s really hard for governments to focus on the long run, focus on things that might not be likely, but that would be a really big impact if it were to occur. I think that if you want governments to be able to work on those types of things, you really need some advocacy to back them up, sort of subsidize that work from the private sectors. I think we�re both able to fill that gap in the private sector, that there wasn�t that much philanthropy, and then also I�m hoping that Open Philanthropy will be able to support the government through advocacy and allow it to really prioritize some of the stuff that we thought was most important.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Does OpenPhil work on any other problem areas where the situation is similar?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I actually think that this was pretty unique among OpenPhil�s cause areas, at least while I was there. Because OpenPhil really does prioritize areas that are both very important and also neglected, often that meant it was areas that were neglected even by the public sector. So biosecurity and pandemic preparedness stood out to us as an area that was a little bit different, because it does have relatively large amounts of spending by governments but that still seemed like there were big gaps. The other thing that�s sort of unique about this area, is that it�s a little hard to define exactly what counts as spending through these pandemics.\\n',\n",
       " '\\n',\n",
       " 'So there�s a lot of work on public health in general that would be helpful from the perspective of pandemic preparedness, but that maybe isn�t the top thing that you might prioritize if all you cared about was using risk from pandemics, or in particular reducing risks from the types of pandemics that could really be global catastrophes. So if you look narrowly at interventions that are specifically focused just on lowering risk from that type of pandemic, then even governments I would say really neglect that.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So there�s just general public health spending designed for disease control that is better than nothing, but it�s not the thing that you�ve been most interested in doing if you were just focused on preventing a pandemic that could kill a really large number of people and was completely new.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Exactly.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: What kind of grants did you make in the area while you were at OpenPhil? Or what grants did OpenPhil make?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: So the main priority for most of the time I was there was actually not grant-making, but it was getting to know the area really well, start to set our strategy, and we made our hire, somebody who�s an expert in the field, Jaime Yassif, who�s now running the program area. But we did make a couple of grants while I was there. One of them was to a Blue Ribbon panel on biodefense in Washington DC. We gave them, to start, for their first year about $300,000 of our funding. They also had a bunch of other funding from other organizations. What they did was they got a panel of real policy luminaries, former policymakers in DC, to convene a set of meetings and identify real priorities in United States biodefense policy.\\n',\n",
       " '\\n',\n",
       " 'So figuring out what the biggest improvements to be made to prevent risks to the United States from epidemics. Not necessarily just things that can be a global catastrophe, but natural epidemics and also the potential use of bio weapons or potential biological accidents. So they put out a big report identifying priority areas and their goal was really both to identify priority areas, just difficult because there are so many different types of work that can be done in this field, and to really call attention in the internal DC world, so this is an important topic. We supported their work and that was one of the first grants that we made in the area.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: How do you think it went?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I haven�t been there for all of the follow-up and it would be great for some folks to, at some point, talk to the new program officer at GMA about it, but our initial question is number one, this type of thing is setting the stage for hopefully having policy changes in the future, and it was largely trying to convene the biodefense community and call attention. So we didn�t really expect policy change to immediately happen, but I think that the initial signs were incredibly successful. So they were able to get a meeting with the vice president; Vice President Joe Biden at the time, and had some follow-up with his staff.\\n',\n",
       " '\\n',\n",
       " 'They were able to get several hearings in front of Congress and there was at least some members of Congress who seemed like they might really have the potential to become champions of this cause, and really bring it on as one of their priorities, who seemed to have brought a lot from the hearings, so those seemed to be really good initial signs of success. And OpenPhil actually, since then, has renewed the grant and given them more funding to continue to work and to sort of do follow-up work saying, �We identified these priorities. Has the government made any progress since then?�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Are there any other grants that it would be interesting to talk about?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, I think the second one that was really interesting was a grant to an organization called iGEM, which is actually a student competition where undergrad students work on really cutting edge synthetic biology projects, which essentially means that they are combining DNA themselves to modify organisms to do really useful things. So one example of that was neat is that you can modify bacteria and basically make that bacteria into a sensor, so you can have it � An example that was really cool was a project where students created a drug-testing bacteria, so someone could find out whether or not some drug; heroin, was pure, or whether it had been adulterated, by basically having the bacteria react differently to different types of the drug.\\n',\n",
       " '\\n',\n",
       " 'And it�s really exciting work, some of the really cutting edge, synthetic biology is happening there and also a lot of those undergrads are going to grow up and be some of the top synthetic biologists down the road. We were really excited about the work that they�re doing, but it�s pretty easy to see how in the long run, this ability for more and more people to be able to modify organisms could also come with safety and security risks. So OpenPhil gave a grant to iGEM�s safety and security team to give them more resources to do things like train those students in how to work safely with the organisms they were working with, because right now the iGEM students are not working with anything particularly dangerous, but those same students down the road could be working with more dangerous organisms.\\n',\n",
       " '\\n',\n",
       " 'And then give them resources to teach the students about security culture, and teach them to be guardians of science, and synthetic biology as the field can only really work if you have a culture where everybody realizes that this is a technology that could have huge benefits, could also have huge risks. So the goal of the grant was both to give iGEM resources to have a really good safety and security program, but also to be able to experiment a little bit with that program, because down the road, what we really want is not just this one competition to have a really good safety and security program, but to know in general, what types of things work for creating a culture of safety and security in the DIY bio community in general.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: Do you know whether that grant worked out yet?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: I think we�re still too early to tell. I think we�ll find out down the road.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So we�ve touched on it, but now let�s really dive into the nature of the pandemic risks we face. Why is pandemic preparedness such an important thing to work on? Like possibly the most important thing for people to be working on?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Sure. I think the way to start to approach that question is to ask � And the way that OpenPhil originally approached that question was to first ask, why do we care so much about working on reducing global catastrophic risk general? and that ended up being a real priority for the Open Philanthropy Project because we care a lot, both about the well-being of current people, but also about the well-being of people down the road in the future generations, really protecting all of the good things that civilization�s brought for people down the line. So we started out by asking � Right now we have this trajectory where global poverty has been going down for a long period of time.\\n',\n",
       " '\\n',\n",
       " 'It seems like the well-being of most people on earth has been getting much better and there�s a lot of hope that in the future that will continue to happen. Are there things that could really destroy that, set us off track? So we went through a process of looking at the main candidates for events that could really disrupt that progress, and if we look at everything from climate change, to pandemics, to risks from emerging technologies to asteroids, war, yes, nuclear conflict, and try to first ask, �What would it take to really get civilization off track?� And that sort of heuristic that we came up with, is we tried to ask, �What are the events that could lead to something like hundreds of millions of deaths in a short period of time?�\\n',\n",
       " '\\n',\n",
       " 'When we asked that, and then limit it to areas where there wasn�t enough philanthropy yet and where there were apparent things that philanthropy could really do about it, biosecurity and pandemic preparedness was really towards the top. And I could go through a few reasons why that was the case. When we talk about biosecurity and pandemic preparedness, we usually split it into two categories. One of them is risks from natural pandemics. It could be everything from the flu, to smallpox, to HIV and AIDS. And the other is risks from pandemics that might be caused by humans. So that kind of splits them into two categories. One of them is potentially the use of biological weapons.\\n',\n",
       " '\\n',\n",
       " 'A second is the possibility of an outbreak that starts from a lab accident. So somebody doing research, working with potentially dangerous organisms, and accidentally allows some of that to get out and starts an outbreak that way. We looked into both of those as part of the cause area, and natural pandemics, at least in the short run, seem much more likely to occur. We have a long history of natural pandemics occurring. We had several natural pandemics over the last century. Most recently was the flu pandemic in 2009 and new outbreaks are emerging all the time. So if we just want to ask, �What�s the most likely source of a big outbreak over the next several years?� That�s going to be where it comes from.\\n',\n",
       " '\\n',\n",
       " 'Then the second question that we asked is, �Could a natural pandemic really get to that level of a global catastrophe?� And that�s a more difficult question for us. We have a lot of examples of outbreaks that were definitely tragedies, where there were thousands of deaths that could have been preventable, but we also have some pretty good evidence of society coming back and being fairly resilient and making it through those events. So we want to ask, �Could a natural pandemic really rise to the level of our sort of threshold of hundreds of millions of deaths?� The main thing that we looked at for evidence was the Spanish flu, which occurred back in 1918. That pandemic killed about 3 to 5% of the world�s population, which if you projected that out to today, ends up at�\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: � million, 200 million.\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, exactly � deaths. So that at least gives us a sense that a one-in-100-year-type pandemic might reach that threshold. Those are really difficult questions about if that occurred today, would it be similarly bad? I think it�s hard to know, and having talked to a lot of the experts in the field, opinions really differ. On the one hand, healthcare�s gotten better. On the other hand, we�ve had globalization and outbreaks can spread a lot faster. So when we look at, at least the possibility, it seems possible that you really could have a pandemic of that size from natural causes.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: There�s also just a lot more people in some very dense cities, so of course, it�s very difficult to control diseases, right?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes. That�s a factor too. And then there are phenomena like global warming, which some folks at least think could really increase pandemic risk in the near future as it forces people to migrate a little bit, and move to areas, and come into contact with animals and disease reservoirs that they haven�t touched before.\\n',\n",
       " '\\n',\n",
       " 'Robert Wiblin: So, the Spanish flu in 1918, �19 killed 3 to 5%, which today would be 200 million, 300 million, but that wasn�t even the worst pandemic in history, right? Because it also had the Black Death and smallpox when it got to the Americas. I�ve read stats where as high as 50% of the population have died, or we think historically they died in the very worst cases, when the Black Death came through some parts of Europe. Is that right?\\n',\n",
       " '\\n',\n",
       " 'Howie Lempel: Yes, so I�m not sure if we�ve gotten up to 50% of the global population�\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mod_text.txt', 'w') as f:\n",
    "    f.write(no_time)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
